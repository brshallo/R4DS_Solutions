[
["index.html", "Yet another study guide to ‘R for Data Science’ Purpose Origin Organization and features Acknowledgements License", " Yet another study guide to ‘R for Data Science’ Bryan Shalloway Last updated: 2019-06-07 Purpose This book contains my solutions and notes to Garrett Grolemund and Hadley Wickham’s excellent book, R for Data Science (Grolemund and Wickham 2017). R for Data Science (R4DS) is my go-to recommendation for people getting started in R programming, “data science”, or the “tidyverse”. First and foremost, this book was set-up as a resource and refresher for myself1. If you are looking for a reliable solutions manual to check your answers as you work through R4DS, I would recommend using the solutions created and mantained by Jeffrey Arnold, R for Data Science: Exercise Solutions2. Though feel free to use Yet another study guide to ‘R for Data Science’ as another point of reference3. Origin I first read and completed the exercises to R4DS in early 2017 on the tail-end of completing a Master’s in Analytics program. My second time going through R4DS came in early 2018 when myself and Stephen Kimel organized an internal “R for Data Science” study group with our colleagues4. In June of 2019 I published my solutions and notes into this book. Organization and features Chapters start with the following: A list of “Key exercises” deemed good for discussion in a study group A list of functions (and sometimes notes) from the chapter5 Chapters also contain: Solutions to exercises Exercise subsections are arranged in the same chapter –&gt; section –&gt; subsection as the original book Chapters, sections, and subsections without exercises are usually not included The beginning of sections may occassionally contain additional notes, e.g. 3.8: Position Adjustment The “Appendix” sections in chapters typically contain alternative solutions to problems or additional notes/thoughts pertaining to the chapter or a related topic I use the numbering scheme {chapter}.{section}.{subsection}.{problem number} to refer to exercise solutions in “Appendix” sections There are a few cautions with using this book6 Acknowledgements Thank you: Garrett Grolemund and Hadley Wickham for writing a phenomenal book! The various tidyverse and RStudio developers for producing outstanding packages, products, as well as resources for learning R for Data Science Online Learning Community and #rstats communities for creating inspiring, safe places to post ideas, ask questions, and grow your R skills Stephen Kimel, who has co-organized a “data science” study group with me and also provided feedback on my R4DS solutions. In many cases I changed my solution to an exercise to a method that mirrored his approach. License This work is licensed under a Creative Commons Attribution 4.0 International License. References "],
["03-data-visualization.html", "Ch 3: Data visualization 3.2: First steps 3.3: Aesthetic mappings 3.5: Facets 3.6: Geometric Objects 3.7: Statistical transformations 3.8: Position Adjustment 3.9: Coordinate systems Appendix", " Ch 3: Data visualization Key questions: 3.6.1. #6 3.8.1. #8 Functions and notes: geom_point: Add points to plot, key args: x, y, size, stroke, colour, alpha, shape geom_smooth: Add line and confidence intervals to x-y plot, can use se to turn off standard errors, can use method to change algorithm to make line. linetype to make dotted line. geom_bar: Stack values on top of each to make bars (default stat = \"count\", can also change to \"identity\". May want to make y = ..prop.. to show y as proportion of values). postion = \"stacked\" may take on values of “identity\",”dodge\", \"fill\" geom_count: Make bar charts out of discrete row values in dataframe. fill to fill bars, colour to outline. geom_jitter: like geom_point() but with randomness added, use width and height args to control (could also use geom_point() with position = \"jitter\") geom_boxplot: box and whiskers plot geom_polygon: Can use to plot points – use with objects created from map_data() geom_abline: use args intercept and slope to create line facet_wrap: Facet multiple charts by one variable; scales = \"free_x\" (or “free”, or “free_y” are helpful) facet_grid: Facet multiple by charts by one or two variables: space is helpful arg (not within facet_wrap()) stat_count: Like geom_bar() stat_summary: can use to explicitly show ranges, e.g. with args fun.ymin = min, fun.ymax = max, fun.y = median stat_bin: Like geom_histogram() stat_smooth: Sames as geom_smooth but can take non-standard geom position adjustments: identity: ; dodge: ; fill: ; position_dodge ; position_fill ; position_identiy ; position_jitter ; position_stack ; ways to override default mapping coord_quickmap: Set aspect ratio for maps coord_flip: Flip x and y coordinates coord_polar: Use polar coordinates – don’t use much (should set theme(aspect.ratio = 1) +labs(x = NULL, y = NULL)) coord_fixed: Fix x and y to be same size distance between tickmarks ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;( mapping = aes(&lt;MAPPINGS&gt;), stat = &lt;STAT&gt;, position = &lt;POSITION&gt; ) + &lt;COORDINATE_FUNCTION&gt; + &lt;FACET_FUNCTION&gt; 3.2: First steps 3.2.4 1. Run ggplot(data = mpg). What do you see? ggplot(data = mpg) A blank grey space. 2. How many rows are in mpg? How many columns? nrow(mtcars) ## [1] 32 ncol(mtcars) ## [1] 11 3. What does the drv variable describe? Read the help for ?mpg to find out. Front wheel, rear wheel or 4 wheel drive. 4. Make a scatterplot of hwy vs cyl. ggplot(mpg)+ geom_point(aes(x = hwy, y = cyl)) 5. What happens if you make a scatterplot of class vs drv? Why is the plot not useful? ggplot(mpg)+ geom_point(aes(x = class, y = drv)) The points stack-up on top of one another so you don’t get a sense of how many are on each point. Any ideas for what methods could you use to improve the view of this data? Jitter points so they don’t line-up, or make point size represent number of points that are stacked. 3.3: Aesthetic mappings 3.3.1. 1. What’s gone wrong with this code? Why are the points not blue? ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;)) The color field is in the aes function so it is expecting a character or factor variable. By inputting “blue” here, ggplot reads this as a character field with the value “blue” that it then supplies it’s default color schemes to (1st: salmon, 2nd: teal) 2. Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg? mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto~ f 18 29 p comp~ ## 2 audi a4 1.8 1999 4 manu~ f 21 29 p comp~ ## 3 audi a4 2 2008 4 manu~ f 20 31 p comp~ ## 4 audi a4 2 2008 4 auto~ f 21 30 p comp~ ## 5 audi a4 2.8 1999 6 auto~ f 16 26 p comp~ ## 6 audi a4 2.8 1999 6 manu~ f 18 26 p comp~ ## 7 audi a4 3.1 2008 6 auto~ f 18 27 p comp~ ## 8 audi a4 q~ 1.8 1999 4 manu~ 4 18 26 p comp~ ## 9 audi a4 q~ 1.8 1999 4 auto~ 4 16 25 p comp~ ## 10 audi a4 q~ 2 2008 4 manu~ 4 20 28 p comp~ ## # ... with 224 more rows The data is in tibble form already so just printing it shows the type, but could also use the glimpse and str functions. 3. Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? ggplot(data = mpg) + geom_point(mapping = aes(x = cty, y = hwy, color = cyl, size = displ, shape = fl)) color: For continuous applies a gradient, for categorical it applies distinct colors based on the number of categories. size: For continuous, applies in order, for categorical will apply in an order that may be arbitrary if there is not an order provided. shape: Will not allow you to input a continuous variable. 4. What happens if you map the same variable to multiple aesthetics? Will map onto both fields. Can be redundant in some cases, in others it can be valuable for clarity. ggplot(data = mpg)+ geom_point(mapping = aes(x = cty, y = hwy, color = fl, shape = fl)) 5. What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point) ?geom_point ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(shape = 21, colour = &quot;black&quot;, fill = &quot;white&quot;, size = 5, stroke = 5) ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(shape = 21, colour = &quot;black&quot;, fill = &quot;white&quot;, size = 5, stroke = 3) For shapes that have a border (like shape = 21), you can colour the inside and outside separately. Use the stroke aesthetic to modify the width of the border (can get similar effects by layering small point on bigger point). 6. What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? ggplot(data = mpg, mapping = aes(x = cty, y = hwy, colour = displ &lt; 5)) + geom_point() The field becomes a logical operator in this case. 3.5: Facets 3.5.1. 1. What happens if you facet on a continuous variable? ggplot(data = mpg, mapping = aes(x = cty, y = hwy))+ geom_point()+ facet_wrap(~cyl) It will facet along all of the unique values. 2. What do the empty cells in plot with facet_grid(drv ~ cyl) mean? ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ cyl) How do they relate to this plot? ggplot(data = mpg) + geom_point(mapping = aes(x = drv, y = cyl)) They represent the locations where there is no point on the above graph (could be made more clear by giving consistent order to axes). 3. What plots does the following code make? What does . do? ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ .) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(. ~ cyl) Can use to specify if to facet by rows or columns. 4. Take the first faceted plot in this section: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~ class, nrow = 2) What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset? Faceting prevents overlapping points in the data. A disadvantage is that you have to move your eye to look at different graphs. Some groups you don’t have much data on as well so those don’t present much information. If there is more data, you may be more comfortable using facetting as each group should have points that you can view. 5. Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol argument? nrow and ncol specify the number of columns or rows to facet by, facet_grid does not have this option because the splits are defined by the number of unique values in each variable. Other important options are scales which let you define if the scales are able to change between each plot. 6. When using facet_grid() you should usually put the variable with more unique levels in the columns. Why? I’m not sure why exactly, if I compare these, it’s not completely unclear. #more unique levels on columns ggplot(data = mpg) + geom_point(mapping = aes(x = cty, y = hwy)) + facet_grid(year ~ class) #more unique levels on rows ggplot(data = mpg) + geom_point(mapping = aes(x = cty, y = hwy)) + facet_grid(class ~ year) My guess though would be that it’s because our computer screens are generally wider than they are tall. Hence there will be more space for viewing a higher number of attributes going across columns than by rows. 3.6: Geometric Objects 3.6.1 1. What geom would you use to draw a line chart? A boxplot? A histogram? An area chart? * geom_line * geom_boxplot * geom_histogram * geom_area + Notice that geom_area is just a special case of geom_ribbon Example of geom_area: huron &lt;- data.frame(year = 1875:1972, level = as.vector(LakeHuron) - 575) h &lt;- ggplot(huron, aes(year)) h + geom_ribbon(aes(ymin = 0, ymax = level)) h + geom_area(aes(y = level)) # Add aesthetic mappings h + geom_ribbon(aes(ymin = level - 1, ymax = level + 1), fill = &quot;grey70&quot;) + geom_line(aes(y = level)) h + geom_area(aes(y = level), fill = &quot;grey70&quot;) + geom_line(aes(y = level)) 2. Run this code in your head and predict what the output will look like. Then, run the code in R and check your predictions. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 3. What does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter? ggplot(data = mpg) + geom_smooth( mapping = aes(x = displ, y = hwy, color = drv), show.legend = FALSE ) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; It get’s rid of the legend that would be assogiated with this geom. You removed it previously to keep it consistent with your other graphs that did not include them to specify the drv. 4. What does the se argument to geom_smooth() do? se here stands for standard error, so if we specify it as FALSE we are saying we do not want to show the standard errors for the plot. 5. Will these two graphs look different? Why/why not? ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; No, because local mappings for each geom are the same as the global mappings in the other. 6. Recreate the R code necessary to generate the following graphs. ggplot(mpg, aes(displ, hwy))+ geom_point() + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(displ, hwy, group = drv))+ geom_point() + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(displ, hwy, colour = drv))+ geom_point() + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(displ, hwy))+ geom_point(aes(colour = drv)) + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(displ, hwy))+ geom_point(aes(color = drv)) + geom_smooth(aes(linetype = drv), se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(mpg, aes(displ, hwy)) + geom_point(colour = &quot;white&quot;, size = 4) + geom_point(aes(colour = drv)) 3.7: Statistical transformations 3.7.1. 1. What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function? The default is geom_pointrange, the point being the mean, and the lines being the standard error on the y value (i.e. the deviation of the mean of the value). ggplot(mpg) + stat_summary(aes(cyl, cty)) ## No summary function supplied, defaulting to `mean_se() Rewritten with geom7: ggplot(mpg)+ geom_pointrange(aes(x = cyl, y = cty), stat = &quot;summary&quot;) ## No summary function supplied, defaulting to `mean_se() The specific example though is actually not the default: ggplot(data = diamonds) + stat_summary( mapping = aes(x = cut, y = depth), fun.ymin = min, fun.ymax = max, fun.y = median ) Rewritten with geom: ggplot(data = diamonds)+ geom_pointrange(aes(x = cut, y = depth), stat = &quot;summary&quot;, fun.ymin = &quot;min&quot;, fun.ymax = &quot;max&quot;, fun.y = &quot;median&quot;) 2. What does geom_col() do? How is it different to geom_bar()? geom_col has \"identity\" as the default stat, so it expects to receive a variable that already has the value aggregated8 3.Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common? ?ggplot2 4.What variables does stat_smooth() compute? What parameters control its behaviour? See here: http://ggplot2.tidyverse.org/reference/#section-layer-stats for a helpful resource. Also, someone who aggregated some online: http://sape.inf.usi.ch/quick-reference/ggplot2/geom9 5. In our proportion bar chart, we need to set group = 1. Why? In other words what is the problem with these two graphs? ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..)) Fixed graphs: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1)) For this second graph though, I would think you would want something more like the following: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color), position = &quot;fill&quot;)+ ylab(&quot;prop&quot;) # ylab would say &quot;count&quot; w/o this (Which could be generated by this code as well, using geom_count()) diamonds %&gt;% count(cut, color) %&gt;% group_by(cut) %&gt;% mutate(prop = n / sum(n)) %&gt;% ggplot(aes(x = cut, y = prop, fill = color))+ geom_col() 3.8: Position Adjustment Some “dodge”\" examples: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;dodge&quot;) diamonds %&gt;% count(cut, color) %&gt;% ggplot(aes(x = cut, y = n, fill = color))+ geom_col(position = &quot;dodge&quot;) the interaction() function can also sometimes be helpful for these types of charts Looking of geom_jitter and only changing width. ggplot(data = mpg, mapping = aes(x = drv, y = hwy))+ geom_jitter(height = 0, width = .2) 3.8.1. 1.What is the problem with this plot? How could you improve it? ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() The points overlap, could use geom_jitter instead ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_jitter() 2. What parameters to geom_jitter() control the amount of jittering? height and width 3. Compare and contrast geom_jitter() with geom_count(). Take the above chart and instead use geom_count. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_count() Can also use geom_count with color, and can use “jitter” in position arg. ggplot(data = mpg, mapping = aes(x = cty, y = hwy, colour = drv)) + geom_count() ggplot(data = mpg, mapping = aes(x = cty, y = hwy, colour = drv)) + geom_count(position = &quot;jitter&quot;) ggplot(data = mpg, mapping = aes(x = cty, y = hwy, colour = drv)) + geom_jitter(size = 3, alpha = 0.3) One problem with geom_count is that the shapes can still block-out other shapes at that same point of different colors. You can flip the orderof the stacking order of the colors with position = “dodge”. Still this seems limited. ggplot(data = mpg, mapping = aes(x = cty, y = hwy, colour = drv)) + geom_count(position = &quot;dodge&quot;) ## Warning: Width not defined. Set with `position_dodge(width = ?)` 4. What’s the default position adjustment for geom_boxplot()? Create a visualisation of the mpg dataset that demonstrates it. dodge (but seems like changing to identity is the same) ggplot(data=mpg, mapping=aes(x=class, y=hwy))+ geom_boxplot() 3.9: Coordinate systems coord_flip is helpful, especially for quickly tackling issues with axis labels coord_quickmap is important to remember if plotting spatial data. coord_polar is important to remember if plotting spatial coordinates. map_data for extracting data on maps of locations 3.9.1. 1.Turn a stacked bar chart into a pie chart using coord_polar(). These are more illustrative than anything, here is a note from the documetantion: NOTE: Use these plots with caution - polar coordinates has major perceptual problems. The main point of these examples is to demonstrate how these common plots can be described in the grammar. Use with EXTREME caution. ggplot(mpg, aes(x = 1, fill = class))+ geom_bar(position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) + scale_x_continuous(labels = NULL) If I want to make multiple levels: ggplot(mpg, aes(x = as.factor(cyl), fill = class))+ geom_bar(position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) 2. What does labs() do? Read the documentation. Used for giving labels. ?labs 3. What’s the difference between coord_quickmap() and coord_map()? The first is an approximation, useful for smaller regions to be proected. For this example, do not see substantial differences. nz &lt;- map_data(&quot;nz&quot;) ggplot(nz,aes(long,lat,group=group))+ geom_polygon(fill=&quot;red&quot;,colour=&quot;black&quot;)+ coord_quickmap() ggplot(nz,aes(long,lat,group=group))+ geom_polygon(fill=&quot;red&quot;,colour=&quot;black&quot;)+ coord_map() 4. What does the plot below tell you about the relationship between city and highway in mpg? Why is coord_fixed() important? What does geom_abline() do? geom_abline() adds a line with a given intercept and slope (either given by aes or by intercept and slope args) coord_fixed() ensures that the ratios between the x and y axis stay at a specified relationship (default: 1). This is important for easily seeing the magnitude of the relationship between variables. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_abline() + coord_fixed() Appendix 3.7.1.1 extension ggplot(mpg, aes(x = cyl, y = cty, group = cyl))+ geom_pointrange(stat = &quot;summary&quot;) ## No summary function supplied, defaulting to `mean_se() This seems to be the same as what you would get by doing the following with dplyr: mpg %&gt;% group_by(cyl) %&gt;% dplyr::summarise(mean = mean(cty), sd = (sum((cty - mean(cty))^2) / (n() - 1))^0.5, n = n(), se = sd / n^0.5, lower = mean - se, upper = mean + se) %&gt;% ggplot(aes(x = cyl, y = mean, group = cyl))+ geom_pointrange(aes(ymin = lower, ymax = upper)) Other geoms you could have set stat_summary to: crossbar: ggplot(mpg) + stat_summary(aes(cyl, cty), geom = &quot;crossbar&quot;) ## No summary function supplied, defaulting to `mean_se() errorbar: ggplot(mpg) + stat_summary(aes(cyl, cty), geom = &quot;errorbar&quot;) ## No summary function supplied, defaulting to `mean_se() linerange: ggplot(mpg) + stat_summary(aes(cyl, cty), geom = &quot;linerange&quot;) ## No summary function supplied, defaulting to `mean_se() See 3.7.1.1 extension for notes on how to relate this to dplyr code.↩ I often use this over geom_bar and do the aggregation with dplyr rather than ggplot2.↩ Though it’s missing some very common ones like geom_col and geom_bar.↩ "],
["05-data-transformations.html", "Ch. 5: Data transformations 5.2: Filter rows 5.3: Arrange rows 5.4: Select columns 5.5: Add new vars 5.6: Grouped summaries… 5.7: Grouped mutates… Appendix", " Ch. 5: Data transformations Key questions: 5.2.4 #1 5.3.1 #4 5.4.1 #1 5.5.2 #1 5.6.7 #1, 3, 6 5.7.1 #2, 3, 4 Functions and notes: filter(): for filtering rows by some condition(s) arrange(): for ordering rows by some condition(s) desc: order by descending instead (often use within arrange or with ranking functions) select(): for selecting columns by name, position, or criteria helper functions: everything, starts_with, ends_with, contains, matches: selects variables that match a regular expression, num_range(\"x\", 1:3): matches x1, x2 and x3 rename(): rename variables w/o dropping variables not indicated mutate(): for changing columns and adding new columns * group_by(): for performing operations grouped by the values of some fields summarise(): for collapsing dataframes into individual rows or aggregates – typically used in conjunction with group_by(), typically used to aggregate %&gt;%: pass the previous output into the first position of the next argument, think of as saying, “then you do…” count: shortcut for &lt;group_by([var])&gt; –&gt; &lt;summarise(n = n())&gt; near: Are two values essentially equal (use to test equivalence and deals with oddities in floats) is.na: TRUE output if NA (and related values) else FALSE between: between(Sepal.Length, 1, 3) is equivalent to Sepal.Length &gt;=1 &amp; Sepal.Length &lt;=3 transmute: mutate but only keep the outputted column(s) lead, lag: take value n positions in lead or lag position log, log2, log10: log funcitons of base e, 2, 10 cumsum, cumprod, cummin, cummax, cummean: Common cumalitive functions &lt;, &lt;=, &gt;, &gt;=, !=: Logical operators min_rank, row_number, dense_rank, percent_rank, cume_dist, ntile: common ranking functions Location: mean; median Measures of spread: sd: standard deviation; IQR(): Interquartile range; mad(): median absolute deviaiton x &lt;- c(1, 2, 3, 4, 6, 7, 8, 8, 10, 100) IQR(x) ## [1] 4.75 mad(x) ## [1] 4.4478 sd(x) ## [1] 30.04238 Rank: min; quantile; max Position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)] but let you set a default value if that position does not exist first(x) ## [1] 1 nth(x, 5) ## [1] 6 last(x) ## [1] 100 measures of rank: min, max, rank, quantile(x, 0.25) is just 0.25 value (generalization of median, but allows you to specify) counts: n() for rows, sum(!is.na(x)) for non-missing rows, for distinct count, use n_distinct(x) Counts and proportions of logical values: sum(x &gt; 10), mean(y == 0) range() returns vector containing min and max of values in a vector (so returns two values). vignette: function to open vignettes e.g. vignette(\"window-functions\") 5.2: Filter rows 5.2.4. 1.Find all flights that… 1.1.Find flights that had an arrival delay of 2 + hrs filter(flights, arr_delay &gt;= 120) %&gt;% glimpse() ## Observations: 10,200 ## Variables: 19 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ dep_time &lt;int&gt; 811, 848, 957, 1114, 1505, 1525, 1549, 1558, 17... ## $ sched_dep_time &lt;int&gt; 630, 1835, 733, 900, 1310, 1340, 1445, 1359, 16... ## $ dep_delay &lt;dbl&gt; 101, 853, 144, 134, 115, 105, 64, 119, 62, 103,... ## $ arr_time &lt;int&gt; 1047, 1001, 1056, 1447, 1638, 1831, 1912, 1718,... ## $ sched_arr_time &lt;int&gt; 830, 1950, 853, 1222, 1431, 1626, 1656, 1515, 1... ## $ arr_delay &lt;dbl&gt; 137, 851, 123, 145, 127, 125, 136, 123, 123, 13... ## $ carrier &lt;chr&gt; &quot;MQ&quot;, &quot;MQ&quot;, &quot;UA&quot;, &quot;UA&quot;, &quot;EV&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;EV&quot;,... ## $ flight &lt;int&gt; 4576, 3944, 856, 1086, 4497, 525, 4181, 5712, 4... ## $ tailnum &lt;chr&gt; &quot;N531MQ&quot;, &quot;N942MQ&quot;, &quot;N534UA&quot;, &quot;N76502&quot;, &quot;N17984... ## $ origin &lt;chr&gt; &quot;LGA&quot;, &quot;JFK&quot;, &quot;EWR&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;... ## $ dest &lt;chr&gt; &quot;CLT&quot;, &quot;BWI&quot;, &quot;BOS&quot;, &quot;IAH&quot;, &quot;RIC&quot;, &quot;MCO&quot;, &quot;MCI&quot;... ## $ air_time &lt;dbl&gt; 118, 41, 37, 248, 63, 152, 234, 53, 119, 154, 2... ## $ distance &lt;dbl&gt; 544, 184, 200, 1416, 277, 937, 1092, 228, 533, ... ## $ hour &lt;dbl&gt; 6, 18, 7, 9, 13, 13, 14, 13, 16, 16, 13, 14, 16... ## $ minute &lt;dbl&gt; 30, 35, 33, 0, 10, 40, 45, 59, 30, 20, 25, 22, ... ## $ time_hour &lt;dttm&gt; 2013-01-01 06:00:00, 2013-01-01 18:00:00, 2013... 1.2.flew to Houston IAH or HOU filter(flights, dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) ## # A tibble: 9,313 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 623 627 -4 933 ## 4 2013 1 1 728 732 -4 1041 ## 5 2013 1 1 739 739 0 1104 ## 6 2013 1 1 908 908 0 1228 ## 7 2013 1 1 1028 1026 2 1350 ## 8 2013 1 1 1044 1045 -1 1352 ## 9 2013 1 1 1114 900 134 1447 ## 10 2013 1 1 1205 1200 5 1503 ## # ... with 9,303 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1.3.flew through American, United or Delta filter(flights, carrier %in% c(&quot;UA&quot;, &quot;AA&quot;,&quot;DL&quot;)) ## # A tibble: 139,504 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 554 600 -6 812 ## 5 2013 1 1 554 558 -4 740 ## 6 2013 1 1 558 600 -2 753 ## 7 2013 1 1 558 600 -2 924 ## 8 2013 1 1 558 600 -2 923 ## 9 2013 1 1 559 600 -1 941 ## 10 2013 1 1 559 600 -1 854 ## # ... with 139,494 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1.4. Departed in Summer filter(flights, month &lt;= 8 &amp; month &gt;= 6) ## # A tibble: 86,995 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 6 1 2 2359 3 341 ## 2 2013 6 1 451 500 -9 624 ## 3 2013 6 1 506 515 -9 715 ## 4 2013 6 1 534 545 -11 800 ## 5 2013 6 1 538 545 -7 925 ## 6 2013 6 1 539 540 -1 832 ## 7 2013 6 1 546 600 -14 850 ## 8 2013 6 1 551 600 -9 828 ## 9 2013 6 1 552 600 -8 647 ## 10 2013 6 1 553 600 -7 700 ## # ... with 86,985 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1.5. Arrived more than 2 hours late, but didn’t leave late filter(flights, arr_delay &gt; 120, dep_delay &gt;= 0) ## # A tibble: 10,008 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 811 630 101 1047 ## 2 2013 1 1 848 1835 853 1001 ## 3 2013 1 1 957 733 144 1056 ## 4 2013 1 1 1114 900 134 1447 ## 5 2013 1 1 1505 1310 115 1638 ## 6 2013 1 1 1525 1340 105 1831 ## 7 2013 1 1 1549 1445 64 1912 ## 8 2013 1 1 1558 1359 119 1718 ## 9 2013 1 1 1732 1630 62 2028 ## 10 2013 1 1 1803 1620 103 2008 ## # ... with 9,998 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1.6. were delayed at least an hour, but made up over 30 mins in flight filter(flights, (arr_delay - dep_delay) &lt;= -30, dep_delay &gt;= 60) ## # A tibble: 2,074 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 1716 1545 91 2140 ## 2 2013 1 1 2205 1720 285 46 ## 3 2013 1 1 2326 2130 116 131 ## 4 2013 1 3 1503 1221 162 1803 ## 5 2013 1 3 1821 1530 171 2131 ## 6 2013 1 3 1839 1700 99 2056 ## 7 2013 1 3 1850 1745 65 2148 ## 8 2013 1 3 1923 1815 68 2036 ## 9 2013 1 3 1941 1759 102 2246 ## 10 2013 1 3 1950 1845 65 2228 ## # ... with 2,064 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # #Equivalent solution: # filter(flights, (arr_delay - dep_delay) &lt;= -30 &amp; dep_delay &gt;= 60) 1.7. departed between midnight and 6am (inclusive) filter(flights, dep_time &gt;= 0 &amp; dep_time &lt;= 600) ## # A tibble: 9,344 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 9,334 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # # Equivalent solution: # filter(flights, dep_time &gt;= 0, dep_time &lt;= 600) 2. Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? This is a shortcut for x &gt;= left &amp; x &lt;= right solving 1.7. using between: filter(flights, between(dep_time, 0, 600)) 3. How many flights have a missing dep_time? What other variables are missing? What might these rows represent? filter(flights, is.na(dep_time)) ## # A tibble: 8,255 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 NA 1630 NA NA ## 2 2013 1 1 NA 1935 NA NA ## 3 2013 1 1 NA 1500 NA NA ## 4 2013 1 1 NA 600 NA NA ## 5 2013 1 2 NA 1540 NA NA ## 6 2013 1 2 NA 1620 NA NA ## 7 2013 1 2 NA 1355 NA NA ## 8 2013 1 2 NA 1420 NA NA ## 9 2013 1 2 NA 1321 NA NA ## 10 2013 1 2 NA 1545 NA NA ## # ... with 8,245 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 8255, perhaps these are canceled flights. 4. Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE &amp; NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!) NA^0 ## [1] 1 Anything raised to the 0 is 1. FALSE &amp; NA ## [1] FALSE For the “AND” operator &amp; for it to be TRUE both values would need to be TRUE so if one is FALSE the entire statment must be. TRUE | NA ## [1] TRUE The “OR” operator | specifies that if at least one of the values is TRUE the whole statement is, so because one is already TRUE the whole statement must be. NA*0 ## [1] NA This does not come-out to 0 as expected because the laws of addition and multiplication here only hold for natural numbers, but it is possible that NA could represent Inf or -Inf in which case the outut is NaN rather than 0. Inf*0 ## [1] NaN See this article for more details: https://math.stackexchange.com/questions/28940/why-is-infinity-multiplied-by-zero-not-an-easy-zero-answer . 5.3: Arrange rows 5.3.1. 1. use arrange() to sort out all missing values to start df &lt;- tibble(x = c(5, 2, NA)) arrange(df, !is.na(x)) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 NA ## 2 5 ## 3 2 2. Find most delayed departures arrange(flights, desc(dep_delay)) %&gt;% glimpse() ## Observations: 336,776 ## Variables: 19 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 1, 6, 1, 9, 7, 4, 3, 6, 7, 12, 5, 1, 2, 5, 12, ... ## $ day &lt;int&gt; 9, 15, 10, 20, 22, 10, 17, 27, 22, 5, 3, 1, 10,... ## $ dep_time &lt;int&gt; 641, 1432, 1121, 1139, 845, 1100, 2321, 959, 22... ## $ sched_dep_time &lt;int&gt; 900, 1935, 1635, 1845, 1600, 1900, 810, 1900, 7... ## $ dep_delay &lt;dbl&gt; 1301, 1137, 1126, 1014, 1005, 960, 911, 899, 89... ## $ arr_time &lt;int&gt; 1242, 1607, 1239, 1457, 1044, 1342, 135, 1236, ... ## $ sched_arr_time &lt;int&gt; 1530, 2120, 1810, 2210, 1815, 2211, 1020, 2226,... ## $ arr_delay &lt;dbl&gt; 1272, 1127, 1109, 1007, 989, 931, 915, 850, 895... ## $ carrier &lt;chr&gt; &quot;HA&quot;, &quot;MQ&quot;, &quot;MQ&quot;, &quot;AA&quot;, &quot;MQ&quot;, &quot;DL&quot;, &quot;DL&quot;, &quot;DL&quot;,... ## $ flight &lt;int&gt; 51, 3535, 3695, 177, 3075, 2391, 2119, 2007, 20... ## $ tailnum &lt;chr&gt; &quot;N384HA&quot;, &quot;N504MQ&quot;, &quot;N517MQ&quot;, &quot;N338AA&quot;, &quot;N665MQ... ## $ origin &lt;chr&gt; &quot;JFK&quot;, &quot;JFK&quot;, &quot;EWR&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;... ## $ dest &lt;chr&gt; &quot;HNL&quot;, &quot;CMH&quot;, &quot;ORD&quot;, &quot;SFO&quot;, &quot;CVG&quot;, &quot;TPA&quot;, &quot;MSP&quot;... ## $ air_time &lt;dbl&gt; 640, 74, 111, 354, 96, 139, 167, 313, 109, 149,... ## $ distance &lt;dbl&gt; 4983, 483, 719, 2586, 589, 1005, 1020, 2454, 76... ## $ hour &lt;dbl&gt; 9, 19, 16, 18, 16, 19, 8, 19, 7, 17, 20, 18, 8,... ## $ minute &lt;dbl&gt; 0, 35, 35, 45, 0, 0, 10, 0, 59, 0, 55, 35, 30, ... ## $ time_hour &lt;dttm&gt; 2013-01-09 09:00:00, 2013-06-15 19:00:00, 2013... 3. Find the fastest flights arrange(flights, air_time) %&gt;% glimpse() ## Observations: 336,776 ## Variables: 19 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 1, 4, 12, 2, 2, 2, 3, 3, 3, 3, 5, 5, 6, 8, 9, 9... ## $ day &lt;int&gt; 16, 13, 6, 3, 5, 12, 2, 8, 18, 19, 8, 19, 12, 1... ## $ dep_time &lt;int&gt; 1355, 537, 922, 2153, 1303, 2123, 1450, 2026, 1... ## $ sched_dep_time &lt;int&gt; 1315, 527, 851, 2129, 1315, 2130, 1500, 1935, 1... ## $ dep_delay &lt;dbl&gt; 40, 10, 31, 24, -12, -7, -10, 51, 87, 41, 137, ... ## $ arr_time &lt;int&gt; 1442, 622, 1021, 2247, 1342, 2211, 1547, 2131, ... ## $ sched_arr_time &lt;int&gt; 1411, 628, 954, 2224, 1411, 2225, 1608, 2056, 1... ## $ arr_delay &lt;dbl&gt; 31, -6, 27, 23, -29, -14, -21, 35, 67, 19, 109,... ## $ carrier &lt;chr&gt; &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;US&quot;, &quot;9E&quot;,... ## $ flight &lt;int&gt; 4368, 4631, 4276, 4619, 4368, 4619, 2132, 3650,... ## $ tailnum &lt;chr&gt; &quot;N16911&quot;, &quot;N12167&quot;, &quot;N27200&quot;, &quot;N13913&quot;, &quot;N13955... ## $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LGA&quot;... ## $ dest &lt;chr&gt; &quot;BDL&quot;, &quot;BDL&quot;, &quot;BDL&quot;, &quot;PHL&quot;, &quot;BDL&quot;, &quot;PHL&quot;, &quot;BOS&quot;... ## $ air_time &lt;dbl&gt; 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,... ## $ distance &lt;dbl&gt; 116, 116, 116, 80, 116, 80, 184, 94, 116, 116, ... ## $ hour &lt;dbl&gt; 13, 5, 8, 21, 13, 21, 15, 19, 13, 21, 21, 21, 2... ## $ minute &lt;dbl&gt; 15, 27, 51, 29, 15, 30, 0, 35, 29, 45, 59, 59, ... ## $ time_hour &lt;dttm&gt; 2013-01-16 13:00:00, 2013-04-13 05:00:00, 2013... 4. Flights traveling the longest distance arrange(flights, desc(distance)) %&gt;% glimpse() ## Observations: 336,776 ## Variables: 19 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ day &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ... ## $ dep_time &lt;int&gt; 857, 909, 914, 900, 858, 1019, 1042, 901, 641, ... ## $ sched_dep_time &lt;int&gt; 900, 900, 900, 900, 900, 900, 900, 900, 900, 90... ## $ dep_delay &lt;dbl&gt; -3, 9, 14, 0, -2, 79, 102, 1, 1301, -1, -5, 1, ... ## $ arr_time &lt;int&gt; 1516, 1525, 1504, 1516, 1519, 1558, 1620, 1504,... ## $ sched_arr_time &lt;int&gt; 1530, 1530, 1530, 1530, 1530, 1530, 1530, 1530,... ## $ arr_delay &lt;dbl&gt; -14, -5, -26, -14, -11, 28, 50, -26, 1272, -41,... ## $ carrier &lt;chr&gt; &quot;HA&quot;, &quot;HA&quot;, &quot;HA&quot;, &quot;HA&quot;, &quot;HA&quot;, &quot;HA&quot;, &quot;HA&quot;, &quot;HA&quot;,... ## $ flight &lt;int&gt; 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,... ## $ tailnum &lt;chr&gt; &quot;N380HA&quot;, &quot;N380HA&quot;, &quot;N380HA&quot;, &quot;N384HA&quot;, &quot;N381HA... ## $ origin &lt;chr&gt; &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;... ## $ dest &lt;chr&gt; &quot;HNL&quot;, &quot;HNL&quot;, &quot;HNL&quot;, &quot;HNL&quot;, &quot;HNL&quot;, &quot;HNL&quot;, &quot;HNL&quot;... ## $ air_time &lt;dbl&gt; 659, 638, 616, 639, 635, 611, 612, 645, 640, 63... ## $ distance &lt;dbl&gt; 4983, 4983, 4983, 4983, 4983, 4983, 4983, 4983,... ## $ hour &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,... ## $ minute &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ time_hour &lt;dttm&gt; 2013-01-01 09:00:00, 2013-01-02 09:00:00, 2013... and the shortest distance. arrange(flights, distance) %&gt;% glimpse() ## Observations: 336,776 ## Variables: 19 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ day &lt;int&gt; 27, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ... ## $ dep_time &lt;int&gt; NA, 2127, 1240, 1829, 2128, 1155, 2125, 2124, 2... ## $ sched_dep_time &lt;int&gt; 106, 2129, 1200, 1615, 2129, 1200, 2129, 2129, ... ## $ dep_delay &lt;dbl&gt; NA, -2, 40, 134, -1, -5, -4, -5, -3, -3, 4, 6, ... ## $ arr_time &lt;int&gt; NA, 2222, 1333, 1937, 2218, 1241, 2224, 2212, 2... ## $ sched_arr_time &lt;int&gt; 245, 2224, 1306, 1721, 2224, 1306, 2224, 2224, ... ## $ arr_delay &lt;dbl&gt; NA, -2, 27, 136, -6, -25, 0, -12, 39, -7, -1, 9... ## $ carrier &lt;chr&gt; &quot;US&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;, &quot;EV&quot;,... ## $ flight &lt;int&gt; 1632, 3833, 4193, 4502, 4645, 4193, 4619, 4619,... ## $ tailnum &lt;chr&gt; NA, &quot;N13989&quot;, &quot;N14972&quot;, &quot;N15983&quot;, &quot;N27962&quot;, &quot;N1... ## $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;EWR&quot;... ## $ dest &lt;chr&gt; &quot;LGA&quot;, &quot;PHL&quot;, &quot;PHL&quot;, &quot;PHL&quot;, &quot;PHL&quot;, &quot;PHL&quot;, &quot;PHL&quot;... ## $ air_time &lt;dbl&gt; NA, 30, 30, 28, 32, 29, 22, 25, 30, 27, 30, 30,... ## $ distance &lt;dbl&gt; 17, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,... ## $ hour &lt;dbl&gt; 1, 21, 12, 16, 21, 12, 21, 21, 21, 21, 21, 21, ... ## $ minute &lt;dbl&gt; 6, 29, 0, 15, 29, 0, 29, 29, 30, 29, 29, 29, 17... ## $ time_hour &lt;dttm&gt; 2013-07-27 01:00:00, 2013-01-03 21:00:00, 2013... 5.4: Select columns 5.4.1. 1. Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. vars &lt;- c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;) #method 1 select(flights, vars) #method 2, probably indexes &lt;- which(names(flights) %in% vars) select(flights, indexes) #method 3 select(flights, contains(&quot;_time&quot;), contains(&quot;_delay&quot;), -contains(&quot;sched&quot;), -contains(&quot;air&quot;)) #method 4 select(flights, starts_with(&quot;dep&quot;), starts_with(&quot;arr&quot;)) %&gt;% select(ends_with(&quot;time&quot;), ends_with(&quot;delay&quot;)) ## # A tibble: 336,776 x 4 ## dep_time arr_time dep_delay arr_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 830 2 11 ## 2 533 850 4 20 ## 3 542 923 2 33 ## 4 544 1004 -1 -18 ## 5 554 812 -6 -25 ## 6 554 740 -4 12 ## 7 555 913 -5 19 ## 8 557 709 -3 -14 ## 9 557 838 -3 -8 ## 10 558 753 -2 8 ## # ... with 336,766 more rows 2. What happens if you include the name of a variable multiple times in a select() call? It only shows-up once. 3. What does the one_of() function do? Why might it be helpful in conjunction with this vector? vars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\") Can be used to select multiple variables with a character vector or to negate selecting certain variables. 4. Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? select(flights, contains(&quot;TIME&quot;)) ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # ... with 336,766 more rows, and 1 more variable: time_hour &lt;dttm&gt; Default is case insensitive, to change this specify ignore.case = FALSE select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) ## # A tibble: 336,776 x 0 5.5: Add new vars Check-out different rank functions x &lt;- c(1, 2, 3, 4, 4, 6, 7, 8, 8, 10) min_rank(x) ## [1] 1 2 3 4 4 6 7 8 8 10 dense_rank(x) ## [1] 1 2 3 4 4 5 6 7 7 8 percent_rank(x) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.3333333 0.5555556 0.6666667 ## [8] 0.7777778 0.7777778 1.0000000 cume_dist(x) ## [1] 0.1 0.2 0.3 0.5 0.5 0.6 0.7 0.9 0.9 1.0 5.5.2. 1. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. time_to_mins &lt;- function(x) (60*(x %/% 100) + (x %% 100)) flights_new &lt;- mutate(flights, DepTime_MinsToMid = time_to_mins(dep_time), #same thing as above, but without calling custom function DepTime_MinsToMid_copy = (60*(dep_time %/% 100) + (dep_time %% 100)), SchedDepTime_MinsToMid = time_to_mins(sched_dep_time)) 2. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? You would expect that: \\(air\\_time = dep\\_time - arr\\_time\\) However this does not seem to be the case when you look at air_time generally… see 5.5.2.2. for more details. 3. Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? You would expect that: \\(dep\\_delay = dep\\_time - sched\\_dep\\_time\\) . Let’s see if this is the case by creating a var dep_delay2 that uses this definition, then see if it is equal to the original dep_delay ##maybe a couple off, but for the most part seems consistent mutate(flights, dep_delay2 = time_to_mins(dep_time) - time_to_mins(sched_dep_time), dep_same = dep_delay == dep_delay2) %&gt;% count(dep_same) ## # A tibble: 3 x 2 ## dep_same n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 1207 ## 2 TRUE 327314 ## 3 NA 8255 Seems generally to align (with dep_delay). Those that are inconsistent are when the delay bleeds into the next day, indicating a problem with my equation above, not the dep_delay value as you can see below. mutate(flights, dep_delay2 = time_to_mins(dep_time) - time_to_mins(sched_dep_time), dep_same = dep_delay == dep_delay2) %&gt;% filter(!dep_same) %&gt;% glimpse() ## Observations: 1,207 ## Variables: 21 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ day &lt;int&gt; 1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 6, 7, 9, 9, 9, 10... ## $ dep_time &lt;int&gt; 848, 42, 126, 32, 50, 235, 25, 106, 14, 37, 16,... ## $ sched_dep_time &lt;int&gt; 1835, 2359, 2250, 2359, 2145, 2359, 2359, 2245,... ## $ dep_delay &lt;dbl&gt; 853, 43, 156, 33, 185, 156, 26, 141, 15, 127, 1... ## $ arr_time &lt;int&gt; 1001, 518, 233, 504, 203, 700, 505, 201, 503, 3... ## $ sched_arr_time &lt;int&gt; 1950, 442, 2359, 442, 2311, 437, 442, 2356, 445... ## $ arr_delay &lt;dbl&gt; 851, 36, 154, 22, 172, 143, 23, 125, 18, 130, 9... ## $ carrier &lt;chr&gt; &quot;MQ&quot;, &quot;B6&quot;, &quot;B6&quot;, &quot;B6&quot;, &quot;B6&quot;, &quot;B6&quot;, &quot;B6&quot;, &quot;B6&quot;,... ## $ flight &lt;int&gt; 3944, 707, 22, 707, 104, 727, 707, 608, 739, 11... ## $ tailnum &lt;chr&gt; &quot;N942MQ&quot;, &quot;N580JB&quot;, &quot;N636JB&quot;, &quot;N763JB&quot;, &quot;N329JB... ## $ origin &lt;chr&gt; &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;JFK&quot;... ## $ dest &lt;chr&gt; &quot;BWI&quot;, &quot;SJU&quot;, &quot;SYR&quot;, &quot;SJU&quot;, &quot;BUF&quot;, &quot;BQN&quot;, &quot;SJU&quot;... ## $ air_time &lt;dbl&gt; 41, 189, 49, 193, 58, 186, 194, 44, 201, 163, 1... ## $ distance &lt;dbl&gt; 184, 1598, 209, 1598, 301, 1576, 1598, 273, 161... ## $ hour &lt;dbl&gt; 18, 23, 22, 23, 21, 23, 23, 22, 23, 22, 23, 23,... ## $ minute &lt;dbl&gt; 35, 59, 50, 59, 45, 59, 59, 45, 59, 30, 59, 59,... ## $ time_hour &lt;dttm&gt; 2013-01-01 18:00:00, 2013-01-02 23:00:00, 2013... ## $ dep_delay2 &lt;dbl&gt; -587, -1397, -1284, -1407, -1255, -1284, -1414,... ## $ dep_same &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE... 4. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank(). mutate(flights, rank_delay = min_rank(-arr_delay)) %&gt;% arrange(rank_delay) %&gt;% filter(rank_delay &lt;= 10) %&gt;% select(flight, sched_dep_time, arr_delay, rank_delay) ## # A tibble: 10 x 4 ## flight sched_dep_time arr_delay rank_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 51 900 1272 1 ## 2 3535 1935 1127 2 ## 3 3695 1635 1109 3 ## 4 177 1845 1007 4 ## 5 3075 1600 989 5 ## 6 2391 1900 931 6 ## 7 2119 810 915 7 ## 8 2047 759 895 8 ## 9 172 1700 878 9 ## 10 3744 2055 875 10 5. What does 1:3 + 1:10 return? Why? 1:3 + 1:10 ## Warning in 1:3 + 1:10: longer object length is not a multiple of shorter ## object length ## [1] 2 4 6 5 7 9 8 10 12 11 This is returned because 1:3 is being recycled as each element is added to an element in 1:10. 6. What trigonometric functions does R provide? ?sin 5.6: Grouped summaries… not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) not_cancelled %&gt;% select(year, month, day, dep_time) %&gt;% group_by(year, month, day) %&gt;% mutate(r = min_rank(desc(dep_time))) %&gt;% mutate(range_min = range(r)[1], range_max = range(r)[2]) %&gt;% filter(r %in% range(r)) 5.6.7. 1. Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. 90th percentile for delays for flights by destination flights %&gt;% group_by(dest) %&gt;% summarise(delay.90 = quantile(arr_delay, 0.90, na.rm = TRUE)) %&gt;% arrange(desc(delay.90)) ## # A tibble: 105 x 2 ## dest delay.90 ## &lt;chr&gt; &lt;dbl&gt; ## 1 TUL 126 ## 2 TYS 109. ## 3 CAE 107 ## 4 DSM 103 ## 5 OKC 99.6 ## 6 BHM 99.2 ## 7 RIC 90 ## 8 PVD 81.3 ## 9 CRW 80.8 ## 10 CVG 80 ## # ... with 95 more rows average dep_delay by hour of day flights %&gt;% group_by(hour) %&gt;% summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = hour, y = avg_delay))+ geom_point()+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). Percentage of flights delayed or canceled by origin flights %&gt;% group_by(origin) %&gt;% summarise(num_delayed = sum(arr_delay &gt; 0, na.rm = TRUE)/n()) ## # A tibble: 3 x 2 ## origin num_delayed ## &lt;chr&gt; &lt;dbl&gt; ## 1 EWR 0.415 ## 2 JFK 0.385 ## 3 LGA 0.382 Percentage of flights canceled by airline (technically not delays, but cancellations…) flights %&gt;% group_by(carrier) %&gt;% summarise(perc_canceled = sum(is.na(arr_delay))/n(), n = n()) %&gt;% ungroup() %&gt;% filter(n &gt;= 1000) %&gt;% mutate(most_rank = min_rank(-perc_canceled)) %&gt;% arrange(most_rank) ## # A tibble: 11 x 4 ## carrier perc_canceled n most_rank ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 9E 0.0632 18460 1 ## 2 EV 0.0566 54173 2 ## 3 MQ 0.0515 26397 3 ## 4 US 0.0343 20536 4 ## 5 FL 0.0261 3260 5 ## 6 AA 0.0239 32729 6 ## 7 WN 0.0188 12275 7 ## 8 UA 0.0151 58665 8 ## 9 B6 0.0107 54635 9 ## 10 DL 0.00940 48110 10 ## 11 VX 0.00891 5162 11 Percentage of flights delayed by airline flights %&gt;% group_by(carrier) %&gt;% summarise(perc_delayed = sum(arr_delay &gt; 0, na.rm = TRUE)/sum(!is.na(arr_delay)), n = n()) %&gt;% ungroup() %&gt;% filter(n &gt;= 1000) %&gt;% mutate(most_rank = min_rank(-perc_delayed)) %&gt;% arrange(most_rank) ## # A tibble: 11 x 4 ## carrier perc_delayed n most_rank ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 FL 0.597 3260 1 ## 2 EV 0.479 54173 2 ## 3 MQ 0.467 26397 3 ## 4 WN 0.440 12275 4 ## 5 B6 0.437 54635 5 ## 6 UA 0.385 58665 6 ## 7 9E 0.384 18460 7 ## 8 US 0.371 20536 8 ## 9 DL 0.344 48110 9 ## 10 VX 0.341 5162 10 ## 11 AA 0.335 32729 11 Consider the following scenarios: 1.1 A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. flights %&gt;% group_by(flight) %&gt;% # filter(!is.na(arr_delay)) %&gt;% ##Keeping this in would exclude the possibility of canceled summarise(early.15 = sum(arr_delay &lt;= -15, na.rm = TRUE)/n(), late.15 = sum(arr_delay &gt;= 15, na.rm = TRUE)/n(), n = n()) %&gt;% ungroup() %&gt;% filter(early.15 == .5, late.15 == .5) ## # A tibble: 18 x 4 ## flight early.15 late.15 n ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 107 0.5 0.5 2 ## 2 2072 0.5 0.5 2 ## 3 2366 0.5 0.5 2 ## 4 2500 0.5 0.5 2 ## 5 2552 0.5 0.5 2 ## 6 3495 0.5 0.5 2 ## 7 3518 0.5 0.5 2 ## 8 3544 0.5 0.5 2 ## 9 3651 0.5 0.5 2 ## 10 3705 0.5 0.5 2 ## 11 3916 0.5 0.5 2 ## 12 3951 0.5 0.5 2 ## 13 4273 0.5 0.5 2 ## 14 4313 0.5 0.5 2 ## 15 5297 0.5 0.5 2 ## 16 5322 0.5 0.5 2 ## 17 5388 0.5 0.5 2 ## 18 5505 0.5 0.5 4 1.2 A flight is always 10 minutes late. flights %&gt;% group_by(flight) %&gt;% summarise(late.10 = sum(arr_delay &gt;= 10)/n()) %&gt;% ungroup() %&gt;% filter(late.10 == 1) ## # A tibble: 93 x 2 ## flight late.10 ## &lt;int&gt; &lt;dbl&gt; ## 1 94 1 ## 2 730 1 ## 3 974 1 ## 4 1084 1 ## 5 1226 1 ## 6 1510 1 ## 7 1514 1 ## 8 1859 1 ## 9 1868 1 ## 10 2101 1 ## # ... with 83 more rows 1.3 A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. flights %&gt;% group_by(flight) %&gt;% # filter(!is.na(arr_delay)) %&gt;% ##Keeping this in would exclude the possibility of canceled summarise(early.30 = sum(arr_delay &lt;= -30, na.rm = TRUE)/n(), late.30 = sum(arr_delay &gt;= 30, na.rm = TRUE)/n(), n = n()) %&gt;% ungroup() %&gt;% filter(early.30 == .5, late.30 == .5) ## # A tibble: 3 x 4 ## flight early.30 late.30 n ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 3651 0.5 0.5 2 ## 2 3916 0.5 0.5 2 ## 3 3951 0.5 0.5 2 1.4 99% of the time a flight is on time. 1% of the time it’s 2 hours late. flights %&gt;% group_by(flight) %&gt;% # filter(!is.na(arr_delay)) %&gt;% ##Keeping this in would exclude the possibility of canceled summarise(ontime = sum(arr_delay &lt;= 0, na.rm = TRUE)/n(), late.120 = sum(arr_delay &gt;= 120, na.rm = TRUE)/n(), n = n()) %&gt;% ungroup() %&gt;% filter(ontime == .99, late.120 == .01) ## # A tibble: 0 x 4 ## # ... with 4 variables: flight &lt;int&gt;, ontime &lt;dbl&gt;, late.120 &lt;dbl&gt;, ## # n &lt;int&gt; Looks like this exact proportion doesn’t happen. Let’s change this to be &gt;= 99% and &lt;= 1%. flights %&gt;% group_by(flight) %&gt;% # filter(!is.na(arr_delay)) %&gt;% ##Keeping this in would exclude the possibility of canceled summarise(ontime = sum(arr_delay &lt;= 0, na.rm = TRUE)/n(), late.120 = sum(arr_delay &gt;= 120, na.rm = TRUE)/n(), n = n()) %&gt;% ungroup() %&gt;% filter(ontime &gt;= .99, late.120 &lt;= .01) ## # A tibble: 391 x 4 ## flight ontime late.120 n ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 46 1 0 2 ## 2 52 1 0 2 ## 3 88 1 0 1 ## 4 90 1 0 1 ## 5 96 1 0 1 ## 6 99 1 0 1 ## 7 106 1 0 1 ## 8 122 1 0 1 ## 9 174 1 0 1 ## 10 202 1 0 5 ## # ... with 381 more rows 2. Which is more important: arrival delay or departure delay? Arrival delay. 3. Come up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()). not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) not_cancelled %&gt;% group_by(dest) %&gt;% summarise(n = n()) ## # A tibble: 104 x 2 ## dest n ## &lt;chr&gt; &lt;int&gt; ## 1 ABQ 254 ## 2 ACK 264 ## 3 ALB 418 ## 4 ANC 8 ## 5 ATL 16837 ## 6 AUS 2411 ## 7 AVL 261 ## 8 BDL 412 ## 9 BGR 358 ## 10 BHM 269 ## # ... with 94 more rows not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise(n = sum(distance)) ## # A tibble: 4,037 x 2 ## tailnum n ## &lt;chr&gt; &lt;dbl&gt; ## 1 D942DN 3418 ## 2 N0EGMQ 239143 ## 3 N10156 109664 ## 4 N102UW 25722 ## 5 N103US 24619 ## 6 N104UW 24616 ## 7 N10575 139903 ## 8 N105UW 23618 ## 9 N107US 21677 ## 10 N108UW 32070 ## # ... with 4,027 more rows 4. Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay)) is slightly suboptimal. Why? Which is the most important column? You only need the is.na(arr_delay) column. By having both, it is doing more checks then is necessary. (While not a perfect method) you can see that the number of rows with just is.na(arr_delay) would be the same in either case. filter(flights, is.na(dep_delay) | is.na(arr_delay)) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 9430 filter(flights, is.na(arr_delay)) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 9430 To be more precise, you could check these with the identical function. check_1 &lt;- filter(flights, is.na(dep_delay) | is.na(arr_delay)) check_2 &lt;- filter(flights, is.na(arr_delay)) identical(check_1, check_2) ## [1] TRUE 5. Look at the number of cancelled flights per day. Is there a pattern? Number of canceled flights by day of month: flights %&gt;% group_by(day) %&gt;% summarise(num = n(), cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), cancelled_perc = cancelled / num) %&gt;% ggplot(aes(x = day, y = cancelled))+ geom_line() Some days of the month have more cancellations Is the proportion of cancelled flights related to the average delay? Proporton of canceled flights and then average delay of flights by day: flights %&gt;% group_by(day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% ggplot(aes(x = day, y = cancelled_perc))+ geom_line() flights %&gt;% group_by(day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% ggplot(aes(x = day, y = avg_delayed))+ geom_line() Looks roughly like there is some overlap. Plot, treating day independently: flights %&gt;% group_by(day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% ggplot(aes(x = cancelled_perc, y = avg_delayed))+ geom_point()+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; suggests positive association 6. Which carrier has the worst delays? flights %&gt;% group_by(carrier) %&gt;% summarise(avg_delay = mean(arr_delay, na.rm = TRUE), n = n()) %&gt;% arrange(desc(avg_delay)) ## # A tibble: 16 x 3 ## carrier avg_delay n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 F9 21.9 685 ## 2 FL 20.1 3260 ## 3 EV 15.8 54173 ## 4 YV 15.6 601 ## 5 OO 11.9 32 ## 6 MQ 10.8 26397 ## 7 WN 9.65 12275 ## 8 B6 9.46 54635 ## 9 9E 7.38 18460 ## 10 UA 3.56 58665 ## 11 US 2.13 20536 ## 12 VX 1.76 5162 ## 13 DL 1.64 48110 ## 14 AA 0.364 32729 ## 15 HA -6.92 342 ## 16 AS -9.93 714 Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %&gt;% group_by(carrier, dest) %&gt;% summarise(n())) Somewhat difficult to untangle in the origin airports because carriers may predominantly go through one of the three. The code below produces the origin name that the carrier that flies from the most along with the proportion of associated flights. flights %&gt;% group_by(carrier, origin) %&gt;% summarise(n = n()) %&gt;% mutate(perc = n / sum(n)) %&gt;% group_by(carrier) %&gt;% mutate(rank = min_rank(-perc)) %&gt;% arrange(carrier, rank) %&gt;% filter(rank == 1) %&gt;% select(carrier, highest_origin = origin, highest_prop = perc, n_total = n) %&gt;% arrange(desc(n_total)) ## # A tibble: 16 x 4 ## # Groups: carrier [16] ## carrier highest_origin highest_prop n_total ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 UA EWR 0.786 46087 ## 2 EV EWR 0.811 43939 ## 3 B6 JFK 0.770 42076 ## 4 DL LGA 0.479 23067 ## 5 MQ LGA 0.641 16928 ## 6 AA LGA 0.472 15459 ## 7 9E JFK 0.794 14651 ## 8 US LGA 0.640 13136 ## 9 WN EWR 0.504 6188 ## 10 VX JFK 0.697 3596 ## 11 FL LGA 1 3260 ## 12 AS EWR 1 714 ## 13 F9 LGA 1 685 ## 14 YV LGA 1 601 ## 15 HA JFK 1 342 ## 16 OO LGA 0.812 26 Below we look at destinations and the carrier that has the highest proportion of flights from one of the NYC destinations (ignoring for specific origin – JFK, LGA, etc. are not seperated). flights %&gt;% group_by(dest, carrier) %&gt;% summarise(n = n()) %&gt;% mutate(perc = n / sum(n)) %&gt;% group_by(dest) %&gt;% mutate(rank = min_rank(-perc)) %&gt;% arrange(carrier, rank) %&gt;% filter(rank == 1) %&gt;% select(dest, highest_carrier = carrier, highest_perc = perc, n_total = n) %&gt;% arrange(desc(n_total)) ## # A tibble: 105 x 4 ## # Groups: dest [105] ## dest highest_carrier highest_perc n_total ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ATL DL 0.614 10571 ## 2 CLT US 0.614 8632 ## 3 DFW AA 0.831 7257 ## 4 MIA AA 0.617 7234 ## 5 ORD UA 0.404 6984 ## 6 IAH UA 0.962 6924 ## 7 SFO UA 0.512 6819 ## 8 FLL B6 0.544 6563 ## 9 MCO B6 0.460 6472 ## 10 LAX UA 0.360 5823 ## # ... with 95 more rows To get at the question of ‘best carrier’, you may consider doing a grouped comparison of average delays or cancellataions controlling for where they are flying to and from what origin… Or build a linear model with the formula, arr_delay ~ carrier + dest + origin. 7. What does the sort argument to count() do. When might you use it? sort orders by n, you may want to use it when you want to see the highest frequency levels. 5.7: Grouped mutates… 5.7.1. 1. Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping. Performs operations on vectors for each group (rather than all together). 2. Which plane (tailnum) has the worst on-time record? flights %&gt;% group_by(tailnum) %&gt;% summarise(n = n(), num_not_delayed = sum(arr_delay &lt;= 0, na.rm = TRUE), ontime_rate = num_not_delayed/ n, sum_delayed_time_grt0 = sum(ifelse(arr_delay &gt;= 0, arr_delay, 0), na.rm = TRUE)) %&gt;% filter(n &gt; 100, !is.na(tailnum)) %&gt;% arrange(ontime_rate) ## # A tibble: 1,200 x 5 ## tailnum n num_not_delayed ontime_rate sum_delayed_time_grt0 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 N505MQ 242 83 0.343 5911 ## 2 N15910 280 105 0.375 8737 ## 3 N36915 228 86 0.377 6392 ## 4 N16919 251 96 0.382 7955 ## 5 N14998 230 88 0.383 7166 ## 6 N14953 256 100 0.391 6550 ## 7 N22971 230 90 0.391 6547 ## 8 N503MQ 191 75 0.393 4420 ## 9 N27152 109 43 0.394 2058 ## 10 N31131 109 43 0.394 2740 ## # ... with 1,190 more rows N505MQ 3. What time of day should you fly if you want to avoid delays as much as possible? average dep_delay by hour of day flights %&gt;% group_by(hour) %&gt;% summarise(med_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = hour, y = med_delay))+ geom_point()+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). Fly in the morning. 4. For each destination, compute the total minutes of delay. For each, flight, compute the proportion of the total delay for its destination. flights %&gt;% filter(arr_delay &gt; 0) %&gt;% group_by(dest, flight) %&gt;% summarise(TotalDelay_DestFlight = sum(arr_delay, na.rm = TRUE)) %&gt;% mutate(TotalDelay_Dest = sum(TotalDelay_DestFlight), PropOfDest = TotalDelay_DestFlight / TotalDelay_Dest) ## # A tibble: 8,505 x 5 ## # Groups: dest [103] ## dest flight TotalDelay_DestFlight TotalDelay_Dest PropOfDest ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 65 1943 4487 0.433 ## 2 ABQ 1505 2544 4487 0.567 ## 3 ACK 1191 1413 2974 0.475 ## 4 ACK 1195 62 2974 0.0208 ## 5 ACK 1291 267 2974 0.0898 ## 6 ACK 1491 1232 2974 0.414 ## 7 ALB 3260 111 9580 0.0116 ## 8 ALB 3264 4 9580 0.000418 ## 9 ALB 3811 599 9580 0.0625 ## 10 ALB 3817 196 9580 0.0205 ## # ... with 8,495 more rows I did this such that flights could not have “negative” delays, this could have been approached differently such that “early arrivals” also got credit… 5. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() explore how the delay of a flight is related to the delay of the immediately preceding flight. flights %&gt;% group_by(origin) %&gt;% mutate(delay_lag = lag(dep_delay, 1), diff_lag = abs(dep_delay -delay_lag)) %&gt;% ungroup() %&gt;% select(dep_delay, delay_lag) %&gt;% na.omit() %&gt;% cor() ## dep_delay delay_lag ## dep_delay 1.0000000 0.3506866 ## delay_lag 0.3506866 1.0000000 Correlation of dep_delayt-1 with dep_delayt is 0.35. Below is a function to get the correlation out for any lag level. cor_by_lag &lt;- function(lag){ flights %&gt;% group_by(origin) %&gt;% mutate(delay_lag = lag(dep_delay, lag), diff_lag = abs(dep_delay -delay_lag)) %&gt;% ungroup() %&gt;% select(dep_delay, delay_lag) %&gt;% na.omit() %&gt;% cor() %&gt;% .[2,1] %&gt;% as.vector() } Let’s see the correlation pushing the lag time back. cor_by_lag(1) ## [1] 0.3506866 cor_by_lag(10) ## [1] 0.2622796 cor_by_lag(100) ## [1] 0.04023232 It makes sense that these values get smaller as flights that are further apart have delay lengths that are less correlated. See 5.7.1.8. for the outputs if iterating this function across many lags. 6. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? flights %&gt;% filter(!is.na(arr_delay)) %&gt;% group_by(dest) %&gt;% mutate(sd_air_time = sd(air_time), mean_air_time = mean(air_time)) %&gt;% ungroup() %&gt;% mutate(supect_fast_cutoff = mean_air_time - 4*sd_air_time, suspect_flag = air_time &lt; supect_fast_cutoff) %&gt;% select(dest, flight, hour, day, month, air_time, sd_air_time, mean_air_time, supect_fast_cutoff, suspect_flag, air_time, air_time) %&gt;% filter(suspect_flag) ## # A tibble: 4 x 10 ## dest flight hour day month air_time sd_air_time mean_air_time ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BNA 3805 19 23 3 70 11.0 114. ## 2 GSP 4292 20 13 5 55 8.13 93.4 ## 3 ATL 1499 17 25 5 65 9.81 113. ## 4 MSP 4667 15 2 7 93 11.8 151. ## # ... with 2 more variables: supect_fast_cutoff &lt;dbl&gt;, suspect_flag &lt;lgl&gt; 7. Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. I found this quesiton ambiguous in terms of what it wants when it says “rank” the carriers using this. What I did was filter to just those destinations that have at least two carriers and then count the number of destinations with multiple carriers that each airline travels to. So it’s almost which airlines have more routes to ‘crowded’ destinations. flights %&gt;% group_by(dest) %&gt;% mutate(n_carrier = n_distinct(carrier)) %&gt;% filter(n_carrier &gt; 1) %&gt;% group_by(carrier) %&gt;% summarise(n_dest = n_distinct(dest)) %&gt;% mutate(rank = min_rank(-n_dest)) %&gt;% arrange(rank) ## # A tibble: 16 x 3 ## carrier n_dest rank ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 EV 51 1 ## 2 9E 48 2 ## 3 UA 42 3 ## 4 DL 39 4 ## 5 B6 35 5 ## 6 AA 19 6 ## 7 MQ 19 6 ## 8 WN 10 8 ## 9 OO 5 9 ## 10 US 5 9 ## 11 VX 4 11 ## 12 YV 3 12 ## 13 FL 2 13 ## 14 AS 1 14 ## 15 F9 1 14 ## 16 HA 1 14 Another way to approach this may have been to say to evaluate the delays between carriers going to the same destination and used that as a way of comparing and ‘ranking’ the best carriers. This would have been a more ambitious problem to answer. 8. For each plane, count the number of flights before the first delay of greater than 1 hour. tail_nums_counts &lt;- flights %&gt;% arrange(tailnum, month, day, dep_time) %&gt;% group_by(tailnum) %&gt;% mutate(cum_sum = cumsum(arr_delay &lt;= 60), nrow = row_number(), nrow_equal = nrow == cum_sum, cum_sum_before = cum_sum * nrow_equal) %&gt;% mutate(total_before_hour = max(cum_sum_before, na.rm = TRUE)) %&gt;% select(year, month, day, dep_time, tailnum, arr_delay, cum_sum, nrow, nrow_equal, cum_sum_before, total_before_hour) %&gt;% ungroup() #let&#39;s change this to get rid of canceled flights, because those don&#39;t count as flights or delays. tail_nums_counts &lt;- flights %&gt;% filter(!is.na(arr_delay)) %&gt;% select(tailnum, month, day, dep_time, arr_delay) %&gt;% arrange(tailnum, month, day, dep_time) %&gt;% group_by(tailnum) %&gt;% mutate(cum_sum = cumsum(arr_delay &lt;= 60), nrow = row_number(), nrow_equal = nrow == cum_sum, cum_sum_before = cum_sum * nrow_equal) %&gt;% mutate(total_before_hour = max(cum_sum_before, na.rm = TRUE)) %&gt;% select(month, day, dep_time, tailnum, arr_delay, cum_sum, nrow, nrow_equal, cum_sum_before, total_before_hour) %&gt;% ungroup() tail_nums_counts %&gt;% filter(!is.na(tailnum)) %&gt;% arrange(desc(nrow), tailnum) %&gt;% distinct(tailnum, .keep_all = TRUE) %&gt;% select(tailnum, total_before_hour) %&gt;% arrange(tailnum) ## # A tibble: 4,037 x 2 ## tailnum total_before_hour ## &lt;chr&gt; &lt;dbl&gt; ## 1 D942DN 0 ## 2 N0EGMQ 0 ## 3 N10156 9 ## 4 N102UW 25 ## 5 N103US 46 ## 6 N104UW 3 ## 7 N10575 0 ## 8 N105UW 22 ## 9 N107US 20 ## 10 N108UW 36 ## # ... with 4,027 more rows Appendix 5.4.1.3. You can also use one_of() for negating specific columns fields by name. select(flights, -one_of(vars)) ## # A tibble: 336,776 x 15 ## year month day sched_dep_time sched_arr_time carrier flight tailnum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 2013 1 1 515 819 UA 1545 N14228 ## 2 2013 1 1 529 830 UA 1714 N24211 ## 3 2013 1 1 540 850 AA 1141 N619AA ## 4 2013 1 1 545 1022 B6 725 N804JB ## 5 2013 1 1 600 837 DL 461 N668DN ## 6 2013 1 1 558 728 UA 1696 N39463 ## 7 2013 1 1 600 854 B6 507 N516JB ## 8 2013 1 1 600 723 EV 5708 N829AS ## 9 2013 1 1 600 846 B6 79 N593JB ## 10 2013 1 1 600 745 AA 301 N3ALAA ## # ... with 336,766 more rows, and 7 more variables: origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; 5.5.2.1. Other, more sophisticated method10 mutate_at(.tbl = flights, .vars = c(&quot;dep_time&quot;, &quot;sched_dep_time&quot;), .funs = funs(new = time_to_mins)) 5.5.2.2. Let’s create this variable. I’ll name it air_calc. First method: flights_new2 &lt;- mutate(flights, # This air_time_clac step is necessary because you need to take into account red-eye flights in calculation air_time_calc = ifelse(dep_time &gt; arr_time, arr_time + 2400, arr_time), air_calc = time_to_mins(air_time_calc) - time_to_mins(dep_time)) The above method is the simple approach, though it doesn’t take into account the timezone of the arrivals locations. To handle this, I do a left_join on the airports dataframe and change arr_time to take into account the timezone and output the value in EST (as opposed to local time). We have not learned about ‘joins’ yet, so don’t worry if this loses you. flights_new2 &lt;- flights %&gt;% left_join(select(nycflights13::airports, dest = faa, tz)) %&gt;% mutate(arr_time_old = arr_time) %&gt;% mutate(arr_time = arr_time - 100*(tz+5)) %&gt;% mutate( # This arr_time_calc step is a helper variable I created to take into account the red-eye flights in calculation arr_time_calc = ifelse(dep_time &gt; arr_time, arr_time + 2400, arr_time), air_calc = time_to_mins(arr_time_calc) - time_to_mins(dep_time)) %&gt;% select(-arr_time_calc) ## Joining, by = &quot;dest&quot; Curiouis if anyone explored the air_time variable and figured out the details of how exactly it was off if there was something systematic? I checked this briefly below, but did not go deep. Closer look at air_time Wanted to look at original air_time variable a little more. Histogram below shows that most differences are now between 20 - 40 minutes from the actual time. flights_new2 %&gt;% group_by(dest) %&gt;% summarise(distance_med = median(distance, na.rm = TRUE), air_calc_med = median(air_calc, na.rm = TRUE), air_old_med = median(air_time, na.rm = TRUE), diff_new_old = air_calc_med - air_old_med, diff_hrs = as.factor(round(diff_new_old/60)), num = n()) %&gt;% ggplot(aes(diff_new_old))+ geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 5 rows containing non-finite values (stat_bin). Regressing diff on arr_delay and dep_delay (remember diff is the difference between air_time and air_calc) mod_air_time2 &lt;- mutate(flights_new2, diff = (air_time - air_calc)) %&gt;% select(-air_time, -air_calc, -flight, -tailnum, -dest) %&gt;% na.omit() %&gt;% lm(diff ~ dep_delay + arr_delay, data = .) summary(mod_air_time2) ## ## Call: ## lm(formula = diff ~ dep_delay + arr_delay, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -93.168 -6.684 0.688 6.878 101.169 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -33.511843 0.024118 -1389.5 &lt;2e-16 *** ## dep_delay 0.533376 0.001355 393.5 &lt;2e-16 *** ## arr_delay -0.552852 0.001217 -454.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.43 on 319806 degrees of freedom ## Multiple R-squared: 0.3956, Adjusted R-squared: 0.3956 ## F-statistic: 1.047e+05 on 2 and 319806 DF, p-value: &lt; 2.2e-16 Doing such accounts for ~40% of the variation in the values. note dep_delay and arr_delay variables are highly colinear – and the coefficients are opposite in the model. flights_new2 %&gt;% select(air_time, air_calc, arr_delay, dep_delay) %&gt;% mutate(diff = air_time - air_calc) %&gt;% select(-air_time, -air_calc) %&gt;% na.omit() %&gt;% cor() ## arr_delay dep_delay diff ## arr_delay 1.0000000 0.91531953 -0.32086698 ## dep_delay 0.9153195 1.00000000 -0.07582942 ## diff -0.3208670 -0.07582942 1.00000000 Often this suggests you may not need to include both variables in the model as they will likely be providing the same information. Though here that is not the case as only including arr_delay associates with a steep decline in R^2 to just account for ~10% of the variation. mod_air_time &lt;- mutate(flights_new2, diff = (air_time - air_calc)) %&gt;% select(-air_time, -air_calc, -flight, -tailnum, -dest) %&gt;% na.omit() %&gt;% lm(diff ~ arr_delay, data = .) summary(mod_air_time) ## ## Call: ## lm(formula = diff ~ arr_delay, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -182.960 -6.385 2.013 7.983 154.382 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.984e+01 2.710e-02 -1101.3 &lt;2e-16 *** ## arr_delay -1.144e-01 5.972e-04 -191.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.14 on 319807 degrees of freedom ## Multiple R-squared: 0.103, Adjusted R-squared: 0.103 ## F-statistic: 3.67e+04 on 1 and 319807 DF, p-value: &lt; 2.2e-16 5.6.7.1. Below is an extension on using the quantile method, but it is far beyond where we are right now. For the question 90th percentile for delays for flights by destination we used quantile to output only the 90th percentile of values for each destination. Here, I want to address what if you had wanted to output the delays at multiple values, say, arbitrarily the 25th, 50th, 75th percentiles. One option would be to create a new variable for each value and in each quantile function sepcify 0.25, 0.50, 0.75 respectively. flights %&gt;% group_by(dest) %&gt;% summarise(delay.25 = quantile(arr_delay, 0.25, na.rm = TRUE), delay.50 = quantile(arr_delay, 0.50, na.rm = TRUE), delay.75 = quantile(arr_delay, 0.75, na.rm = TRUE)) ## # A tibble: 105 x 4 ## dest delay.25 delay.50 delay.75 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ -24 -5.5 22.8 ## 2 ACK -13 -3 10 ## 3 ALB -17 -4 28 ## 4 ANC -10.8 1.5 10 ## 5 ATL -12 -1 16 ## 6 AUS -19 -5 15 ## 7 AVL -11 -1 13 ## 8 BDL -18 -10 14 ## 9 BGR -21.8 -9 19.8 ## 10 BHM -20 -2 34 ## # ... with 95 more rows But there is a lot of replication here and the quantile function is also able to output more than one value by specifying the probs argument. quantile(c(1:100), probs = c(0.25, .50, 0.75)) ## 25% 50% 75% ## 25.75 50.50 75.25 So, in theory, rather than calling quantile multiple times, you could just call it once. However for any variable you create summarise is expecting only a single value output for each row, so just passing it in as-is will cause it to fail. flights %&gt;% group_by(dest) %&gt;% summarise(delays = quantile(arr_delay, probs = c(0.25, .50, 0.75), na.rm = TRUE)) ## Error: Column `delays` must be length 1 (a summary value), not 3 To make this work you need to make the value a list, so that it will output a single list in each row of the column[This style is covered at the end of the book in the section ‘list-columns’ in iteration.][Also you need your dataframe to be in a tibble form rather than traditional dataframes for list-cols to work]. I am going to create another list-column field of the quantiles I specified. prob_vals &lt;- seq(from = 0.25, to = 0.75, by = 0.25) flights_quantiles &lt;- flights %&gt;% group_by(dest) %&gt;% summarise(delays_val = list(quantile(arr_delay, probs = prob_vals, na.rm = TRUE)), delays_q = list(c(&#39;25th&#39;, &#39;50th&#39;, &#39;75th&#39;))) flights_quantiles ## # A tibble: 105 x 3 ## dest delays_val delays_q ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 ABQ &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 2 ACK &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 3 ALB &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 4 ANC &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 5 ATL &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 6 AUS &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 7 AVL &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 8 BDL &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 9 BGR &lt;dbl [3]&gt; &lt;chr [3]&gt; ## 10 BHM &lt;dbl [3]&gt; &lt;chr [3]&gt; ## # ... with 95 more rows To convert these outputs out of the list-col format, I can use the function unnest. flights_quantiles %&gt;% unnest() ## # A tibble: 315 x 3 ## dest delays_val delays_q ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ABQ -24 25th ## 2 ABQ -5.5 50th ## 3 ABQ 22.8 75th ## 4 ACK -13 25th ## 5 ACK -3 50th ## 6 ACK 10 75th ## 7 ALB -17 25th ## 8 ALB -4 50th ## 9 ALB 28 75th ## 10 ANC -10.8 25th ## # ... with 305 more rows This will output the values as individual rows, repeating the dest value for the length of the list. If I want to spread the delays_quantile values into seperate columns I can use the spread function that is in the tidying R chapter. flights_quantiles %&gt;% unnest() %&gt;% spread(key = delays_q, value = delays_val, sep = &quot;_&quot;) ## # A tibble: 105 x 4 ## dest delays_q_25th delays_q_50th delays_q_75th ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ -24 -5.5 22.8 ## 2 ACK -13 -3 10 ## 3 ALB -17 -4 28 ## 4 ANC -10.8 1.5 10 ## 5 ATL -12 -1 16 ## 6 AUS -19 -5 15 ## 7 AVL -11 -1 13 ## 8 BDL -18 -10 14 ## 9 BGR -21.8 -9 19.8 ## 10 BHM -20 -2 34 ## # ... with 95 more rows Let’s plot our unnested (but not unspread) data to see roughly the distribution of the delays for each destination at our quantiles of interest11. flights_quantiles %&gt;% unnest() %&gt;% # mutate(delays_q = forcats::fct_reorder(f = delays_q, x = delays_val, fun = mean, na.rm = TRUE)) %&gt;% ggplot(aes(x = delays_q, y = delays_val))+ geom_boxplot() ## Warning: Removed 3 rows containing non-finite values (stat_boxplot). It can be a hassle naming the values explicitly. quantile’s default probs argument value is 0, 0.25, 0.5, 0.75, 1. Rather than needing to type the delays_q values list(c('0%', '25%', '50%', '75%', '100%')) you could have generated the values of these names dynamically using the map function in the purrr package (see chapter on iteration) with example for this by passing the names function over each value in delays_val. flights_quantiles2 &lt;- flights %&gt;% group_by(dest) %&gt;% summarise(delays_val = list(quantile(arr_delay, na.rm = TRUE)), delays_q = list(c(&#39;0th&#39;, &#39;25th&#39;, &#39;50th&#39;, &#39;75th&#39;, &#39;100th&#39;))) %&gt;% mutate(delays_q2 = purrr::map(delays_val, names)) flights_quantiles2 ## # A tibble: 105 x 4 ## dest delays_val delays_q delays_q2 ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 ABQ &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 2 ACK &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 3 ALB &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 4 ANC &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 5 ATL &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 6 AUS &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 7 AVL &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 8 BDL &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 9 BGR &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## 10 BHM &lt;dbl [5]&gt; &lt;chr [5]&gt; &lt;chr [5]&gt; ## # ... with 95 more rows And then let’s unnest the data12. flights_quantiles2 %&gt;% unnest() ## # A tibble: 525 x 4 ## dest delays_val delays_q delays_q2 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ABQ -61 0th 0% ## 2 ABQ -24 25th 25% ## 3 ABQ -5.5 50th 50% ## 4 ABQ 22.8 75th 75% ## 5 ABQ 153 100th 100% ## 6 ACK -25 0th 0% ## 7 ACK -13 25th 25% ## 8 ACK -3 50th 50% ## 9 ACK 10 75th 75% ## 10 ACK 221 100th 100% ## # ... with 515 more rows 5.6.7.1.4. But let’s look at those flights that have the greatest differences in proportion on-time vs. 2 hours late while still having values in both categories13. flights %&gt;% group_by(flight) %&gt;% summarise(ontime = sum(arr_delay &lt;= 0, na.rm = TRUE)/n(), late.120 = sum(arr_delay &gt;= 120, na.rm = TRUE)/n(), n = n()) %&gt;% ungroup() %&gt;% filter_at(c(&quot;ontime&quot;, &quot;late.120&quot;), all_vars(. != 0 &amp; . != 1)) %&gt;% mutate(max_dist = abs(ontime - late.120)) %&gt;% arrange(desc(max_dist)) ## # A tibble: 2,098 x 5 ## flight ontime late.120 n max_dist ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 5288 0.927 0.0244 41 0.902 ## 2 2085 0.901 0.00658 152 0.895 ## 3 2174 0.914 0.0286 35 0.886 ## 4 2243 0.9 0.0167 120 0.883 ## 5 2180 0.889 0.0131 153 0.876 ## 6 2118 0.867 0.00699 143 0.860 ## 7 1167 0.864 0.00662 302 0.858 ## 8 3613 0.886 0.0286 35 0.857 ## 9 1772 0.891 0.0364 55 0.855 ## 10 1157 0.847 0.00667 150 0.84 ## # ... with 2,088 more rows 5.6.7.4. To measure the difference in speed you can use the microbenchmark function microbenchmark::microbenchmark(sub_optimal = filter(flights, is.na(dep_delay) | is.na(arr_delay)), optimal = filter(flights, is.na(arr_delay)), times = 10) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## sub_optimal 5.5279 6.2409 6.55796 6.74025 6.9686 7.2225 10 b ## optimal 3.9316 4.3135 4.55498 4.57885 4.8483 5.1514 10 a 5.6.7.5. Explore the percentage delayed vs. percentage cancelled. flights %&gt;% group_by(day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), delayed = sum(arr_delay &gt; 0, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num, delayed_perc = delayed / num) %&gt;% ggplot(aes(x = day))+ geom_line(aes(y = cancelled_perc), colour = &quot;dark blue&quot;)+ geom_line(aes(y = delayed_perc), colour = &quot;dark red&quot;) Let’s try faceting by origin and looking at both values next to each other. flights %&gt;% group_by(origin, day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% gather(key = type, value = value, avg_delayed, cancelled_perc) %&gt;% ggplot(aes(x = day, y = value))+ geom_line()+ facet_grid(type ~ origin, scales = &quot;free_y&quot;) Look’s like the relationship across origins with the delay overlaid with color (not actually crazy about how this look). flights %&gt;% group_by(origin, day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% ggplot(aes(x = day, y = cancelled_perc, colour = avg_delayed))+ geom_line()+ facet_grid(origin ~ .) Let’s look at values as individual points and overlay a geom_smooth flights %&gt;% group_by(origin, day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% ggplot(aes(avg_delayed, cancelled_perc, colour = origin))+ geom_point()+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Modeling approach: We also could approach this using a model and regressing the average proportion of cancelled flights on average delay. cancelled_mod1 &lt;- flights %&gt;% group_by(origin, day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num) %&gt;% lm(cancelled_perc ~ avg_delayed, data = .) summary(cancelled_mod1) ## ## Call: ## lm(formula = cancelled_perc ~ avg_delayed, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.026363 -0.009392 -0.002610 0.006196 0.048436 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0152588 0.0020945 7.285 1.12e-10 *** ## avg_delayed 0.0018688 0.0002311 8.086 2.54e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01342 on 91 degrees of freedom ## Multiple R-squared: 0.4181, Adjusted R-squared: 0.4117 ## F-statistic: 65.39 on 1 and 91 DF, p-value: 2.537e-12 # ggplot(aes(x = day, y = cancelled_perc))+ # geom_line() If you were confused by the . in lm(cancelled_perc ~ avg_delayed, data = .), the dot specifies where the output from the prior steps should be piped into. The default is for it to go into the first argument, but for the lm function, data is not the first argument, so I have to explicitly tell it that the prior steps output should be inputted into the data argument of the lm function. See On piping dots for more details. The average delay accounts for 42% of the variation in the proportion of canceled flights. Modeling the log-odds of the proportion of cancelled flights might be more successful as it produces a variable not constrained by 0 to 1, better aligning with the assumptions of linear regression. cancelled_mod2 &lt;- flights %&gt;% group_by(origin, day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num, cancelled_logodds = log(cancelled / (num - cancelled))) %&gt;% lm(cancelled_logodds ~ avg_delayed, data = .) To convert logodds back to percentage, I built the following equation. convert_logodds &lt;- function(log_odds) exp(log_odds) / (1 + exp(log_odds)) Let’s calculate the MAE or mean absolute error on our percentages. cancelled_preds2 &lt;- flights %&gt;% group_by(origin, day) %&gt;% summarise(cancelled = sum(is.na(arr_delay)), avg_delayed = mean(arr_delay, na.rm = TRUE), num = n(), cancelled_perc = cancelled / num, cancelled_logodds = log(cancelled / (num - cancelled))) %&gt;% ungroup() %&gt;% modelr::spread_predictions(cancelled_mod1, cancelled_mod2) %&gt;% mutate(cancelled_mod2 = convert_logodds(cancelled_mod2)) cancelled_preds2 %&gt;% summarise(MAE1 = mean(abs(cancelled_perc - cancelled_mod1), na.rm = TRUE), MAE2 = mean(abs(cancelled_perc - cancelled_mod2), na.rm = TRUE), mean_value = mean(cancelled_perc, na.rm = TRUE)) ## # A tibble: 1 x 3 ## MAE1 MAE2 mean_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0101 0.00954 0.0279 Let’s look at the differences in the outputs of the predictions from these models. cancelled_preds2 %&gt;% ggplot(aes(avg_delayed, cancelled_perc))+ geom_point()+ scale_size_continuous(range = c(1, 2))+ geom_line(aes(y = cancelled_mod1), colour = &quot;blue&quot;, size = 1)+ geom_line(aes(y = cancelled_mod2), colour = &quot;red&quot;, size = 1) 14 5.6.7.6. As an example, let’s look at just Atl flights from LGA and compare DL, FL, MQ. flights %&gt;% filter(dest == &#39;ATL&#39;, origin == &#39;LGA&#39;) %&gt;% count(carrier) ## # A tibble: 5 x 2 ## carrier n ## &lt;chr&gt; &lt;int&gt; ## 1 DL 5544 ## 2 EV 1 ## 3 FL 2337 ## 4 MQ 2322 ## 5 WN 59 And compare the median delays between the three primary carriers DL, FL, MQ. carriers_lga_atl &lt;- flights %&gt;% filter(dest == &#39;ATL&#39;, origin == &#39;LGA&#39;) %&gt;% group_by(carrier) %&gt;% # filter out small samples mutate(n_tot = n()) %&gt;% filter(n_tot &gt; 100) %&gt;% select(-n_tot) %&gt;% ### filter(!is.na(arr_delay)) %&gt;% ungroup() label &lt;- carriers_lga_atl %&gt;% group_by(carrier) %&gt;% summarise(arr_delay = median(arr_delay, na.rm = TRUE)) carriers_lga_atl %&gt;% select(carrier, arr_delay) %&gt;% ggplot()+ geom_boxplot(aes(carrier, arr_delay, colour = carrier), outlier.shape = NA)+ coord_cartesian(y = c(-60, 75))+ geom_text(mapping = aes(x = carrier, group = carrier, y = arr_delay + 5, label = arr_delay), data = label) Or perhaps you want to use a statistical method to compare if the differences in the grouped are significant… carriers_lga_atl %&gt;% lm(arr_delay ~ carrier, data = .) %&gt;% summary() ## ## Call: ## lm(formula = arr_delay ~ carrier, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -64.74 -22.33 -11.33 4.67 888.67 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.3273 0.6149 10.29 &lt; 2e-16 *** ## carrierFL 14.4172 1.1340 12.71 &lt; 2e-16 *** ## carrierMQ 7.7067 1.1417 6.75 1.56e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 45.48 on 9979 degrees of freedom ## Multiple R-squared: 0.01692, Adjusted R-squared: 0.01672 ## F-statistic: 85.86 on 2 and 9979 DF, p-value: &lt; 2.2e-16 This shows the mean delay for DL is ~6.3, FL is ~20.7, MQ is ~14 and FL and MQ are significantly different from DL (and DL is significantly different from 0)15. The carrier accouts for ~1.6% of the variation in arrival… etc…. 5.7.1.6. Let’s look at the fastest 20 air_times for each destination. flights_new2 %&gt;% group_by(dest) %&gt;% mutate(min_rank = min_rank(air_time)) %&gt;% filter(min_rank &lt; 20) %&gt;% ggplot(aes(distance, air_time, colour = dest))+ geom_point()+ guides(colour = FALSE) Let’s do the same for my custom air_time calculation air_calc. flights_new2 %&gt;% group_by(dest) %&gt;% mutate(min_rank = min_rank(air_calc)) %&gt;% filter(min_rank &lt; 20) %&gt;% ggplot(aes(distance, air_calc, colour = dest))+ geom_point()+ guides(colour = FALSE) Rather than the fastest 20, let’s look at the mean dist and air_time for each16. First using the air_time value. flights_new2 %&gt;% mutate_at(.vars = c(&quot;dep_time&quot;, &quot;arr_time&quot;), .funs = funs(time_to_mins)) %&gt;% group_by(dest) %&gt;% summarise(mean_air = mean(air_time, na.rm = TRUE), mean_dist = mean(distance, na.rm = TRUE)) %&gt;% ggplot(., aes(x = mean_dist, y = mean_air))+ geom_point(aes(colour = dest))+ scale_y_continuous(breaks = seq(0, 660, 60))+ guides(colour = FALSE) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## please use list() instead ## ## # Before: ## funs(name = f(.) ## ## # After: ## list(name = ~f(.)) ## This warning is displayed once per session. ## Warning: Removed 1 rows containing missing values (geom_point). Then with the custom air_calc. flights_new2 %&gt;% mutate_at(.vars = c(&quot;dep_time&quot;, &quot;arr_time&quot;), .funs = funs(time_to_mins)) %&gt;% group_by(dest) %&gt;% summarise(mean_air = mean(air_calc, na.rm = TRUE), mean_dist = mean(distance, na.rm = TRUE)) %&gt;% ggplot(., aes(x = mean_dist, y = mean_air))+ geom_point(aes(colour = dest))+ scale_y_continuous(breaks = seq(0, 660, 60))+ guides(colour = FALSE) ## Warning: Removed 5 rows containing missing values (geom_point). 5.7.1.5 Let’s run this for every 3 lags (1, 4, 7, …) and plot. lags_cors &lt;- tibble(lag = seq(1,200, 3)) %&gt;% mutate(cor = purrr::map_dbl(lag, cor_by_lag)) lags_cors %&gt;% ggplot(aes(x = lag, cor))+ geom_line()+ coord_cartesian(ylim = c(0, 0.40)) 5.7.1.8. tail_nums_counts %&gt;% nest() %&gt;% sample_n(10) %&gt;% unnest() %&gt;% View() On piping dots The . let’s you explicitly state where to pipe the output from the prior steps. The default is to have it go into the first argument of the function. Let’s look at an example: flights %&gt;% filter(!is.na(arr_delay)) %&gt;% count(origin) ## # A tibble: 3 x 2 ## origin n ## &lt;chr&gt; &lt;int&gt; ## 1 EWR 117127 ## 2 JFK 109079 ## 3 LGA 101140 This is the exact same thing as the code below, I just added the dots to be explicit about where in the function the output from the prior steps will go: flights %&gt;% filter(., !is.na(arr_delay)) %&gt;% count(., origin) ## # A tibble: 3 x 2 ## origin n ## &lt;chr&gt; &lt;int&gt; ## 1 EWR 117127 ## 2 JFK 109079 ## 3 LGA 101140 Functions in dplyr, etc. expect dataframes in the first argument, so the default piping behavior works fine you don’t end-up using the dot in this way. However functions outside of the tidyverse are not always so consistent and may expect the dataframe (or w/e your output from the prior step is) in a different location of the function, hence the need to use the dot to specify where it should go. The example below uses base R’s lm (linear models) function to regress arr_delay on dep_delay and distance17. The first argument expects a function, the second argument the data, hence the need for the dot. flights %&gt;% filter(., !is.na(arr_delay)) %&gt;% lm(arr_delay ~ dep_delay + distance, .) ## ## Call: ## lm(formula = arr_delay ~ dep_delay + distance, data = .) ## ## Coefficients: ## (Intercept) dep_delay distance ## -3.212779 1.018077 -0.002551 When using the . in piping, I will usually make the argment name I am piping into explicit. This makes it more clear and also means if I have the position order wrong it doesn’t matter. flights %&gt;% filter(., !is.na(arr_delay)) %&gt;% lm(arr_delay ~ dep_delay + distance, data = .) You can also use the . in conjunction with R’s subsetting to output vectors. In the example below I filter flights, then extract the arr_delay column as a vector and pipe it into the base R function quantile. flights %&gt;% filter(!is.na(arr_delay)) %&gt;% .$arr_delay %&gt;% quantile(probs = seq(from = 0, to = 1, by = 0.10)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## -86 -26 -19 -14 -10 -5 1 9 21 52 1272 quantile is expecting a numeric vector in it’s first argument so the above works. If instead of .$arr_delay, you’d tried select(arr_delay) the function would have failed because the select statement outputs a dataframe rather than a vector (and quantile would have become very angry with you). One weakness with the above method is it only allows you to input a single vector into the base R function (while many funcitons can take in multiple vectors). A better way of doing this is to use the with function. The with function allows you to pipe a dataframe into the first argument and then reference the column in that dataframe with just the field names. This makes using those base R funcitons easier and more similar in syntax to tidyverse functions. For example, the above example would look become. flights %&gt;% filter(!is.na(arr_delay)) %&gt;% with(quantile(arr_delay, probs = seq(from = 0, to = 1, by = 0.10))) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## -86 -26 -19 -14 -10 -5 1 9 21 52 1272 This method also makes it easy to input multiple field names in this style. Let’s look at this with the table function18 flights %&gt;% filter(!is.na(arr_delay)) %&gt;% with(table(origin, carrier)) ## carrier ## origin 9E AA AS B6 DL EV F9 FL HA MQ OO ## EWR 1193 3363 709 6472 4295 41557 0 0 0 2097 6 ## JFK 13742 13600 0 41666 20559 1326 0 0 342 6838 0 ## LGA 2359 14984 0 5911 22804 8225 681 3175 0 16102 23 ## carrier ## origin UA US VX WN YV ## EWR 45501 4326 1552 6056 0 ## JFK 4478 2964 3564 0 0 ## LGA 7803 12541 0 5988 544 plotly The plotly package has a cool function ggplotly that allows you to add wrappers ggplot that turn it into html that allow you to do things like zoom-in and hover over points. It also has a frame argument that allows you to make animations or filter between points. Here is an example from the flights dataset. p &lt;- flights %&gt;% group_by(hour, month) %&gt;% summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = hour, y = avg_delay, group = month, frame = month))+ geom_point()+ geom_smooth() plotly::ggplotly(p) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). This is the base from which this is built. flights %&gt;% group_by(hour, month) %&gt;% summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = hour, y = avg_delay, group = month))+ geom_point()+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). This method is helpful for if you have more than ust a couple variables you are applying a transformation to.↩ The mutate step that is commented-out would reorder the delays_q variable according to the mean value of the delays_val, but this is not necessary here so I commented it out. You will learn more about this in the factors chapter.lm↩ The names assigned by the quantile function are a little different from those I supplied.↩ The output below is actually just maximizing difference in proportion 2 hrs late vs on time, it does not matter whether the higher proportion is on-time or late. It just happens in practice that the higher proprotion is generally the on-time↩ Another approach may be to try and identify the individual risk of having a flight cancelled based on the average delay. If this is the case, you may want to use model evaluation techniques that seperate models based on the assigned probabilities in which case MAE may actually not be the most appropriate evaluation technique. You could try using logistic regression for this. You may also consider taking into account the weight of each of the points. I had discussions on these, but decided they were too in the weeds so deleted them even from the appendix…↩ Repeated t-test methods could be used for comparing MQ and FL, see function pairwise.t.test↩ Each colour corresponds with a dest though I excluded the legend.↩ You may want to add a step to pipe this into summary() after the lm step as well.↩ table produces contingency tables.↩ "],
["07-exploratory-data-analysis.html", "Ch. 7: Data exploration 7.3: Variation 7.4: Missing values 7.5: Covariation Appendix", " Ch. 7: Data exploration Key questions: 7.3.4. #1 7.5.1.1 #2 7.5.3.1. #2, 4 Functions and notes: cut_width: specify binsize of each cut (often use with geom_boxplot) cut_number: specify number of groups to make, allowing for variable binsize (often use with geom_boxplot) geom_histogram: key args are bins, binwidth geom_freqpoly: for if you want to have overlapping histograms (so outputs lines instead) can set y as ..density.. to equalize scale of each (similar to how geom_density does). geom_boxplot: adjust outliers with outlier.colour, outlier.fill, … geom_violin: Creates double sided histograms for each factor of x geom_bin2d: scatter plot of x and y values, but use shading to determine count/density in each point geom_hex: same as geom_bin2d but hexagon instead of square shapes are shaded in reorder: arg1 = variable to reorder, arg2 = variable to reorder it by arg3 = function to reorder by (e.g. median, mean, max…) coord_cartesian: adjust x,y window w/o filtering out values that are excluded from view xlim; ylim: adjust window and filter out values not within window (same method as scale_x(/y)_continuous) these v. coord_cartesian is important for geoms like geom_smooth that aggregate as they visualize ifelse: vectorized if else (not to be confused with if and else functions) dplyr::if_else is more strict alternative case_when: create new variable that relies on complex combination of existing variables often use when you have complex or multiple ifelse statements accruing 7.3: Variation 7.3.4. 1. Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. x has some 0s which signifies a data colletion error, y and z have extreme outliers (z more so). x_hist &lt;- ggplot(diamonds)+ geom_histogram(aes(x = x), binwidth = 0.1)+ coord_cartesian(xlim = c(0, 10)) y_hist &lt;- ggplot(diamonds)+ geom_histogram(aes(x = y), binwidth = 0.1)+ coord_cartesian(xlim = c(0, 10)) z_hist &lt;- ggplot(diamonds)+ geom_histogram(aes(x = z), binwidth = 0.1)+ coord_cartesian(xlim = c(0, 10)) gridExtra::grid.arrange(x_hist, y_hist, z_hist, ncol = 1) All three have peaks and troughs on even points. X and y have more similar distributions than z. I would say that x and y are likely length and width and z depth because diamonds are typically circular on the face so will have the same ratio of length and width and we see this is the case for the x and y dimensions, whereas z tends to be more shallow. diamonds %&gt;% sample_n(1000) %&gt;% ggplot()+ geom_point(aes(x, y))+ coord_fixed() diamonds %&gt;% sample_n(1000) %&gt;% ggplot()+ geom_point(aes(x, z))+ coord_fixed() 2. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.) ggplot(diamonds)+ geom_histogram(aes(x = price), binwidth=10) Price is right skewed. Also notice that from ~1450 to ~1550 there are diamonds. ggplot(diamonds)+ geom_histogram(aes(x = price), binwidth = 5)+coord_cartesian(xlim = c(1400,1600)) 3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? filter(diamonds, carat == 0.99) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 23 filter(diamonds, carat == 1) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1558 For visual scale. ggplot(diamonds)+ geom_histogram(aes(x=carat), binwidth=.01)+ coord_cartesian(xlim=c(.99,1)) The difference may be caused by jewlers rounding-up because people want to buy ‘1’ carat diamonds not 0.99 carat diamonds. It could also be that some listings are simpoly only in integers19. 4.Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows? coord_cartesian does not change data ust window view where as xlim and ylim will get rid of data outside of domain20. 7.4: Missing values 7.4.1. 1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference? With numeric data they both filter out NAs, though for categorical / character variables the barplot will create a seperate olumn with the category. This is because NA can just be thought of as another category though it is difficulty to place it within a distribution of values. Treats these the same. mutate(diamonds, carattest=ifelse(carat&lt;1.5 &amp; carat&gt;.7, NA, carat)) %&gt;% ggplot() + geom_histogram(aes(x=carattest)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 20543 rows containing non-finite values (stat_bin). mutate(diamonds, carattest=ifelse(carat&lt;1.5 &amp; carat&gt;.7, NA, color)) %&gt;% ggplot() + geom_bar(aes(x=carattest)) ## Warning: Removed 20543 rows containing non-finite values (stat_count). For character than it creates a new bar for NAs mutate(diamonds, carattest=ifelse(carat&lt;1.5 &amp; carat&gt;.7, NA, color)) %&gt;% ggplot() + geom_bar(aes(x = as.character(carattest))) 2. What does na.rm = TRUE do in mean() and sum()? Filters it out of the vector of values. 7.5: Covariation 7.5.1.1. 1. Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights. Looks like while non-cancelled flights happen at similar frequency in mornings and evenings, cancelled flights happen at a greater frequency in the evenings. nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot(mapping = aes(x=sched_dep_time, y=..density..)) + geom_freqpoly(mapping = aes(colour = cancelled), binwidth = .25)+ xlim(c(5,25)) ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 4 rows containing missing values (geom_path). Let’s look at the same plot but smooth the distributions to make the pattern easier to see. nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot(mapping = aes(x=sched_dep_time)) + geom_density(mapping = aes(fill = cancelled), alpha = 0.30)+ xlim(c(5,25)) ## Warning: Removed 1 rows containing non-finite values (stat_density). 2.What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? carat is the most important for predicting price. cor(diamonds$price, select(diamonds, carat, depth, table, x, y, z)) ## carat depth table x y z ## [1,] 0.9215913 -0.0106474 0.1271339 0.8844352 0.8654209 0.8612494 fair cut seem to associate with a higher carat thus while lower quality diamonds may be selling for more that is being driven by the carat of the diamond (the most important factor in price) and the quality simply cannot offset this. ggplot(data = diamonds, aes(x = cut, y = carat))+ geom_boxplot()+ coord_flip() 3.Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()? ggplot(diamonds)+ ggstance::geom_boxploth(aes(x = carat, y = cut)) ggplot(diamonds)+ geom_boxplot(aes(x = cut, y = carat))+ coord_flip() Looks like it does the exact same thing as flipping x and y and using coord_flip() 4. One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots? I found this helpful This produces a ‘letter-value’ boxplot which means that in the first box you have the middle ~1/2 of data, then in the adoining boxes the next ~1/4, so within the middle 3 boxes you have the middle ~3/4 of data, next two boxes is ~7/8ths, then ~15/16th etc. set.seed(1234) a &lt;- diamonds %&gt;% ggplot()+ lvplot::geom_lv(aes(x = cut, y = price)) set.seed(1234) b &lt;- diamonds %&gt;% ggplot()+ geom_boxplot(aes(x = cut, y = price)) Perhaps a helpful way to understand this is to see what it looks like at different specified ‘k’ values (which) You can see the letters when you add fill = ..LV.. to the aesthetic. diamonds %&gt;% ggplot()+ lvplot::geom_lv(aes(x = cut, y = price, alpha = ..LV..), fill = &quot;blue&quot;)+ scale_alpha_discrete(range = c(0.7, 0)) ## Warning: Using alpha for a discrete variable is not advised. diamonds %&gt;% ggplot()+ lvplot::geom_lv(aes(x = cut, y = price, fill = ..LV..)) Letters represent ‘median’, ‘fourths’, ‘eights’… 5. Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method? ggplot(diamonds,aes(x = cut, y = carat))+ geom_violin() ggplot(diamonds,aes(colour = cut, x = carat, y = ..density..))+ geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(diamonds, aes(x = carat, y = ..density..))+ geom_histogram()+ facet_wrap(~cut) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. I like how geom_freqpoly has points directly overlaying but it can also be tough to read some, and the lines can overlap and be tough to tell apart, you also have to specify density for this and geom_histogram whereas for geom_violin it is the default. The tails in geom_violin can be easy to read but they also pull these for each of the of the values whereas by faceting geomo_histogram and setting scales = \"free\" you can have independent scales. I think the biggest advantage of the histogram is that it is the most familiar so people will know what you’re looking at. 6. If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does. ggplot(mpg, aes(x = displ, y = cty, color = drv))+ geom_point() ggplot(mpg, aes(x = displ, y = cty, color = drv))+ geom_jitter() ggplot(mpg, aes(x = displ, y = cty, color = drv))+ geom_beeswarm() ## Warning in f(...): The default behavior of beeswarm has changed in version ## 0.6.0. In versions &lt;0.6.0, this plot would have been dodged on the y- ## axis. In versions &gt;=0.6.0, grouponX=FALSE must be explicitly set to group ## on y-axis. Please set grouponX=TRUE/FALSE to avoid this warning and ensure ## proper axis choice. ggplot(mpg, aes(x = displ, y = cty, color = drv))+ geom_quasirandom() ## Warning in f(...): The default behavior of beeswarm has changed in version ## 0.6.0. In versions &lt;0.6.0, this plot would have been dodged on the y- ## axis. In versions &gt;=0.6.0, grouponX=FALSE must be explicitly set to group ## on y-axis. Please set grouponX=TRUE/FALSE to avoid this warning and ensure ## proper axis choice. geom_jitter is similar to geom_point but it provides random noise to the points. You can control these with the width and height arguments. This is valuable as it allows you to better see points that may overlap one another. geom_beeswarm adds variation in a uniform pattern by default across only the x-axis. geom-quasirandom also defaults to distributing the points across the x-axis however it produces quasi-random variation, ‘quasi’ because it looks as though points follow some interrelationship21 and if you run the plot multiple times you will get the exact same plot whereas for geom_jitter you will get a slightly different plot each time. To see the differences between geom_beeswarm and geom_quasirandom` it’s helpful to look at the plots above, but holding the y value constant at 1. plot_orig &lt;- ggplot(mpg, aes(x = displ, y = cty, color = drv))+ geom_point() plot_bees &lt;- ggplot(mpg, aes(x = 1, y = cty, color = drv))+ geom_beeswarm() plot_quasi &lt;- ggplot(mpg, aes(x = 1, y = cty, color = drv))+ geom_quasirandom() gridExtra::grid.arrange(plot_orig, plot_bees, plot_quasi, ncol = 1) 7.5.2.1. 1. How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut? Proportion cut in color: (change group_by() to group_by(cut, color) to set-up the converse) cut_in_color_graph &lt;- diamonds %&gt;% group_by(color, cut) %&gt;% summarise(n = n()) %&gt;% mutate(proportion_cut_in_color = n/sum(n)) %&gt;% ggplot(aes(x = color, y = cut))+ geom_tile(aes(fill = proportion_cut_in_color))+ labs(fill = &quot;proportion\\ncut in color&quot;) cut_in_color_graph This makes it clear that ideal cuts dominate the proportions of multiple colors, not ust G22 2. Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? I improved the original graph by adding in a filter so that only destinations that received over 10000 flights were included: flights %&gt;% group_by(dest, month) %&gt;% summarise(delay_mean = mean(dep_delay, na.rm=TRUE), n = n()) %&gt;% mutate(sum_n = sum(n)) %&gt;% select(dest, month, delay_mean, n, sum_n) %&gt;% as.data.frame() %&gt;% filter(dest == &quot;ABQ&quot;) %&gt;% #the sum on n will be at the dest level here filter(sum_n &gt; 30) %&gt;% ggplot(aes(x = as.factor(month), y = dest, fill = delay_mean))+ geom_tile() Another way to improve it may be to group the destinations into regions. This also will prevent you from filtering out data. We aren’t given region information, but we do have lat and long points in the airports dataset. See Appendix for notes 3. Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above? If you’re comparing the proportion of cut in color and want to be looking at how the specific cut proportion is changing, it may easier to view this while looking left to right vs. down to up. Compare the two plots below. cut_in_color_graph cut_in_color_graph+ coord_flip() 7.5.3. Two-d histograms smaller &lt;- diamonds %&gt;% filter(carat &lt; 3) ggplot(data = smaller) + geom_hex(mapping = aes(x = carat, y = price)) #can change bin number ggplot(data = smaller) + geom_bin2d(mapping = aes(x = carat, y = price), bins = c(30, 30)) # #or binwidth (roughly equivalent chart would be created) # ggplot(data = smaller) + # geom_bin2d(mapping = aes(x = carat, y = price), binwidth = c(.1, 1000)) Binned boxplots, violins, and lvs #split by width ggplot(smaller, aes(x = carat, y = price))+ geom_boxplot(aes(group = cut_width(carat, 0.1))) #split to get approximately same number in each box with cut_number() ggplot(smaller, aes(x = carat, y = price))+ geom_boxplot(aes(group = cut_number(carat, 20))) These methods don’t seem to work quite as well with violin plots or letter value plots: ##violin ggplot(smaller, aes(x = carat, y = price))+ geom_violin(aes(group = cut_width(carat, 0.1))) ggplot(smaller, aes(x = carat, y = price))+ geom_violin(aes(group = cut_number(carat, 20))) ##letter value ggplot(smaller, aes(x = carat, y = price))+ lvplot::geom_lv(aes(group = cut_width(carat, 0.1))) ggplot(smaller, aes(x = carat, y = price))+ lvplot::geom_lv(aes(group = cut_number(carat, 20))) They look a little bit improved if you allow for fewer values per bin compared to the examples with geom_boxplot() ggplot(smaller, aes(x = carat, y = price))+ geom_violin(aes(group = cut_number(carat, 10))) ggplot(smaller, aes(x = carat, y = price))+ geom_violin(aes(group = cut_width(carat, 0.25))) 7.5.3.1. 1. Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price? You should keep in mind how many lines you are going to create, they may overlap each other and look busy if you’re not careful. ggplot(smaller, aes(x = price)) + geom_freqpoly(aes(colour = cut_number(carat, 10))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. For the visualization below I wrapped it in the funciton plotly::ggplotly(). This funciton wraps your ggplot in html so that you can do things like hover over the points. p &lt;- ggplot(smaller, aes(x=price))+ geom_freqpoly(aes(colour = cut_width(carat, 0.25))) plotly::ggplotly(p) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2. Visualise the distribution of carat, partitioned by price. ggplot(diamonds, aes(x = price, y = carat))+ geom_violin(aes(group = cut_width(price, 2500))) 3. How does the price distribution of very large diamonds compare to small diamonds. Is it as you expect, or does it surprise you? diamonds %&gt;% mutate(percent_rank = percent_rank(carat), small = percent_rank &lt; 0.025, large = percent_rank &gt; 0.975) %&gt;% filter(small | large) %&gt;% ggplot(aes(large, price)) + geom_violin()+ facet_wrap(~large) Small diamonds have a left-skewed price distribution, large diamonds have a right skewed price distribution. 4. Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price. ggplot(diamonds, aes(x = carat, y = price))+ geom_jitter(aes(colour = cut), alpha = 0.2)+ geom_smooth(aes(colour = cut)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ggplot(diamonds, aes(x = carat, y = price))+ geom_boxplot(aes(group = cut_width(carat, 0.5), colour = cut))+ facet_grid(. ~ cut) ##I think this gives a better visualization, but is a little more complicated to produce, I also have the github version of ggplot and do not know whether the `preserve` arg is available in current CRAN installation. diamonds %&gt;% mutate(carat = cut(carat, 5)) %&gt;% ggplot(aes(x = carat, y = price))+ geom_boxplot(aes(group = interaction(cut_width(carat, 0.5), cut), fill = cut), position = position_dodge(preserve = &quot;single&quot;)) 5.Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. ggplot(data = diamonds) + geom_point(mapping = aes(x = x, y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) Why is a scatterplot a better display than a binned plot for this case? Binned plots give less precise value estimates at each point (constrained by the granularity of the binning) so outliers do not show-up as clearly. They also show less precise relationships between the data. The level of variability (at least with boxplots) can also be tougher to intuit. For example, let’s look at the plot below as a binned boxplot. ggplot(data = diamonds) + geom_boxplot(mapping = aes(x = cut_width(x, 1), y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) Appendix 7.5.2.1.2. Plot below shows four regions I’ll split the country into. Seems like for a few destinations the lat and long points were likely misentered (probably backwards). all_states &lt;- map_data(&quot;state&quot;) p &lt;- geom_polygon(data = all_states, aes(x = long, y = lat, group = group, label = NULL), colour = &quot;white&quot;, fill = &quot;grey10&quot;) dest_regions &lt;- nycflights13::airports %&gt;% mutate(lat_cut = cut(percent_rank(lat), 2, labels = c(&quot;S&quot;, &quot;N&quot;)), lon_cut = cut(percent_rank(lon), 2, labels = c(&quot;W&quot;, &quot;E&quot;)), quadrant = paste0(lat_cut, lon_cut)) point_plot &lt;- dest_regions %&gt;% ggplot(aes(lon, lat, colour = quadrant))+ p+ geom_point() point_plot+ coord_quickmap() Now let’s join our region information with our flight data and do our calculations grouping by quadrant rather than dest. Note that those quadrants with NA (did not join with flights) looked to be Pueorto Rico or other non-state locations. flights %&gt;% left_join(dest_regions, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% group_by(quadrant, month) %&gt;% summarise(delay_mean = mean(dep_delay, na.rm=TRUE), n = n()) %&gt;% mutate(sum_n = sum(n)) %&gt;% #the sum on n will be at the dest level here # filter(sum_n &gt; 10000) %&gt;% ggplot(aes(x = as.factor(month), y = quadrant, fill = delay_mean))+ geom_tile()+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;) 7.5.3.1.4. To get the fill value to vary need to iterate through and make each graph seperate, can’t ust use facet. diamonds_nest &lt;- diamonds %&gt;% group_by(cut) %&gt;% tidyr::nest() plot_free &lt;- function(df, name){ ggplot(df)+ geom_bin2d(aes(carat, price))+ ggtitle(name) } gridExtra::grid.arrange(grobs = mutate(diamonds_nest, out = purrr::map2(data, cut, plot_free))$out) diamonds %&gt;% mutate(cut = forcats::as_factor(as.character(cut), levels = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;))) %&gt;% # with(contrasts(cut)) lm(log(price) ~ log(carat) + cut, data = .) %&gt;% summary() ## ## Call: ## lm(formula = log(price) ~ log(carat) + cut, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52247 -0.16484 -0.00587 0.16087 1.38115 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.517337 0.001996 4267.70 &lt;2e-16 *** ## log(carat) 1.695771 0.001910 887.68 &lt;2e-16 *** ## cutPremium -0.078994 0.002810 -28.11 &lt;2e-16 *** ## cutGood -0.153967 0.004046 -38.06 &lt;2e-16 *** ## cutVery Good -0.076458 0.002904 -26.32 &lt;2e-16 *** ## cutFair -0.317212 0.006632 -47.83 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2545 on 53934 degrees of freedom ## Multiple R-squared: 0.9371, Adjusted R-squared: 0.9371 ## F-statistic: 1.607e+05 on 5 and 53934 DF, p-value: &lt; 2.2e-16 contrasts(diamonds$cut) ## .L .Q .C ^4 ## [1,] -0.6324555 0.5345225 -3.162278e-01 0.1195229 ## [2,] -0.3162278 -0.2672612 6.324555e-01 -0.4780914 ## [3,] 0.0000000 -0.5345225 -4.095972e-16 0.7171372 ## [4,] 0.3162278 -0.2672612 -6.324555e-01 -0.4780914 ## [5,] 0.6324555 0.5345225 3.162278e-01 0.1195229 count(diamonds, cut) ## # A tibble: 5 x 2 ## cut n ## &lt;ord&gt; &lt;int&gt; ## 1 Fair 1610 ## 2 Good 4906 ## 3 Very Good 12082 ## 4 Premium 13791 ## 5 Ideal 21551 not necessarily rounding one way or the other.↩ This is especially important when building things like boxplots whose graphs depend on all points in the graph.↩ Would need to read documentation for details.↩ I though am only so-so about this visualization with geom_tile…↩ "],
["10-tibbles.html", "Ch. 10: Tibbles 10.5: Exercises", " Ch. 10: Tibbles Key questions: none Functions and notes: tibble: produces a dataframe w/ some other helpful qualities that have advantages over data.frame see vignette(\"tibble\") as_tibble: convert to a tibble tribble: transposed tibble - set-up for data entry into a tibble in code print: can use print to set how the tibble will print nycflights13::flights %&gt;% print(n = 2, width = Inf) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## sched_arr_time arr_delay carrier flight tailnum origin dest air_time ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 819 11 UA 1545 N14228 EWR IAH 227 ## 2 830 20 UA 1714 N24211 LGA IAH 227 ## distance hour minute time_hour ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 1400 5 15 2013-01-01 05:00:00 ## 2 1416 5 29 2013-01-01 05:00:00 ## # ... with 3.368e+05 more rows * Also can convert with `as.data.frame` or use `options`, see [10.5: Exercises], problem 6 enframe: let’s you encode name and value, see 10.5: Exercises, problem 5 below class: for checking the class of the object Though is not fully accurate, in that the actual object class of vectors is “base”, not double, etc., so kind of lies… 10.5: Exercises 1. How can you tell if an object is a tibble? (Hint: try printing mtcars, which is a regular data frame). Could look at printing, e.g. only prints first 15 rows and enough variables where you can see them all, or by checking explicitly the class function23 2. Compare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data frame behaviours cause you frustration? Tibbles never change type of input e.g. from strings to factors Tibbles never change names of variables, never creates row names Tibbles print in a more concise and readable format This difference is made more stark if working with list-columns 3. If you have the name of a variable stored in an object, e.g. var &lt;- “mpg”, how can you extract the reference variable from a tibble? var &lt;- &quot;var_name&quot; # Will extract the column as an atomic vector df[[var]] 4. Practice referring to non-syntactic names in the following data frame by: df &lt;- tibble(`1` = 1:10, `2` = 11:20) a. Extracting the variable called 1. df %&gt;% select(1) ## # A tibble: 10 x 1 ## `1` ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 b. Plotting a scatterplot of 1 vs 2. df %&gt;% ggplot(aes(x = `1`, y = `2`))+ geom_point() c. Creating a new column called 3 which is 2 divided by 1. df %&gt;% mutate(`3` = `1` / `2`) ## # A tibble: 10 x 3 ## `1` `2` `3` ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 11 0.0909 ## 2 2 12 0.167 ## 3 3 13 0.231 ## 4 4 14 0.286 ## 5 5 15 0.333 ## 6 6 16 0.375 ## 7 7 17 0.412 ## 8 8 18 0.444 ## 9 9 19 0.474 ## 10 10 20 0.5 d. Renaming the columns to one, two and three. df %&gt;% mutate(`3` = `1` / `2`) %&gt;% rename(one = `1`, two = `2`, three = `3`) ## # A tibble: 10 x 3 ## one two three ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 11 0.0909 ## 2 2 12 0.167 ## 3 3 13 0.231 ## 4 4 14 0.286 ## 5 5 15 0.333 ## 6 6 16 0.375 ## 7 7 17 0.412 ## 8 8 18 0.444 ## 9 9 19 0.474 ## 10 10 20 0.5 5. What does tibble::enframe() do? When might you use it? Let’s you encode “name” and “value” as a tibble from a named vector tibble::enframe(c(a = 5, b = 8)) ## # A tibble: 2 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 5 ## 2 b 8 tibble::enframe(c(a = 5:8, b = 7:10)) ## # A tibble: 8 x 2 ## name value ## &lt;chr&gt; &lt;int&gt; ## 1 a1 5 ## 2 a2 6 ## 3 a3 7 ## 4 a4 8 ## 5 b1 7 ## 6 b2 8 ## 7 b3 9 ## 8 b4 10 6. What option controls how many additional column names are printed at the footer of a tibble? argument tibble.width options(tibble.print_max = n, tibble.print_min = m) options(tibble.width = Inf) options(dplyr.print_min = Inf) #to always show all rows or could check a few other things such as if list-cols are supported↩ "],
["11-data-import.html", "Ch. 11: Data import 11.2: Getting started 11.3: Parsing a vector", " Ch. 11: Data import Key questions: 11.2.2 # 1, 5 Functions and notes: read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter. read_fwf() reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions(). read_table() reads a common variation of fixed width files where columns are separated by white space. data.table::fread, good for raw speed read_log() reads Apache style log files. (But also check out webreadr which is built on top of read_log() and provides many more helpful tools.) read_log(readr_example(&quot;example.log&quot;)) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## X2 = col_character(), ## X3 = col_character(), ## X4 = col_character(), ## X5 = col_character(), ## X6 = col_integer(), ## X7 = col_integer() ## ) ## # A tibble: 2 x 7 ## X1 X2 X3 X4 X5 X6 X7 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 172.21.~ &lt;NA&gt; &quot;Microsoft~ 08/Apr/2001~ GET /scripts/iisadmi~ 200 3401 ## 2 127.0.0~ &lt;NA&gt; frank 10/Oct/2000~ GET /apache_pb.gif H~ 200 2326 # readr_example finds the correct path associated with the package by doing the following: ## system.file(&quot;extdata&quot;, path, package = &quot;readr&quot;, mustWork = TRUE) parse_*(): take character vector and return more specialized vector parse_logical, parse_integer, parse_double, parse_number24, parse_character, parse_factor(has levels as an argument), parse_datetime, parse_date, parse_time (these last three have an arg offormat`) locale argument for use in parse functions to affect formatting and to pass into argument locale = locale(&lt;arg&gt; = \"&lt;value&gt;\")) for double, e.g. locale = locale(decimal_mark = \",\") for number, e.g. locale = locale(grouping_mark = \".\") for character, e.g. locale = locale(encoding = \"Latin1\") for dates, e.g. locale = locale(lang = \"fr\") (to see built-in language options use date_name_langs and can create own with date_names) problems: returns problems on import charToRaw will show underlying representation of a character25 guess_encoding: can guess encoding – generally would use this with charToRaw and helps avoid figuring out encoding by hand x1 &lt;- &quot;El Ni\\xf1o was particularly bad this year&quot; guess_encoding(charToRaw(x1)) ## # A tibble: 2 x 2 ## encoding confidence ## &lt;chr&gt; &lt;dbl&gt; ## 1 ISO-8859-1 0.46 ## 2 ISO-8859-9 0.23 If defaults don’t work (primarily for dates, times, numbers) can use following to specify parsing: Year : %Y (4 digits). : %y (2 digits); 00-69 -&gt; 2000-2069, 70-99 -&gt; 1970-1999. Month %m (2 digits). %b (abbreviated name, like “Jan”). %B (full name, “January”). Day %d (2 digits). %e (optional leading space). Time %H 0-23 hour. %I 0-12, must be used with %p. %p AM/PM indicator. %M minutes. %S integer seconds. %OS real seconds. %Z Time zone (as name, e.g. America/Chicago). Beware of abbreviations: if you’re American, note that “EST” is a Canadian time zone that does not have daylight savings time. It is not Eastern Standard Time! We’ll come back to this [time zones]. %z (as offset from UTC, e.g. +0800). Non-digits %. skips one non-digit character. %* skips any number of non-digits. guess_parser: returns what readr would think the character vector you provide it should be parsed into parse_guess: uses readr’s guess of the vector type to parse the column col_*: counterpoint to parse_* functions except for use when data is in a file rather than a string already loaded in R (as needed for parse_*) cols: use this to pass in the col_* types, col_types = cols( x = col_double(), y = col_date() ) to read in all columns as character use col_types = cols(.default = col_character()) can set n_max to smallish number if reading in large file and still debugging parsing issues Recommend always input cols, if you want to be strict when loading in data set stop_for_problems read_lines read into character vector of lines (use when having major issues) read_file read in as character vector of length 1 (use when having major issues) read_rds reads in R’s custom binary format26 feather::read_feather: fast binary file format shared across languages27 writing files^[readr functions will encodes strings in UTF-8 and saves dates and date-times in ISO8601): write_csv, write_tsv, write_excel_csv, write_rds28, * feather::read_feather other packages for reading-in / writing data: haven, readxl, DBI, odbc, jsonlite, xml2, rio 11.2: Getting started 11.2.2. 1. What function would you use to read a file where fields were separated with “|”? read_delim for example: read_delim(&quot;a|b|c\\n1|2|3&quot;, delim = &quot;|&quot;) ## # A tibble: 1 x 3 ## a b c ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 2 3 2. Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?29 col_names, col_types, locale, na, quoted_na, quote, trim_ws, skip, n_max, guess_max, progress 3. What are the most important arguments to read_fwf()? widths 4. Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like \" or ’. By convention, read_csv() assumes that the quoting character will be \", and if you want to change it you’ll need to use read_delim() instead. What arguments do you need to specify to read the following text into a data frame? &quot;x,y\\n1,&#39;a,b&#39;&quot; ## [1] &quot;x,y\\n1,&#39;a,b&#39;&quot; read_delim(&quot;x,y\\n1,&#39;a,b&#39;&quot;, delim = &quot;,&quot;, quote = &quot;&#39;&quot;) ## # A tibble: 1 x 2 ## x y ## &lt;int&gt; &lt;chr&gt; ## 1 1 a,b 5. Identify what is wrong with each of the following inline CSV files. What happens when you run the code? read_csv(\"a,b\\n1,2,3\\n4,5,6\") needs 3rd column header, skips 3rd argument on each line, corrected: read_csv(\"a,b\\n1,2\\n3,4\\n5,6\") read_csv(\"a,b,c\\n1,2\\n1,2,3,4\") missing 3rd value on 2nd line so currently makes NA, corrected: read_csv(\"a,b,c\\n1,2, 1\\n2,3,4\") read_csv(\"a,b\\n\\\"1\") 2nd value missing and 2nd quote mark missing (though quotes are unnecessary), corrected: read_csv(\"a,b\\n\\\"1\\\",\\\"2\\\"\") read_csv(\"a,b\\n1,2\\na,b\") Have character and numeric types, read_csv(\"a;b\\n1;3\") need to make read_csv2() because is seperated by semicolons, corrected: read_csv2(\"a;b\\n1;3\") 11.3: Parsing a vector 11.3.5. 1. What are the most important arguments to locale()? It depends on the parse_* type, e.g. for double, e.g. locale = locale(decimal_mark = \",\") for number, e.g. locale = locale(grouping_mark = \".\") for character, e.g. locale = locale(encoding = \"Latin1\") for dates, e.g. locale = locale(lang = \"fr\") Below are a few examples for double and number parse_double(&quot;1.23&quot;) ## [1] 1.23 parse_double(&quot;1,23&quot;, locale = locale(decimal_mark=&quot;,&quot;)) ## [1] 1.23 parse_number(&quot;the cost is $125.34, it&#39;s a good deal&quot;) #Slightly different than book, captures decimal ## [1] 125.34 parse_number(&quot;$123,456,789&quot;) ## [1] 123456789 parse_number(&quot;$123.456.789&quot;) ## [1] 123.456 parse_number(&quot;$123.456.789&quot;, locale = locale(grouping_mark = &quot;.&quot;))#used in europe ## [1] 123456789 parse_number(&quot;$123&#39;456&#39;789&quot;, locale = locale(grouping_mark = &quot;&#39;&quot;))#used in Switzerland ## [1] 123456789 2. What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”? can’t set both to be same–if you change one, other automatically changes parse_number(&quot;$135.435,45&quot;, locale = locale(grouping_mark = &quot;.&quot;, decimal_mark = &quot;,&quot;)) ## [1] 135435.4 parse_number(&quot;$135.435,45&quot;, locale = locale(grouping_mark = &quot;.&quot;)) ## [1] 135435.4 3. I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful. date_format and time_format in locale() let you set the default date and time formats parse_date(&quot;31 january 2015&quot;, format = &quot;%d %B %Y&quot;) ## [1] &quot;2015-01-31&quot; parse_date(&quot;31 january 2015&quot;, locale = locale(date_format = &quot;%d %B %Y&quot;)) ## [1] &quot;2015-01-31&quot; #let&#39;s you change it in locale() 4. If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly. I live in the US. 5. What’s the difference between read_csv() and read_csv2()? * Second expects semicolons 6. What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out. Europe tends to use “%d-%m-%Y” Asia tends to use “%d.%m.%Y” 7. Generate the correct format string to parse each of the following dates and times: d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 t1 &lt;- &quot;1705&quot; t2 &lt;- &quot;11:15:10.12 PM&quot; t3 &lt;- &quot;11:::15:10.12 PM&quot; Solutions: parse_date(d1, &quot;%B %d, %Y&quot;) ## [1] &quot;2010-01-01&quot; parse_date(d2, &quot;%Y-%b-%d&quot;) ## [1] &quot;2015-03-07&quot; parse_date(d3, &quot;%d-%b-%Y&quot;) ## [1] &quot;2017-06-06&quot; parse_date(d3, &quot;%d%.%b-%Y&quot;) #could use this alternatively ## [1] &quot;2017-06-06&quot; parse_date(d4, &quot;%B %d (%Y)&quot;) ## [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; parse_date(d5, &quot;%m/%d/%y&quot;) ## [1] &quot;2014-12-30&quot; parse_time(t1, &quot;%H%M&quot;) ## 17:05:00 parse_time(t2, &quot;%I:%M:%OS %p&quot;) ## 23:15:10.12 parse_time(t3, &quot;%I%*%M:%OS %p&quot;) ## 23:15:10.12 can be helpful for dealing with parsing currencies or percentages for example…↩ e.g. helpful for use w/ parse_char↩ readRDS is base R version↩ though does not support list-cols↩ base R version is saveRDS↩ * skip = n, comment = # any line that starts w/ input to comment will be skipped, col_names = FALSE or perhaps c(\"x\", \"y\", \"z\"), na = \".\"↩ "],
["12-tidy-data.html", "Ch. 12: Tidy data 12.2: Tidy data 12.3: Spreading and gathering 12.4: Separating and uniting 12.5: Missing values 12.6 Case Study", " Ch. 12: Tidy data Key questions: 12.3.3. #4 12.4.3. #1 12.6.1 #4 Functions and notes: spread: pivot, e.g. spread(iris, Species) gather: unpivot, e.g. gather(mpg, drv, class, key = \"drive_or_class\", value = \"value\") separate: one column into many, e.g. separate(table3, rate, into = c(\"cases\", \"population\"), sep = \"/\") default uses non-alphanumeric character as sep, can also use number to separate by width extract similar to separate but specify what to pull-out rather than what to split by unite inverse of separate # example distinguishing separate, extract, unite tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f&quot;, &quot;h,i,j&quot;, &quot;k,l,m&quot;)) %&gt;% tidyr::separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), sep = &quot;,&quot;, remove = FALSE) %&gt;% tidyr::unite(one, two, three, col = &quot;x2&quot;, sep = &quot;,&quot;, remove = FALSE) %&gt;% tidyr::extract(x2, into = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), regex = &quot;([a-z]+),([a-z]+),([a-z]+)&quot;, remove = FALSE) ## # A tibble: 4 x 8 ## x x2 a b c one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a,b,c a,b,c a b c a b c ## 2 d,e,f d,e,f d e f d e f ## 3 h,i,j h,i,j h i j h i j ## 4 k,l,m k,l,m k l m k l m complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary. fill() takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward). # examples of complete and fill treatment &lt;- tribble( ~ person, ~ treatment, ~response, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) treatment %&gt;% fill(person) ## # A tibble: 4 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Derrick Whitmore 2 10 ## 3 Derrick Whitmore 3 9 ## 4 Katherine Burke 1 4 treatment %&gt;% fill(person) %&gt;% complete(person, treatment) ## # A tibble: 6 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Derrick Whitmore 2 10 ## 3 Derrick Whitmore 3 9 ## 4 Katherine Burke 1 4 ## 5 Katherine Burke 2 NA ## 6 Katherine Burke 3 NA 12.2: Tidy data 12.2.1. 1. Using prose, describe how the variables and observations are organised in each of the sample tables. table1: each country-year is a row with cases and pop as values table2: each country-year-type is a row table3: each country-year is a row with rate containing values for both cases and population table4a and table4b: a represents cases, b population, each row is a country and then column are the year for the value 2. Compute the rate for table2, and table4a + table4b. You will need to perform four operations: Extract the number of TB cases per country per year. Extract the matching population per country per year. Divide cases by population, and multiply by 10000. Store back in the appropriate place. Which representation is easiest to work with? Which is hardest? Why? with table2: table2 %&gt;% spread(type, count) %&gt;% mutate(rate = 1000 * cases / population) %&gt;% arrange(country, year) ## # A tibble: 6 x 5 ## country year cases population rate ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 0.0373 ## 2 Afghanistan 2000 2666 20595360 0.129 ## 3 Brazil 1999 37737 172006362 0.219 ## 4 Brazil 2000 80488 174504898 0.461 ## 5 China 1999 212258 1272915272 0.167 ## 6 China 2000 213766 1280428583 0.167 with table4 ‘a’ and ‘b’`: table4a %&gt;% gather(2,3, key = &quot;year&quot;, value = &quot;cases&quot;) %&gt;% inner_join(table4b %&gt;% gather(c(2,3), key = &quot;year&quot;, value = &quot;population&quot;), by = c(&quot;country&quot;, &quot;year&quot;)) %&gt;% mutate(rate = 1000 * cases / population) ## # A tibble: 6 x 5 ## country year cases population rate ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 0.0373 ## 2 Brazil 1999 37737 172006362 0.219 ## 3 China 1999 212258 1272915272 0.167 ## 4 Afghanistan 2000 2666 20595360 0.129 ## 5 Brazil 2000 80488 174504898 0.461 ## 6 China 2000 213766 1280428583 0.167 between these, table2 was easier, though table1 would have been easiest – is fewer steps to get 1 row = 1 observation (if we define an observation as a country in a year with certain attributes) 3. Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first? table2 %&gt;% spread(type, count) %&gt;% ggplot(aes(x = year, y = cases, group = country))+ geom_line(colour = &quot;grey50&quot;)+ geom_point(aes(colour = country)) first had to spread data 12.3: Spreading and gathering 12.3.3. 1. Why are gather() and spread() not perfectly symmetrical? Carefully consider the following example: stocks &lt;- tibble( year = c(2015, 2015, 2016, 2016), half = c( 1, 2, 1, 2), return = c(1.88, 0.59, 0.92, 0.17) ) stocks %&gt;% spread(year, return) %&gt;% gather(&quot;year&quot;, &quot;return&quot;, `2015`:`2016`) ## # A tibble: 4 x 3 ## half year return ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2015 1.88 ## 2 2 2015 0.59 ## 3 1 2016 0.92 ## 4 2 2016 0.17 (Hint: look at the variable types and think about column names.) are not perfectly symmetrical, because type for key = changes to character when using gather – column type information is not transferred. position of columns change as well Both spread() and gather() have a convert argument. What does it do?* Use this to automatically change key column type, otherwise will default in gather for example to become a character type. 2. Why does this code fail? table4a %&gt;% gather(1999, 2000, key = &quot;year&quot;, value = &quot;cases&quot;) ## Error in inds_combine(.vars, ind_list): Position must be between 0 and n Need backticks on year column names table4a %&gt;% gather(`1999`, `2000`, key = &quot;year&quot;, value = &quot;cases&quot;) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 3. Why does spreading this tibble fail? How could you add a new column to fix the problem? people &lt;- tribble( ~name, ~key, ~value, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) people %&gt;% spread(key = &quot;key&quot;, value = &quot;value&quot;) ## Error: Each row of output must be identified by a unique combination of keys. ## Keys are shared for 2 rows: ## * 1, 3 ## Do you need to create unique ID with tibble::rowid_to_column()? Fails because you have more than one age for philip woods, could add a unique ID column and it will work. people %&gt;% mutate(id = 1:n()) %&gt;% spread(key = &quot;key&quot;, value = &quot;value&quot;) ## # A tibble: 5 x 4 ## name id age height ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jessica Cordero 4 37 NA ## 2 Jessica Cordero 5 NA 156 ## 3 Phillip Woods 1 45 NA ## 4 Phillip Woods 2 NA 186 ## 5 Phillip Woods 3 50 NA 4. Tidy the simple tibble below. Do you need to spread or gather it? What are the variables? preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) Need to gather gender preg %&gt;% gather(male, female, key=&quot;gender&quot;, value=&quot;Number&quot;) ## # A tibble: 4 x 3 ## pregnant gender Number ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 yes male NA ## 2 no male 20 ## 3 yes female 10 ## 4 no female 12 12.4: Separating and uniting 12.4.3. 1. What do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) ## Warning: Expected 3 pieces. Additional pieces discarded in 1 rows [2]. ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e f ## 3 h i j tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 1 rows [2]. ## # A tibble: 3 x 3 ## one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a b c ## 2 d e &lt;NA&gt; ## 3 f g i fill determines what to do when there are too few arguments, default is to fill right arguments with NA can change this though. tribble(~a,~b, &quot;so it goes&quot;,&quot;hello,you,are&quot;) %&gt;% separate(b, into=c(&quot;e&quot;,&quot;f&quot;,&quot;g&quot;, &quot;h&quot;), sep=&quot;,&quot;, fill = &quot;left&quot;) ## # A tibble: 1 x 5 ## a e f g h ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 so it goes &lt;NA&gt; hello you are extra determines what to do when you have more splits than you do into spaces. Default is to drop extra Can change to limit num of splits to length of into with extra = \"merge\" tribble(~a,~b, &quot;so it goes&quot;,&quot;hello,you,are&quot;) %&gt;% separate(b, into = c(&quot;e&quot;, &quot;f&quot;), sep = &quot;,&quot;, extra = &quot;merge&quot;) ## # A tibble: 1 x 3 ## a e f ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 so it goes hello you,are 2. Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE? remove = FALSE allows you to specify to keep the input column(s) tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f&quot;, &quot;h,i,j&quot;, &quot;k,l,m&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), remove = FALSE) %&gt;% unite(one, two, three, col = &quot;x2&quot;, sep = &quot;,&quot;, remove = FALSE) ## # A tibble: 4 x 5 ## x x2 one two three ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a,b,c a,b,c a b c ## 2 d,e,f d,e,f d e f ## 3 h,i,j h,i,j h i j ## 4 k,l,m k,l,m k l m 3. Compare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite? extract() is like separate() but provide what to capture rather than what to split by as in regex instead of sep. df &lt;- data.frame(x = c(&quot;a-b&quot;, &quot;a-d&quot;, &quot;b-c&quot;, &quot;d&amp;e&quot;, NA), y = 1) df %&gt;% extract(col = x, into = c(&quot;1st&quot;, &quot;2nd&quot;), regex = &quot;([A-z]).([A-z])&quot;) ## 1st 2nd y ## 1 a b 1 ## 2 a d 1 ## 3 b c 1 ## 4 d e 1 ## 5 &lt;NA&gt; &lt;NA&gt; 1 df %&gt;% separate(col = x, into = c(&quot;1st&quot;, &quot;2nd&quot;), sep = &quot;[^A-z]&quot;) ## 1st 2nd y ## 1 a b 1 ## 2 a d 1 ## 3 b c 1 ## 4 d e 1 ## 5 &lt;NA&gt; &lt;NA&gt; 1 Because there are many ways to split something up, but only one way to bring multiple things together… 12.5: Missing values 12.5.1. 1. Compare and contrast the fill arguments to spread() and complete(). Both create open cells by filling out those that are not currently in the dataset, complete though does it by adding rows of iterations not included, whereas spread does it by the process of spreading out fields and naturally generating values that did not have row values previously. Thefill in each specifies what value should go into these created cells. treatment2 &lt;- tribble( ~ person, ~ treatment, ~response, &quot;Derrick Whitmore&quot;, 1, 7, &quot;Derrick Whitmore&quot;, 2, 10, &quot;Derrick Whitmore&quot;, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) treatment2 %&gt;% complete(person, treatment, fill = list(response = 0)) ## # A tibble: 6 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Derrick Whitmore 2 10 ## 3 Derrick Whitmore 3 9 ## 4 Katherine Burke 1 4 ## 5 Katherine Burke 2 0 ## 6 Katherine Burke 3 0 treatment2 %&gt;% spread(key = treatment, value = response, fill = 0) ## # A tibble: 2 x 4 ## person `1` `2` `3` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 7 10 9 ## 2 Katherine Burke 4 0 0 2. What does the .direction argument to fill() do? Let’s you fill either up or down. E.g. below is filling up example. treatment &lt;- tribble( ~ person, ~ treatment, ~response, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) treatment %&gt;% fill(person, .direction = &quot;up&quot;) ## # A tibble: 4 x 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Katherine Burke 2 10 ## 3 Katherine Burke 3 9 ## 4 Katherine Burke 1 4 12.6 Case Study 12.6.1. 1. In this case study I set na.rm = TRUE just to make it easier to check that we had the correct values. Is this reasonable? Think about how missing values are represented in this dataset. Are there implicit missing values? What’s the difference between an NA and zero? In this case it’s reasonable, an NA perhaps means the metric wasn’t recorded in that year, whereas 0 means it was recorded but there were 0 cases. Implicit missing values represented by say Afghanistan not having any reported cases for females. 2. What happens if you neglect the mutate() step? (mutate(key = stringr::str_replace(key, \"newrel\", \"new_rel\"))) You would have had one less column, so ‘newtype’ would have been on column, rather than these splitting. 3. I claimed that iso2 and iso3 were redundant with country. Confirm this claim. who %&gt;% select(1:3) %&gt;% distinct() %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 219 who %&gt;% select(1:3) %&gt;% distinct() %&gt;% unite(country, iso2, iso3, col = &quot;country_combined&quot;) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 219 Both of the above are the same length. 4. For each country, year, and sex compute the total number of cases of TB. Make an informative visualisation of the data. who_present &lt;- who %&gt;% gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %&gt;% mutate(code = stringr::str_replace(code, &quot;newrel&quot;, &quot;new_rel&quot;)) %&gt;% separate(code, c(&quot;new&quot;, &quot;var&quot;, &quot;sexage&quot;)) %&gt;% select(-new, -iso2, -iso3) %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) who_present %&gt;% group_by(sex, year, country) %&gt;% summarise(mean=mean(value)) %&gt;% ggplot(aes(x=year, y=mean, colour=sex))+ geom_point()+ geom_jitter() #ratio of female tb cases over time who_present %&gt;% group_by(sex, year) %&gt;% summarise(meansex=sum(value)) %&gt;% ungroup() %&gt;% group_by(year) %&gt;% mutate(tot=sum(meansex)) %&gt;% ungroup() %&gt;% mutate(ratio=meansex/tot) %&gt;% filter(sex==&quot;f&quot;) %&gt;% ggplot(aes(x=year, y=ratio, colour=sex))+ geom_line() #countries with the most outbreaks who_present %&gt;% group_by(country, year) %&gt;% summarise(n=sum(value)) %&gt;% ungroup() %&gt;% group_by(country) %&gt;% mutate(total_country=sum(n)) %&gt;% filter(total_country&gt;1000000) %&gt;% ggplot(aes(x=year,y=n,colour=country))+ geom_line() #countries with the most split by gender as well who_present %&gt;% group_by(country, sex, year) %&gt;% summarise(n=sum(value)) %&gt;% ungroup() %&gt;% group_by(country) %&gt;% mutate(total_country=sum(n)) %&gt;% filter(total_country&gt;1000000) %&gt;% ggplot(aes(x=year,y=n,colour=sex))+ geom_line()+ facet_wrap(~country) #take log and summarise who_present %&gt;% group_by(country, year) %&gt;% summarise(n=sum(value), logn=log(n)) %&gt;% ungroup() %&gt;% group_by(country) %&gt;% mutate(total_c=sum(n)) %&gt;% filter(total_c&gt;1000000) %&gt;% ggplot(aes(x=year,y=logn, colour=country))+ geom_line(show.legend=TRUE) #average # of countries with more female TB cases who_present %&gt;% group_by(country, year, sex) %&gt;% summarise(n=sum(value), logn=log(n)) %&gt;% ungroup() %&gt;% group_by(country, year) %&gt;% mutate(total_c=sum(n)) %&gt;% ungroup() %&gt;% mutate(perc_gender=n/total_c, femalemore=ifelse(perc_gender&gt;.5,1,0)) %&gt;% filter(sex==&quot;f&quot;) %&gt;% group_by(year) %&gt;% summarise(summaryfem=mean(femalemore,na.rm=TRUE )) %&gt;% ggplot(aes(x=year,y=summaryfem))+ geom_line() "],
["13-relational-data.html", "Ch. 13: Relational data 13.2 nycflights13 13.3 Keys 13.4 Mutating joins 13.5 Filtering joins Appendix", " Ch. 13: Relational data Key questions: 13.4.6 , #1, 3, 4 13.5.1, #2, 4 Functions and notes: “The relations of three or more tables are always a property of the relations between each pairs.” Three families of verbs in relational data: Mutating joins, which add new variables to one data frame from matching observations in another. inner_join: match when equal left_join: keep all observations in table in 1st arg right_join: keep all observations in table in 2nd arg full_join: keep all observations in table in 1st and 2nd arg Filtering joins, which filter observations from one data frame based on whether or not they match an observation in the other table. semi_join(x, y) keeps all observations in x that have a match in y. anti_join(x, y) drops all observations in x that have a match in y. Set operations, which treat observations as if they were set elements. intersect(x, y): return only observations in both x and y (when inputs are a df, is comparing across all values in a row). union(x, y): return unique observations in x and y. setdiff(x, y): return observations in x, but not in y. base::merge() can perform all four types of mutating join: dplyr merge inner_join(x, y) merge(x, y) left_join(x, y) merge(x, y, all.x = TRUE) right_join(x, y) merge(x, y, all.y = TRUE), full_join(x, y) merge(x, y, all.x = TRUE, all.y = TRUE) SQL is the inspiration for dplyr’s conventions, so the translation is straightforward: dplyr SQL inner_join(x, y, by = \"z\") SELECT * FROM x INNER JOIN y USING (z) left_join(x, y, by = \"z\") SELECT * FROM x LEFT OUTER JOIN y USING (z) right_join(x, y, by = \"z\") SELECT * FROM x RIGHT OUTER JOIN y USING (z) full_join(x, y, by = \"z\") SELECT * FROM x FULL OUTER JOIN y USING (z) 13.2 nycflights13 flights airlines airports planes weather 13.2.1 Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine? To draw a line from origin to destination, I need the lat lon points from airportsas well as the dest and origin variables fromflights`. I forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram? origin from weather connects tofaafromairports` in a many to one relationship weather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights? It would connect to dest. We know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables? Make a set of days that are less popular and have these dates connect by month and day 13.3 Keys 13.3.1 Add a surrogate key to flights. flights %&gt;% mutate(surrogate_key = row_number()) %&gt;% glimpse() ## Observations: 336,776 ## Variables: 20 ## $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 55... ## $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 60... ## $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2... ## $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 7... ## $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 7... ## $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -... ## $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;,... ## $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79... ## $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN... ## $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;... ## $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;... ## $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138... ## $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 94... ## $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5,... ## $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013... ## $ surrogate_key &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ... Identify the keys in the following datasets Lahman::Batting: player, year, stint Lahman::Batting %&gt;% count(playerID, yearID, stint) %&gt;% filter(n &gt; 1) ## # A tibble: 0 x 4 ## # ... with 4 variables: playerID &lt;chr&gt;, yearID &lt;int&gt;, stint &lt;int&gt;, n &lt;int&gt; babynames::babynames: name, sex, year babynames::babynames %&gt;% count(name, sex, year) %&gt;% filter(n &gt; 1) ## # A tibble: 0 x 4 ## # ... with 4 variables: name &lt;chr&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;, n &lt;int&gt; nasaweather::atmos: lat, long, year, month nasaweather::atmos %&gt;% count(lat, long, year, month) %&gt;% filter(n &gt; 1) ## # A tibble: 0 x 5 ## # ... with 5 variables: lat &lt;dbl&gt;, long &lt;dbl&gt;, year &lt;int&gt;, month &lt;int&gt;, ## # n &lt;int&gt; fueleconomy::vehicles: id fueleconomy::vehicles %&gt;% count(id) %&gt;% filter(n &gt; 1) ## # A tibble: 0 x 2 ## # ... with 2 variables: id &lt;int&gt;, n &lt;int&gt; ggplot2::diamonds: needs surrogate diamonds %&gt;% count(x, y, z, depth, table, carat, cut, color, price, clarity ) %&gt;% filter(n &gt; 1) ## # A tibble: 143 x 11 ## x y z depth table carat cut color price clarity n ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; &lt;ord&gt; &lt;int&gt; ## 1 0 0 0 64.1 60 0.71 Good F 2130 SI2 2 ## 2 4.23 4.26 2.69 63.4 57 0.3 Good J 394 VS1 2 ## 3 4.26 4.23 2.69 63.4 57 0.3 Very Good J 506 VS1 2 ## 4 4.26 4.29 2.66 62.2 57 0.3 Ideal H 450 SI1 2 ## 5 4.27 4.28 2.66 62.2 57 0.3 Ideal H 450 SI1 2 ## 6 4.29 4.31 2.71 63 55 0.3 Very Good G 526 VS2 2 ## 7 4.29 4.31 2.73 63.5 56 0.31 Good D 571 SI1 2 ## 8 4.31 4.28 2.67 62.2 58 0.3 Premium D 709 SI1 2 ## 9 4.31 4.29 2.71 63 55 0.3 Ideal G 675 VS2 2 ## 10 4.31 4.29 2.73 63.5 56 0.31 Very Good D 732 SI1 2 ## # ... with 133 more rows diamonds %&gt;% mutate(surrogate_id = row_number()) ## # A tibble: 53,940 x 11 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Prem~ E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Prem~ I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very~ J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very~ I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very~ H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very~ H VS1 59.4 61 338 4 4.05 2.39 ## # ... with 53,930 more rows, and 1 more variable: surrogate_id &lt;int&gt; Draw a diagram illustrating the connections between the Batting,Master, and Salaries tables in the Lahman package. Draw another diagram that shows the relationship between Master, Managers, AwardsManagers. Lahman::Batting and Lahman::Master combine by playerID Lahman::Batting and Lahman::Salaries combine by playerID, yearID Lahman::Master and Lahman::Salaries combine by playerID Lahman::Master and Lahman::Managers combine by playerID Lahman::Master and Lahman::AwardsManagers combine by playerID How would you characterise the relationship between the Batting, Pitching, and Fielding tables? All connect by playerID, yearID, stint 13.4 Mutating joins The most commonly used join is the left join: you use this whenever you look up additional data from another table, because it preserves the original observations even when there isn’t a match. 13.4.6 Compute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States: flights %&gt;% semi_join(airports, c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% group_by(dest) %&gt;% summarise(delay = mean(arr_delay, na.rm=TRUE)) %&gt;% left_join(airports, by = c(&quot;dest&quot;=&quot;faa&quot;)) %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point(aes(colour = delay)) + coord_quickmap()+ # see chapter 28 for information on scales scale_color_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;) Add the location of the origin and destination (i.e. the lat and lon) to flights. flights %&gt;% left_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% left_join(airports, by = c(&quot;origin&quot; = &quot;faa&quot;), suffix = c(&quot;_dest&quot;, &quot;_origin&quot;)) %&gt;% select(flight, carrier, dest, lat_dest, lon_dest, origin, lat_origin, lon_origin) ## # A tibble: 336,776 x 8 ## flight carrier dest lat_dest lon_dest origin lat_origin lon_origin ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1545 UA IAH 30.0 -95.3 EWR 40.7 -74.2 ## 2 1714 UA IAH 30.0 -95.3 LGA 40.8 -73.9 ## 3 1141 AA MIA 25.8 -80.3 JFK 40.6 -73.8 ## 4 725 B6 BQN NA NA JFK 40.6 -73.8 ## 5 461 DL ATL 33.6 -84.4 LGA 40.8 -73.9 ## 6 1696 UA ORD 42.0 -87.9 EWR 40.7 -74.2 ## 7 507 B6 FLL 26.1 -80.2 EWR 40.7 -74.2 ## 8 5708 EV IAD 38.9 -77.5 LGA 40.8 -73.9 ## 9 79 B6 MCO 28.4 -81.3 JFK 40.6 -73.8 ## 10 301 AA ORD 42.0 -87.9 LGA 40.8 -73.9 ## # ... with 336,766 more rows Note that the suffix allows you to tag names onto first and second table, hence why vector is length 2 Is there a relationship between the age of a plane and its delays? group_by(flights, tailnum) %&gt;% summarise(avg_delay = mean(arr_delay, na.rm=TRUE), n = n()) %&gt;% left_join(planes, by=&quot;tailnum&quot;) %&gt;% mutate(age = 2013 - year) %&gt;% filter(n &gt; 50, age &lt; 30) %&gt;% ggplot(aes(x = age, y = avg_delay))+ ggbeeswarm::geom_quasirandom()+ geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Looks as though planes that are roughly 5 to 10 years old have higher delays… Let’s look at same thing using boxplots. group_by(flights, tailnum) %&gt;% summarise(avg_delay = mean(arr_delay, na.rm=TRUE), n = n()) %&gt;% left_join(planes, by=&quot;tailnum&quot;) %&gt;% mutate(age = 2013 - year) %&gt;% filter(n &gt; 50, age &lt;= 30, age &gt;= 0) %&gt;% ggplot()+ geom_boxplot(aes(x = cut_width(age, 2, boundary = 0), y = avg_delay))+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) Perhaps there is not an overall trend association between age and delays, though it seems that the particular group of planes in that time range seem to have delays than either newer or older planes. On the other hand, there does almost look to be a seasonality pattern – though this may just be me seeing things… perhaps worth exploring more… A simple way to test for a non-linear relationship would be to discretize age and then pass it through an anova… nycflights13::flights %&gt;% select(arr_delay, tailnum) %&gt;% left_join(planes, by=&quot;tailnum&quot;) %&gt;% filter(!is.na(arr_delay)) %&gt;% mutate(age = 2013 - year, age_round_5 = (5 * age %/% 5) %&gt;% as.factor()) %&gt;% with(aov(arr_delay ~ age_round_5)) %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age_round_5 11 1062080 96553 47.92 &lt;2e-16 *** ## Residuals 273841 551756442 2015 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 53493 observations deleted due to missingness There are weaknesses to using anova, but the low p-value above suggests test arrival delay is not randomly distributed across age The reason for such a difference may be trivial or may be confounded by a more interesting pattern… but these are deeper questions What weather conditions make it more likely to see a delay? There are a lot of ways you could have approached this problem. Below, I look at the average weather value for each of the groups FALSE, TRUE and Canceled – FALSE corresponding with non-delayed flights, TRUE with delayed flights and Canceled with flights that were canceled. If I were feeling fancy, I would have also added the standard errors on these… flights_weath &lt;- mutate(flights, delay_TF = dep_delay &gt; 0) %&gt;% separate(sched_dep_time, into = c(&quot;hour_sched&quot;, &quot;min_sched&quot;), sep = -3, remove = FALSE, convert = TRUE) %&gt;% left_join(weather, by = c(&quot;origin&quot;, &quot;year&quot;,&quot;month&quot;, &quot;day&quot;, &quot;hour_sched&quot;=&quot;hour&quot;)) flights_weath_gath &lt;- flights_weath %&gt;% select(sched_dep_time, delay_TF, sched_dep_time, temp:visib) %&gt;% mutate(key = row_number()) %&gt;% gather(temp, dewp, humid, wind_dir, wind_speed, wind_gust, precip, pressure, visib, key=&quot;weather&quot;, value=&quot;values&quot;) flights_summarized &lt;- flights_weath_gath %&gt;% group_by(weather, delay_TF) %&gt;% summarise(median_weath = median(values, na.rm = TRUE), mean_weath = mean(values, na.rm = TRUE), sum_n = sum(!is.na(values))) %&gt;% ungroup() %&gt;% mutate(delay_TF = ifelse(is.na(delay_TF), &quot;Canceled&quot;, delay_TF), delay_TF = forcats::as_factor(delay_TF, c(FALSE, TRUE, &quot;Canceled&quot;))) flights_summarized %&gt;% ggplot(aes(x = delay_TF, y = mean_weath, fill = delay_TF))+ geom_col()+ facet_wrap(~weather, scales = &quot;free_y&quot;)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) While precipitation is the largest difference, my guess is that the standard error on this would be much greater day to day because as you can see the values are very low, so it could be that a few cases with a lot of rain may tick it up, but it may be tough to actually use this as a predictor… What happened on June 13 2013? Display the spatial pattern of delays, and then use Google to cross-reference with the weather. Looks like East coast is getting hammered and flights arriving from Atlanta an similar locations were very delayed. Guessing either weather issue, or problem in Atl or delta. 13.5 Filtering joins 13.5.1 What does it mean for a flight to have a missing tailnum? All flights with a missing tailnum in the flights table were cancelled as you can see below. flights %&gt;% count(is.na(tailnum), is.na(arr_delay)) ## # A tibble: 3 x 3 ## `is.na(tailnum)` `is.na(arr_delay)` n ## &lt;lgl&gt; &lt;lgl&gt; &lt;int&gt; ## 1 FALSE FALSE 327346 ## 2 FALSE TRUE 6918 ## 3 TRUE TRUE 2512 What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.) flights %&gt;% anti_join(planes, by=&quot;tailnum&quot;) %&gt;% count(carrier, sort = TRUE) ## # A tibble: 10 x 2 ## carrier n ## &lt;chr&gt; &lt;int&gt; ## 1 MQ 25397 ## 2 AA 22558 ## 3 UA 1693 ## 4 9E 1044 ## 5 B6 830 ## 6 US 699 ## 7 FL 187 ## 8 DL 110 ## 9 F9 50 ## 10 WN 38 flights %&gt;% mutate(in_planes = tailnum %in% planes$tailnum) %&gt;% group_by(carrier) %&gt;% summarise(flights_inPlanes = sum(in_planes), n = n(), perc_inPlanes = flights_inPlanes / n) %&gt;% ungroup() ## # A tibble: 16 x 4 ## carrier flights_inPlanes n perc_inPlanes ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9E 17416 18460 0.943 ## 2 AA 10171 32729 0.311 ## 3 AS 714 714 1 ## 4 B6 53805 54635 0.985 ## 5 DL 48000 48110 0.998 ## 6 EV 54173 54173 1 ## 7 F9 635 685 0.927 ## 8 FL 3073 3260 0.943 ## 9 HA 342 342 1 ## 10 MQ 1000 26397 0.0379 ## 11 OO 32 32 1 ## 12 UA 56972 58665 0.971 ## 13 US 19837 20536 0.966 ## 14 VX 5162 5162 1 ## 15 WN 12237 12275 0.997 ## 16 YV 601 601 1 Some carriers do not have many of their tailnums data in the planes table. (Come back.) Filter flights to only show flights with planes that have flown at least 100 flights. planes_many &lt;- flights %&gt;% count(tailnum, sort=TRUE) %&gt;% filter(n &gt; 100) semi_join(flights, planes_many) ## Joining, by = &quot;tailnum&quot; ## # A tibble: 229,202 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 544 545 -1 1004 ## 4 2013 1 1 554 558 -4 740 ## 5 2013 1 1 555 600 -5 913 ## 6 2013 1 1 557 600 -3 709 ## 7 2013 1 1 557 600 -3 838 ## 8 2013 1 1 558 600 -2 849 ## 9 2013 1 1 558 600 -2 853 ## 10 2013 1 1 558 600 -2 923 ## # ... with 229,192 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; add_count() is another helpful function that could have been used here Combine fueleconomy::vehicles and fueleconomy::common to find only the records for the most common models. fueleconomy::vehicles %&gt;% semi_join(fueleconomy::common, by=c(&quot;make&quot;, &quot;model&quot;)) ## # A tibble: 14,531 x 12 ## id make model year class trans drive cyl displ fuel hwy cty ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1833 Acura Integ~ 1986 Subc~ Auto~ Fron~ 4 1.6 Regu~ 28 22 ## 2 1834 Acura Integ~ 1986 Subc~ Manu~ Fron~ 4 1.6 Regu~ 28 23 ## 3 3037 Acura Integ~ 1987 Subc~ Auto~ Fron~ 4 1.6 Regu~ 28 22 ## 4 3038 Acura Integ~ 1987 Subc~ Manu~ Fron~ 4 1.6 Regu~ 28 23 ## 5 4183 Acura Integ~ 1988 Subc~ Auto~ Fron~ 4 1.6 Regu~ 27 22 ## 6 4184 Acura Integ~ 1988 Subc~ Manu~ Fron~ 4 1.6 Regu~ 28 23 ## 7 5303 Acura Integ~ 1989 Subc~ Auto~ Fron~ 4 1.6 Regu~ 27 22 ## 8 5304 Acura Integ~ 1989 Subc~ Manu~ Fron~ 4 1.6 Regu~ 28 23 ## 9 6442 Acura Integ~ 1990 Subc~ Auto~ Fron~ 4 1.8 Regu~ 24 20 ## 10 6443 Acura Integ~ 1990 Subc~ Manu~ Fron~ 4 1.8 Regu~ 26 21 ## # ... with 14,521 more rows Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns? First: Create two variables that together capture all 48 hour time windows across the year, at the day window of granularity (e.g. the time of day the flight takes off does not matter in establishing time windows for this example, only the day). Second: Gather these time windows into a single dataframe (note that this will increase the length of your data by ~364/365 * 100 %) Third: Group by window_start_date and calculate average arr_delay and related metrics. delays_windows &lt;- flights %&gt;% #First mutate(date_flight = lubridate::as_date(time_hour)) %&gt;% mutate(startdate_window1 = cut.Date(date_flight, &quot;2 day&quot;)) %&gt;% mutate(date_flight2 = ifelse(!(date_flight == min(date_flight, na.rm = TRUE)), date_flight, NA), date_flight2 = lubridate::as_date(date_flight2), startdate_window2 = cut.Date(date_flight2, &quot;2 day&quot;)) %&gt;% select(-date_flight, -date_flight2) %&gt;% #Second gather(startdate_window1, startdate_window2, key = &quot;start_window&quot;, value = &quot;window_start_date&quot;) %&gt;% filter(!is.na(window_start_date)) %&gt;% #Third group_by(window_start_date) %&gt;% summarise(num = n(), perc_cancelled = sum(is.na(arr_delay)) / n(), mean_delay = mean(arr_delay, na.rm = TRUE), perc_delay = mean(arr_delay &gt; 0, na.rm = TRUE), total_delay_mins = sum(arr_delay, na.rm = TRUE)) %&gt;% ungroup() ## Warning: attributes are not identical across measure variables; ## they will be dropped #don&#39;t worry about warning of &#39;attributes are not identical...&#39;, that is #because the cut function assigns attributes to the value, it&#39;s fine if #these are dropped here. Create tibble of worst 2-day period for mean arr_delay WorstWindow &lt;- delays_windows %&gt;% mutate(mean_delay_rank = dplyr::min_rank(-mean_delay)) %&gt;% filter(mean_delay_rank &lt;= 1) WorstDates &lt;- tibble(dates = c(lubridate::as_date(WorstWindow$window_start_date), lubridate::as_date(WorstWindow$window_start_date) + lubridate::duration(1, &quot;days&quot;))) Ammend weather data so that weather is an average across three NY locations rather than seperate for each30 weather_ammended &lt;- weather %&gt;% mutate(time_hour = lubridate::make_datetime(year, month, day, hour)) %&gt;% select(-one_of(&quot;origin&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;)) %&gt;% group_by(time_hour) %&gt;% summarise_all(mean, na.rm = TRUE) %&gt;% ungroup() Filtering join to just times weather for worst 2 days weather_worst &lt;- weather_ammended %&gt;% mutate(dates = as_date(time_hour)) %&gt;% semi_join(WorstDates) ## Joining, by = &quot;dates&quot; Plot of hourly weather values across 48 hour time windows. weather_worst %&gt;% select(-dates) %&gt;% gather(temp:visib, key = &quot;weather_type&quot;, value = &quot;weather_value&quot;) %&gt;% ggplot(aes(x = time_hour, y = weather_value))+ geom_line()+ facet_grid(weather_type ~ ., scales = &quot;free_y&quot;)+ labs(title = &#39;Hourly weather values across worst 48 hours of delays&#39;) Patterns: wind_gust and wind_speed are the same. See high level of colinearity in spikes and changes, e.g. increase in precip corresponds with decrease in visib and perhaps uptick in wind_spee Perhaps, we want to view how the average hourly weather values compare on the worst days to average weather days. Create summary of average hourly weather values for worst 48 hour period, for average period, and then append these and plot. bind_rows( weather_worst %&gt;% summarise_at(vars(temp:visib), mean, na.rm = TRUE) %&gt;% mutate(category = &quot;weather on worst 48&quot;) %&gt;% gather(temp:visib, key = weather_type, value = weather_val) , weather_ammended %&gt;% summarise_at(vars(temp:visib), mean, na.rm = TRUE) %&gt;% mutate(category = &quot;weather on average&quot;) %&gt;% gather(temp:visib, key = weather_type, value = weather_val) ) %&gt;% ggplot(aes(x = category, y = weather_val, fill = category))+ geom_col()+ facet_wrap(~weather_type, scales = &quot;free_y&quot;)+ labs(title = &quot;Hourly average weather values on worst 48 hour window of delays vs. hourly average weather across year&quot;, caption = &quot;Note that delays are based on mean(arr_delay, na.rm = TRUE)&quot;) For this to be the worst 48 hour period, the weather doesn’t actually seem to be as extreme as I would have guessed. Let’s add-in average arr_delay by planned departure time to this to see how the delay times throughout the day varied, to see if there was a surge or change in weather that led to the huge change in delays. flights %&gt;% mutate(dates = as_date(time_hour)) %&gt;% semi_join(WorstDates) %&gt;% group_by(time_hour) %&gt;% summarise(value = mean(arr_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(value_type = &quot;Mean_ArrDelay&quot;) %&gt;% bind_rows( weather_worst %&gt;% select(-dates) %&gt;% gather(temp:visib, key = &quot;value_type&quot;, value = &quot;value&quot;) ) %&gt;% mutate(weather_attr = !(value_type == &quot;Mean_ArrDelay&quot;), value_type = forcats::fct_relevel(value_type, &quot;Mean_ArrDelay&quot;)) %&gt;% ggplot(aes(x = time_hour, value, colour = weather_attr))+ geom_line()+ facet_grid(value_type ~ ., scales = &quot;free_y&quot;)+ labs(title = &#39;Hourly weather and delay values across worst 48 hours of delays&#39;) ## Joining, by = &quot;dates&quot; Maybe that first uptick in precipitation corresponded with the increase in delay… but still, looks extreme like an incident caused this. I cheched the news and it looks like a plane was crash landed onto the tarmac at one of the airports on this day https://en.wikipedia.org/wiki/Southwest_Airlines_Flight_345#cite_note-DMN_Aircraft_Totaled_20160808-4 , I checked the incident time and it occurred at 17:45 Jul 22, looks like it overlaps with the time we see the uptick in delays. I show plots and models of 48 hour time windows in a variety of other contexts and detail in Appendix What does anti_join(flights, airports, by = c(\"dest\" = \"faa\")) tell you? What does anti_join(airports, flights, by = c(\"faa\" = \"dest\")) tell you? anti_join(flights, airports, by = c(\"dest\" = \"faa\")) – tells me the flight dests missing an airport anti_join(airports, flights, by = c(\"faa\" = \"dest\")) – tells me the airports with no flights coming to them You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above. tail_carr &lt;- flights %&gt;% filter(!is.na(tailnum)) %&gt;% distinct(carrier, tailnum) %&gt;% count(tailnum, sort=TRUE) tail_carr %&gt;% filter(n &gt; 1) ## # A tibble: 17 x 2 ## tailnum n ## &lt;chr&gt; &lt;int&gt; ## 1 N146PQ 2 ## 2 N153PQ 2 ## 3 N176PQ 2 ## 4 N181PQ 2 ## 5 N197PQ 2 ## 6 N200PQ 2 ## 7 N228PQ 2 ## 8 N232PQ 2 ## 9 N933AT 2 ## 10 N935AT 2 ## 11 N977AT 2 ## 12 N978AT 2 ## 13 N979AT 2 ## 14 N981AT 2 ## 15 N989AT 2 ## 16 N990AT 2 ## 17 N994AT 2 You should reject that hypothesis, you can see that 17 tailnums are duplicated on multiple carriers. Below is code to show those 17 tailnums flights %&gt;% distinct(carrier, tailnum) %&gt;% filter(!is.na(tailnum)) %&gt;% group_by(tailnum) %&gt;% mutate(n_tail = n()) %&gt;% ungroup() %&gt;% filter(n_tail &gt; 1) %&gt;% arrange(desc(n_tail), tailnum) ## # A tibble: 34 x 3 ## carrier tailnum n_tail ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 9E N146PQ 2 ## 2 EV N146PQ 2 ## 3 9E N153PQ 2 ## 4 EV N153PQ 2 ## 5 9E N176PQ 2 ## 6 EV N176PQ 2 ## 7 9E N181PQ 2 ## 8 EV N181PQ 2 ## 9 9E N197PQ 2 ## 10 EV N197PQ 2 ## # ... with 24 more rows Appendix 13.5.1.4 Graph all of these metrics at once using roughly the same method as used on 13.4.6 #4. delays_windows %&gt;% gather(perc_cancelled, mean_delay, perc_delay, key = value_type, value = val) %&gt;% mutate(window_start_date = lubridate::as_date(window_start_date)) %&gt;% ggplot(aes(window_start_date, val))+ geom_line()+ facet_wrap(~value_type, scales = &quot;free_y&quot;, ncol = 1)+ scale_x_date(date_labels = &quot;%b %d&quot;)+ labs(title = &#39;Measures of delay across 48 hour time windows&#39;) Create 48 hour windows for weather data. Follow exact same steps as above. weather_windows &lt;- weather_ammended %&gt;% mutate(date_flight = lubridate::as_date(time_hour)) %&gt;% mutate(startdate_window1 = cut.Date(date_flight, &quot;2 day&quot;)) %&gt;% mutate(date_flight2 = ifelse(!(date_flight == min(date_flight, na.rm = TRUE)), date_flight, NA), date_flight2 = lubridate::as_date(date_flight2), startdate_window2 = cut.Date(date_flight2, &quot;2 day&quot;)) %&gt;% select(-date_flight, -date_flight2) %&gt;% #Second gather(startdate_window1, startdate_window2, key = &quot;start_window&quot;, value = &quot;window_start_date&quot;) %&gt;% filter(!is.na(window_start_date)) %&gt;% #Third group_by(window_start_date) %&gt;% summarise_at(vars(temp:visib), mean, na.rm = TRUE) %&gt;% ungroup() %&gt;% select(-wind_gust) ## Warning: attributes are not identical across measure variables; ## they will be dropped Graph using same method as above… weather_windows %&gt;% gather(temp:visib, key = weather_type, value = val) %&gt;% mutate(window_start_date = lubridate::as_date(window_start_date)) %&gt;% ggplot(aes(x = window_start_date, y = val))+ geom_line()+ facet_wrap(~weather_type, ncol = 1, scales = &quot;free_y&quot;)+ scale_x_date(date_labels = &quot;%b %d&quot;)+ labs(title = &#39;Measures of weather across 48 hour time windows&#39;) Connect delays and weather data weather_delay_joined &lt;- left_join(delays_windows, weather_windows, by = &quot;window_start_date&quot;) %&gt;% select(mean_delay, temp:visib, window_start_date) %&gt;% select(-dewp) %&gt;% #is almost completely correlated with temp so removed one of them... na.omit() Plot of 48 hour window of weather scores against mean delay keeping intact order of observations weather_delay_joined %&gt;% gather(mean_delay, temp:visib, key = value_type, value = val) %&gt;% mutate(window_start_date = lubridate::as_date(window_start_date), value_type = forcats::fct_relevel(value_type, &quot;mean_delay&quot;)) %&gt;% ggplot(aes(x = window_start_date, y = val, colour = ! value_type == &quot;mean_delay&quot;))+ geom_line()+ facet_wrap(~value_type, scales = &quot;free_y&quot;, ncol = 1)+ labs(colour = &quot;Weather value&quot;, title = &quot;Mean delay and weather value in 2-day rolling window&quot;) Plot of mean_delay against weather type, each point representing a different ‘window’ weather_delay_joined %&gt;% gather(temp:visib, key = weather_type, value = weather_val) %&gt;% ggplot(aes(x = weather_val, y = mean_delay))+ geom_point()+ geom_smooth()+ facet_wrap(~weather_type, scales = &quot;free_x&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; In a sense, these plots are not really valid as they obscure the fact that each point is not an independent observation (because there is a high level of association with w/e the value was on a single day with what it was in the previous day). E.g. mean_delay has a correlation of ~ 0.68 with prior days value as shown below… This is often ignored and we can also ignore it for now as it gets into time series and things we don’t need to worry about for now… but somthing to be aware… weather_delay_joined %&gt;% mutate(mean_delay_lag = lag(mean_delay)) %&gt;% select(mean_delay, mean_delay_lag) %&gt;% na.omit() %&gt;% cor() ## mean_delay mean_delay_lag ## mean_delay 1.0000000 0.6795631 ## mean_delay_lag 0.6795631 1.0000000 Data is not Independent (as mentioned above) and many problems associated with this… but let’s ignore this for now and just look at a few statisitics… Can see below that raw correlation of mean_delay is highest with humid. weather_delay_joined %&gt;% select(-window_start_date) %&gt;% cor() ## mean_delay temp humid wind_dir wind_speed ## mean_delay 1.00000000 0.08515338 0.4549140 -0.05371522 0.16262585 ## temp 0.08515338 1.00000000 0.3036520 -0.25906906 -0.40160692 ## humid 0.45491403 0.30365205 1.0000000 -0.51010505 -0.30383181 ## wind_dir -0.05371522 -0.25906906 -0.5101050 1.00000000 0.50039832 ## wind_speed 0.16262585 -0.40160692 -0.3038318 0.50039832 1.00000000 ## precip 0.36475598 0.02775525 0.4481898 -0.12853817 0.11176053 ## pressure -0.31716918 -0.23873857 -0.2363718 -0.26627495 -0.25716938 ## visib -0.38740156 0.12290097 -0.6647598 0.26307685 0.05275072 ## precip pressure visib ## mean_delay 0.36475598 -0.3171692 -0.38740156 ## temp 0.02775525 -0.2387386 0.12290097 ## humid 0.44818978 -0.2363718 -0.66475984 ## wind_dir -0.12853817 -0.2662749 0.26307685 ## wind_speed 0.11176053 -0.2571694 0.05275072 ## precip 1.00000000 -0.2265636 -0.44400337 ## pressure -0.22656357 1.0000000 0.12032520 ## visib -0.44400337 0.1203252 1.00000000 When accounting for other variables, see relationship with windspeed seems to emerge as important… weather_delay_joined %&gt;% select(-window_start_date) %&gt;% lm(mean_delay ~ ., data = .) %&gt;% summary() ## ## Call: ## lm(formula = mean_delay ~ ., data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.179 -7.581 -1.374 5.271 38.008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 169.56872 132.07737 1.284 0.2000 ## temp 0.05460 0.04702 1.161 0.2464 ## humid 0.48158 0.09088 5.299 2.04e-07 *** ## wind_dir 0.01420 0.01376 1.032 0.3026 ## wind_speed 1.15641 0.25561 4.524 8.28e-06 *** ## precip 140.84141 78.84192 1.786 0.0749 . ## pressure -0.19722 0.12476 -1.581 0.1148 ## visib -1.15009 0.80567 -1.427 0.1543 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.64 on 356 degrees of freedom ## Multiple R-squared: 0.3332, Adjusted R-squared: 0.3201 ## F-statistic: 25.42 on 7 and 356 DF, p-value: &lt; 2.2e-16 There a variety of reasons31 you may want to evaluate how the change in an attribute relates to the change in another attribute. In the cases below I plot the diffs for example: (average value on 2013-02-07 to 2013-02-08) - (average value on 2013-02-08 to 2013-02-09) Note that the time windows are not distinct but overlap by 24 hours. If doing a thorough account of time-series you would do a lot more than I show below… weather_delay_joined %&gt;% gather(mean_delay, temp:visib, key = value_type, value = val) %&gt;% mutate(window_start_date = lubridate::as_date(window_start_date), value_type = forcats::fct_relevel(value_type, &quot;mean_delay&quot;)) %&gt;% group_by(value_type) %&gt;% mutate(value_diff = val - lag(val)) %&gt;% ggplot(aes(x = window_start_date, y = value_diff, colour = !value_type == &quot;mean_delay&quot;))+ geom_line()+ facet_wrap(~value_type, scales = &quot;free_y&quot;, ncol = 1)+ labs(colour = &quot;Weather value&quot;, title = &quot;Plot of diffs in value&quot;) ## Warning: Removed 2 rows containing missing values (geom_path). Let’s plot these diffs as a scatter plot now (no longer looking at the order in which the observations emerged) weather_delay_joined %&gt;% gather(temp:visib, key = weather_type, value = val) %&gt;% group_by(weather_type) %&gt;% mutate(weather_diff = val - lag(val), delay_diff = mean_delay - lag(mean_delay)) %&gt;% ungroup() %&gt;% ggplot(aes(x = weather_diff, y = delay_diff))+ geom_point()+ geom_smooth()+ facet_wrap(~weather_type, scales = &quot;free_x&quot;)+ labs(title = &quot;scatter plot of diffs in value&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 7 rows containing non-finite values (stat_smooth). ## Warning: Removed 7 rows containing missing values (geom_point). Let’s look at the correlatioin and regression against these diffs diff_data &lt;- weather_delay_joined %&gt;% gather(mean_delay, temp:visib, key = value_type, value = val) %&gt;% group_by(value_type) %&gt;% mutate(diff = val - lag(val)) %&gt;% ungroup() %&gt;% select(-val) %&gt;% spread(key = value_type, value = diff) diff_data %&gt;% select(-window_start_date) %&gt;% na.omit() %&gt;% cor() ## humid mean_delay precip pressure temp ## humid 1.0000000 0.54331654 0.48014091 -0.3427556 0.318534448 ## mean_delay 0.5433165 1.00000000 0.51510649 -0.3247584 0.150601446 ## precip 0.4801409 0.51510649 1.00000000 -0.3014413 0.074916969 ## pressure -0.3427556 -0.32475840 -0.30144131 1.0000000 -0.488629288 ## temp 0.3185344 0.15060145 0.07491697 -0.4886293 1.000000000 ## visib -0.7393902 -0.53844191 -0.49795469 0.2721685 -0.206815887 ## wind_dir -0.4978895 -0.20689204 -0.20823801 -0.2443716 -0.003608694 ## wind_speed -0.1964910 0.05738881 0.15742776 -0.3687487 -0.085437521 ## visib wind_dir wind_speed ## humid -0.73939024 -0.497889528 -0.19649100 ## mean_delay -0.53844191 -0.206892045 0.05738881 ## precip -0.49795469 -0.208238012 0.15742776 ## pressure 0.27216848 -0.244371617 -0.36874869 ## temp -0.20681589 -0.003608694 -0.08543752 ## visib 1.00000000 0.378625695 0.06152223 ## wind_dir 0.37862569 1.000000000 0.43970745 ## wind_speed 0.06152223 0.439707451 1.00000000 diff_data %&gt;% select(-window_start_date) %&gt;% lm(mean_delay ~ ., data = .) %&gt;% summary() ## ## Call: ## lm(formula = mean_delay ~ ., data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.843 -4.394 -0.189 3.749 27.177 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.022454 0.460301 -0.049 0.961121 ## humid 0.281416 0.082305 3.419 0.000701 *** ## precip 324.087906 63.453719 5.107 5.34e-07 *** ## pressure -0.275033 0.149084 -1.845 0.065895 . ## temp -0.127570 0.143134 -0.891 0.373394 ## visib -2.420046 0.728749 -3.321 0.000991 *** ## wind_dir 0.002373 0.012316 0.193 0.847329 ## wind_speed 0.128749 0.226138 0.569 0.569487 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.77 on 355 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.4111, Adjusted R-squared: 0.3995 ## F-statistic: 35.4 on 7 and 355 DF, p-value: &lt; 2.2e-16 note that a weighted average based on traffic would be more appropriate here because the mean_delay values will be weighted by number of flights going through each – hopefully lack of substantial difference between locatioins means this won’t be too impactful…↩ Especially in cases where your observations are not independent↩ "],
["14-strings.html", "Ch. 14: Strings 14.2: String basics 14.3: Matching patterns w/ regex 14.4 Tools 14.5: Other types of patterns 14.7: stringi Appendix", " Ch. 14: Strings Key questions: 14.2.5. #3, 6 14.3.2.1. #2 14.3.3.1 #1, #2 Functions and notes: writeLines: see raw contents of a string (prints each string in a vector on a new line) str_length: number of characters in a string str_c: combine two or more strings use collapse arg to make vector of strings to single string str_replace_na: print NA as “NA” str_sub: start and end args to specify position to remove (or replace), can use negative numbers as well to represent from back str_to_lower, str_to_upper, str_to_upper: for changing string case locale arg (to handle slight differences in characters) str_order, str_sort: more robust version of order and sort which take allow a locale argument str_view, str_view_all: shows how character and regular expression match \\d: matches any digit. \\s: matches any whitespace (e.g. space, tab, newline). [abc]: matches a, b, or c. [^abc]: matches anything except a, b, or c. {n}: exactly n {n,}: n or more {,m}: at most m {n,m}: between n and m str_detect: returns logical vector of TRUE/FALSE values str_subset: subset of TRUE values from str_detect str_count: number of matches in a string str_extract: extract actual text of a match str_extract_all: returns list with all matches simplify = TRUE returns a matrix str_match: similar to str_extract but gives each individual component of match in a matrix, rather than a character vector (also have a str_match_all) tidyr::extract: like str_match but name columns with matches which are moved into new columns str_replace, str_replace_all: replace matches with new strings str_split split a string into pieces – default is individual words (returns list) simplify = TRUE again will return a matrix boundary use to specify level of split, e.g. str_view_all(x, boundary(\"word\")) str_locate, str_locate_all: gives starting an dending positions of each match regex use in match to specify more options, e.g. str_view(bananas, regex(\"banana\", ignore_case = TRUE)) multiline = TRUE allows ^ and $ to match start and end of each line (rather than of string) comments = TRUE allows you to add comments on a complex regular expression dotall = TRUE allows . to match more than just letters e.g. \\\\n fixed, coll related alternatives to regex apropos searches all objects available from global environment (e.g. say you can’t remember function name) dir: lists all files in a directory pattern arg takes a regex stringi more comprehensive package than stringr (~5x as many funs) 14.2: String basics Use wrteLines to show what string ‘This string has a \\n new line’ looks like printed. string_exp &lt;- &#39;This string has a \\n new line&#39; print(string_exp) ## [1] &quot;This string has a \\n new line&quot; writeLines(string_exp) ## This string has a ## new line To see full list of specifal characters: ?&#39;&quot;&#39; Objects of length 0 are silently dropped. This is particularly useful in conjunction with if: name &lt;- &quot;Bryan&quot; time_of_day &lt;- &quot;morning&quot; birthday &lt;- FALSE str_c( &quot;Good &quot;, time_of_day, &quot; &quot;, name, if (birthday) &quot; and HAPPY BIRTHDAY&quot;, &quot;.&quot; ) ## [1] &quot;Good morning Bryan.&quot; Collapse vectors into single string str_c(c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), collapse = &quot;, &quot;) ## [1] &quot;xa, yb, zc&quot; Can use assignment form of str_sub() x &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;) str_sub(x, 1, 1) &lt;- str_to_lower(str_sub(x, 1, 1)) x ## [1] &quot;apple&quot; &quot;banana&quot; &quot;pear&quot; str_pad looks interesting str_pad(&quot;the dogs come for you.&quot;, width = 40, pad = &quot;,&quot;, side = &quot;both&quot;) #must specify width =, side = default is left ## [1] &quot;,,,,,,,,,the dogs come for you.,,,,,,,,,&quot; 14.2.5 In code that doesn’t use stringr, you’ll often see paste() and paste0(). What’s the difference between the two functions? paste0() has no sep argument and just appends any value provided like another string vector. They differ from str_c() in that they automatically convert NA values to character. paste(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, c(&quot;x&quot;, &quot;y&quot;), sep = &quot;-&quot;) ## [1] &quot;a-b-c-x&quot; &quot;a-b-c-y&quot; paste0(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, c(&quot;x&quot;, &quot;y&quot;), sep = &quot;-&quot;) ## [1] &quot;abcx-&quot; &quot;abcy-&quot; What stringr function are they equivalent to? paste() and paste0() are similar to str_c() though are different in how they handle NAs (see below). They also will return a warning when recycling vectors whose legth do not have a common factor. paste(c(&quot;a&quot;, &quot;b&quot;, &quot;x&quot;), c(&quot;x&quot;, &quot;y&quot;), sep = &quot;-&quot;) ## [1] &quot;a-x&quot; &quot;b-y&quot; &quot;x-x&quot; str_c(c(&quot;a&quot;, &quot;b&quot;, &quot;x&quot;), c(&quot;x&quot;, &quot;y&quot;), sep = &quot;-&quot;) ## Warning in stri_c(..., sep = sep, collapse = collapse, ignore_null = TRUE): ## longer object length is not a multiple of shorter object length ## [1] &quot;a-x&quot; &quot;b-y&quot; &quot;x-x&quot; How do the functions differ in their handling of NA? paste(c(&quot;a&quot;, &quot;b&quot;), c(NA, &quot;y&quot;), sep = &quot;-&quot;) ## [1] &quot;a-NA&quot; &quot;b-y&quot; str_c(c(&quot;a&quot;, &quot;b&quot;), c(NA, &quot;y&quot;), sep = &quot;-&quot;) ## [1] NA &quot;b-y&quot; In your own words, describe the difference between the sep and collapse arguments to str_c(). sep puts characters between items within a vector, collapse puts a character between vectors being collapsed Use str_length() and str_sub() to extract the middle character from a string. x &lt;- &quot;world&quot; str_sub(x, start = ceiling(str_length(x) / 2), end = ceiling(str_length(x) / 2)) ## [1] &quot;r&quot; What will you do if the string has an even number of characters? In this circumstance the above solution would take the anterior middle value, below is a solution that would return both middle values. x &lt;- &quot;worlds&quot; str_sub(x, ceiling(str_length(x) / 2 + 1), start = ceiling(str_length(x) / 2 + 1)) ## [1] &quot;l&quot; str_sub(x, start = ifelse(str_length(x) %% 2 == 0, floor(str_length(x) / 2), ceiling(str_length(x) / 2 )), end = floor(str_length(x) / 2) + 1) ## [1] &quot;rl&quot; What does str_wrap() do? When might you want to use it? Use indent for first line, exdent for others could use str_wrap() for editing of documents etc., setting width = 1 will give each word its own line str_wrap(&quot;Tonight, we dine in Hell.&quot;, width = 10, indent = 0, exdent = 3) %&gt;% writeLines() ## Tonight, ## we dine in ## Hell. What does str_trim() do? What’s the opposite of str_trim()? Removes whitespace from beginning and end of character, side argument specifies which side str_trim(&quot; so much white space &quot;, side = &quot;right&quot;) # (default is &#39;both&#39;) ## [1] &quot; so much white space&quot; Write a function that turns (e.g.) a vector c(\"a\", \"b\", \"c\") into the string a, b, and c. Think carefully about what it should do if given a vector of length 0, 1, or 2. vec_to_string &lt;- function(x) { #If 1 or 0 length vector if (length(x) &lt; 2) return(x) comma &lt;- ifelse(length(x) &gt; 2, &quot;, &quot;, &quot; &quot;) b &lt;- str_c(x, collapse = comma) #replace &#39;,&#39; with &#39;and&#39; in last str_sub(b,-(str_length(x)[length(x)] + 1), -(str_length(x)[length(x)] + 1)) &lt;- &quot; and &quot; return(b) } x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) vec_to_string(x) ## [1] &quot;a, b, c, and d&quot; 14.3: Matching patterns w/ regex x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;an&quot;) To match a literal \\ need \\\\\\\\ because both string and regex will escape it. x &lt;- &quot;a\\\\b&quot; writeLines(x) ## a\\b str_view(x,&quot;\\\\\\\\&quot;) Using \\b to set boundary between words (not used often) apropos(&quot;\\\\bsum\\\\b&quot;) ## [1] &quot;contr.sum&quot; &quot;sum&quot; apropos(&quot;^(sum)$&quot;) ## [1] &quot;sum&quot; Other special characters: \\d: matches any digit. \\s: matches any whitespace (e.g. space, tab, newline). [abc]: matches a, b, or c. [^abc]: matches anything except a, b, or c. Controlling number of times: ?: 0 or 1 +: 1 or more *: 0 or more {n}: exactly n {n,}: n or more {,m}: at most m {n,m}: between n and m By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them. This is an advanced feature of regular expressions, but it’s useful to know that it exists: x &lt;- &quot;1888 is the longest year in Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &#39;C{2,3}&#39;) str_view(x, &#39;C{2,3}?&#39;) 14.3.1.1 Explain why each of these strings don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\". \"\\\" -&gt; leaves open quote string because escapes quote \"\\\\\", -&gt; escapes second \\ so left with blank \"\\\\\\\" -&gt; third \\ escapes quote so left with open quote as well How would you match the sequence \"'\\? x &lt;- &quot;alfred\\&quot;&#39;\\\\goes&quot; writeLines(x) ## alfred&quot;&#39;\\goes str_view(x, &quot;\\\\\\&quot;&#39;\\\\\\\\&quot;) What patterns will the regular expression \\..\\..\\.. match? Would match 6 character string of following form “(dot)(anychar)(dot)(anychar)(dot)(anychar)” x &lt;- c(&quot;alf.r.e.dd.ss..lsdf.d.kj&quot;) str_view(x, pattern = &quot;\\\\..\\\\..\\\\..&quot;) How would you represent it as a string? x_pattern &lt;- &quot;\\\\..\\\\..\\\\..&quot; writeLines(x_pattern) ## \\..\\..\\.. 14.3.2.1 How would you match the literal string \"$^$\"? x &lt;- &quot;so it goes $^$ here&quot; str_view(x, &quot;\\\\$\\\\^\\\\$&quot;) Given the corpus of common words in stringr::words, create regular expressions that find all words that: Start with “y”. str_view(stringr::words, &quot;^y&quot;, match = TRUE) End with “x” str_view(stringr::words, &quot;x$&quot;, match = TRUE) Are exactly three letters long. (Don’t cheat by using str_length()!) str_view(stringr::words, &quot;^...$&quot;, match = TRUE) Have seven letters or more. str_view(stringr::words, &quot;.......&quot;, match = TRUE) Since this list is long, you might want to use the match argument to str_view() to show only the matching or non-matching words. 14.3.3.1 Create regular expressions to find all words that: Start with a vowel. str_view(stringr::words, &quot;^[aeiou]&quot;, match = TRUE) That only contain consonants. (Hint: thinking about matching “not”-vowels.) str_view(stringr::words, &quot;^[^aeiou]*[^aeiouy]$&quot;, match = TRUE) End with ed, but not with eed. str_view(stringr::words, &quot;[^e]ed$&quot;, match = TRUE) End with ing or ise. str_view(stringr::words, &quot;(ing|ise)$&quot;, match = TRUE) Empirically verify the rule “i before e except after c”. str_view(stringr::words, &quot;(^(ei))|cie|[^c]ei&quot;, match = TRUE) Is “q” always followed by a “u”? str_view(stringr::words, &quot;q[^u]&quot;, match = TRUE) of the words in list, yes. Write a regular expression that matches a word if it’s probably written in British English, not American English. str_view(stringr::words, &quot;(l|b)our|parat&quot;, match = TRUE) Create a regular expression that will match telephone numbers as commonly written in your country. x &lt;- c(&quot;dkl kls. klk. _&quot;, &quot;(425) 591-6020&quot;, &quot;her number is (581) 434-3242&quot;, &quot;442&quot;, &quot; dsi&quot;) str_view(x, &quot;\\\\(\\\\d\\\\d\\\\d\\\\)\\\\s\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d&quot;) Aboves not a good way to solve this, will see better methods in next section. 14.3.4.1 Describe the equivalents of ?, +, * in {m,n} form. ? : {0,1} + : {1, } * : {0, } Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.) ^.*$ : starts with anything, and ends with anything–matches whole thing str_view(x, &quot;^.*$&quot;) \"\\\\{.+\\\\}\" : match text in brackets greater than nothing x &lt;- c(&quot;test&quot;, &quot;some in {brackets}&quot;, &quot;just {} no match&quot;) str_view(x, &quot;\\\\{.+\\\\}&quot;) \\d{4}-\\d{2}-\\d{2}: 4 numbers - 2 numbers - 2 numbers x &lt;- c(&quot;4444-22-22&quot;, &quot;test&quot;, &quot;333-4444-22&quot;) str_view(x, &quot;\\\\d{4}-\\\\d{2}-\\\\d{2}&quot;) \"\\\\\\\\{4}\": 4 brackets x &lt;- c(&quot;\\\\\\\\\\\\\\\\&quot;, &quot;\\\\\\\\\\\\&quot;, &quot;\\\\\\\\&quot;, &quot;\\\\&quot;) writeLines(x) ## \\\\\\\\ ## \\\\\\ ## \\\\ ## \\ str_view(x, &quot;\\\\\\\\{4}&quot;) x &lt;- c(&quot;\\\\\\\\\\\\\\\\&quot;, &quot;\\\\\\\\\\\\&quot;, &quot;\\\\\\\\&quot;, &quot;\\\\&quot;) str_view(x, &quot;\\\\\\\\\\\\\\\\&quot;) Create regular expressions to find all words that: find all words that start with three consonants str_view(stringr::words, &quot;^[^aeoiouy]{3}&quot;, match = TRUE) Include y because when it shows up otherwise, is in vowel form. have three or more vowels in a row str_view(stringr::words, &quot;[aeiou]{3}&quot;, match = TRUE) In this case, do not include the y. have 2 or more vowel-consonant pairs in a row str_view(stringr::words, &quot;([aeiou][^aeiou]){2,}&quot;, match = TRUE) Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner. 14.3.5.1 Describe, in words, what these expressions will match: I change questions 1 and 3 to what I think they were meant to be written as (.)\\\\1\\\\1 and (.)\\\\1 respectively. (.)\\\\1\\\\1 : repeat the char in the first group, and then repeat that char again \"(.)(.)\\\\2\\\\1\" : 1st char, 2nd char followed by 2nd char, first char (..)\\\\1 : 2 chars repeated twice \"(.).\\\\1.\\\\1\" : chars shows-up 3 times with one character between each \"(.)(.)(.).*\\\\3\\\\2\\\\1\" : 3 chars in one order with * chars between, then 3 chars with 3 letters in the reverse order of what it started x &lt;- c(&quot;steefddff&quot;, &quot;ssdfsdfffsdasdlkd&quot;, &quot;DLKKJIOWdkl&quot;, &quot;klnlsd&quot;, &quot;t11&quot;, &quot;(.)\\1\\1&quot;) str_view_all(x, &quot;(.)\\\\1\\\\1&quot;, match = TRUE) #xxx str_view_all(fruit, &quot;(.)(.)\\\\2\\\\1&quot;, match = TRUE) #xyyx str_view_all(fruit, &quot;(..)\\\\1&quot;, match = TRUE) #xxyy str_view(stringr::words, &quot;(.).\\\\1.\\\\1&quot;, match = TRUE) #x.x.x str_view(stringr::words, &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot;, match = TRUE) #xyz.*zyx Construct regular expressions to match words that: Start and end with the same character. str_view(stringr::words, &quot;^(.).*\\\\1$&quot;, match = TRUE) Contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice.) str_view(stringr::words, &quot;(..).*\\\\1&quot;, match = TRUE) Contain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.) str_view(stringr::words, &quot;(.).+\\\\1.+\\\\1&quot;, match = TRUE) 14.4 Tools noun &lt;- &quot;(a|the) ([^ \\\\.]+)&quot; has_noun &lt;- sentences %&gt;% str_subset(noun) %&gt;% head(10) has_noun %&gt;% str_extract_all(noun, simplify = TRUE) #creates split into seperate pieces has_noun %&gt;% str_match_all(noun) #Can make dataframe with, but need to name all tibble(has_noun = has_noun) %&gt;% extract(has_noun, into = c(&quot;article&quot;, &quot;noun&quot;), regex = noun) When using boundary() with str_split can set to “character”, “line”, “sentence”, and “word” and gives alternative to splitting by pattern. 14.4.2 For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls. Find all words that start or end with x. str_subset(words, &quot;^x|x$&quot;) ## [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; Find all words that start with a vowel and end with a consonant. str_subset(words, &quot;^[aeiou].*[^aeiouy]$&quot;) ## [1] &quot;about&quot; &quot;accept&quot; &quot;account&quot; &quot;across&quot; &quot;act&quot; ## [6] &quot;actual&quot; &quot;add&quot; &quot;address&quot; &quot;admit&quot; &quot;affect&quot; ## [11] &quot;afford&quot; &quot;after&quot; &quot;afternoon&quot; &quot;again&quot; &quot;against&quot; ## [16] &quot;agent&quot; &quot;air&quot; &quot;all&quot; &quot;allow&quot; &quot;almost&quot; ## [21] &quot;along&quot; &quot;alright&quot; &quot;although&quot; &quot;always&quot; &quot;amount&quot; ## [26] &quot;and&quot; &quot;another&quot; &quot;answer&quot; &quot;apart&quot; &quot;apparent&quot; ## [31] &quot;appear&quot; &quot;appoint&quot; &quot;approach&quot; &quot;arm&quot; &quot;around&quot; ## [36] &quot;art&quot; &quot;as&quot; &quot;ask&quot; &quot;at&quot; &quot;attend&quot; ## [41] &quot;awful&quot; &quot;each&quot; &quot;east&quot; &quot;eat&quot; &quot;effect&quot; ## [46] &quot;egg&quot; &quot;eight&quot; &quot;either&quot; &quot;elect&quot; &quot;electric&quot; ## [51] &quot;eleven&quot; &quot;end&quot; &quot;english&quot; &quot;enough&quot; &quot;enter&quot; ## [56] &quot;environment&quot; &quot;equal&quot; &quot;especial&quot; &quot;even&quot; &quot;evening&quot; ## [61] &quot;ever&quot; &quot;exact&quot; &quot;except&quot; &quot;exist&quot; &quot;expect&quot; ## [66] &quot;explain&quot; &quot;express&quot; &quot;if&quot; &quot;important&quot; &quot;in&quot; ## [71] &quot;indeed&quot; &quot;individual&quot; &quot;inform&quot; &quot;instead&quot; &quot;interest&quot; ## [76] &quot;invest&quot; &quot;it&quot; &quot;item&quot; &quot;obvious&quot; &quot;occasion&quot; ## [81] &quot;odd&quot; &quot;of&quot; &quot;off&quot; &quot;offer&quot; &quot;often&quot; ## [86] &quot;old&quot; &quot;on&quot; &quot;open&quot; &quot;or&quot; &quot;order&quot; ## [91] &quot;original&quot; &quot;other&quot; &quot;ought&quot; &quot;out&quot; &quot;over&quot; ## [96] &quot;own&quot; &quot;under&quot; &quot;understand&quot; &quot;union&quot; &quot;unit&quot; ## [101] &quot;unless&quot; &quot;until&quot; &quot;up&quot; &quot;upon&quot; &quot;usual&quot; Counted y as a vowel if ending with, but not to start. This does not work perfect. For example words like ygritte would still be included even though y is activng as a vowel there whereas words like boy would be excluded even though acting as a consonant there. From here on out I am going to always exclude y. Are there any words that contain at least one of each different vowel? vowels &lt;- c(&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;) words[str_detect(words, &quot;a&quot;) &amp; str_detect(words, &quot;e&quot;) &amp; str_detect(words, &quot;i&quot;) &amp; str_detect(words, &quot;o&quot;) &amp; str_detect(words, &quot;u&quot;)] ## character(0) No. What word has the highest number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?) vowel_counts &lt;- tibble(words = words, n_string = str_length(words), n_vowel = str_count(words, vowels), prop_vowel = n_vowel / n_string) ‘Experience’ has the most vowels vowel_counts %&gt;% arrange(desc(n_vowel)) ## # A tibble: 980 x 4 ## words n_string n_vowel prop_vowel ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 experience 10 4 0.4 ## 2 individual 10 3 0.3 ## 3 achieve 7 2 0.286 ## 4 actual 6 2 0.333 ## 5 afternoon 9 2 0.222 ## 6 against 7 2 0.286 ## 7 already 7 2 0.286 ## 8 america 7 2 0.286 ## 9 benefit 7 2 0.286 ## 10 choose 6 2 0.333 ## # ... with 970 more rows ‘a’ has the highest proportion vowel_counts %&gt;% arrange(desc(prop_vowel)) ## # A tibble: 980 x 4 ## words n_string n_vowel prop_vowel ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 a 1 1 1 ## 2 too 3 2 0.667 ## 3 wee 3 2 0.667 ## 4 feed 4 2 0.5 ## 5 in 2 1 0.5 ## 6 look 4 2 0.5 ## 7 need 4 2 0.5 ## 8 room 4 2 0.5 ## 9 so 2 1 0.5 ## 10 soon 4 2 0.5 ## # ... with 970 more rows 14.4.3.1 In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a colour. Modify the regex to fix the problem. Add space in front of colors: colours &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) %&gt;% paste0(&quot; &quot;, .) colour_match &lt;- str_c(colours, collapse = &quot;|&quot;) more &lt;- sentences[str_count(sentences, colour_match) &gt; 1] str_view_all(more, colour_match) From the Harvard sentences data, extract: The first word from each sentence. str_extract(sentences, &quot;[A-z]*&quot;) All words ending in ing. #ends in &quot;ing&quot; or &quot;ing.&quot; sent_ing &lt;- str_subset(sentences, &quot;.*ing(\\\\.|\\\\s)&quot;) str_extract_all(sent_ing, &quot;[A-z]+ing&quot;, simplify=TRUE) All plurals. str_subset(sentences, &quot;[A-z]*s(\\\\.|\\\\s)&quot;) %&gt;% #take all sentences that have a word ending in s str_extract_all(&quot;[A-z]*s\\\\b&quot;, simplify = TRUE) %&gt;% .[str_length(.) &gt; 3] %&gt;% #get rid of the short words str_subset(&quot;.*[^s]s$&quot;) %&gt;% #get rid of words ending in &#39;ss&#39; str_subset(&quot;.*[^i]s$&quot;) #get rid of &#39;this&#39; 14.4.4.1 Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word. #Create regex expression nums &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;, &quot;six&quot;, &quot;seven&quot;, &quot;eight&quot;, &quot;nine&quot;) nums_c &lt;- str_c(nums, collapse = &quot;|&quot;) # see stringr cheatsheet: &quot;(?&lt;![:alpha:])&quot; means not preceded by re &lt;- str_c(&quot;(&quot;, &quot;(?&lt;![:alpha:])&quot;, &quot;(&quot;, nums_c, &quot;))&quot;, &quot; &quot;, &quot;([^ \\\\.]+)&quot;, sep = &quot;&quot;) sentences %&gt;% str_subset(regex(re, ignore_case = TRUE)) %&gt;% str_extract_all(regex(re, ignore_case = TRUE)) %&gt;% unlist() %&gt;% tibble::enframe(name = NULL) %&gt;% separate(col = &quot;value&quot;, into = c(&quot;num&quot;, &quot;following&quot;), remove = FALSE) ## # A tibble: 30 x 3 ## value num following ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Four hours Four hours ## 2 Two blue Two blue ## 3 seven books seven books ## 4 two met two met ## 5 two factors two factors ## 6 three lists three lists ## 7 Two plus Two plus ## 8 seven is seven is ## 9 two when two when ## 10 Eight miles Eight miles ## # ... with 20 more rows I’d initially appended \"\\\\b\" in front of each number to prevent things like “someone” being captured – however this didn’t work with cases where a sentence started with a number – hence switched to using the not preceded by method in the stringr cheatsheet. Find all contractions. Separate out the pieces before and after the apostrophe. #note the () facilitate the split with functions contr &lt;- &quot;([^ \\\\.]+)&#39;([^ \\\\.]*)&quot; sentences %&gt;% #note the improvement this word definition is to the above [^ ]+ str_subset(contr) %&gt;% str_match_all(contr) ## [[1]] ## [,1] [,2] [,3] ## [1,] &quot;It&#39;s&quot; &quot;It&quot; &quot;s&quot; ## ## [[2]] ## [,1] [,2] [,3] ## [1,] &quot;man&#39;s&quot; &quot;man&quot; &quot;s&quot; ## ## [[3]] ## [,1] [,2] [,3] ## [1,] &quot;don&#39;t&quot; &quot;don&quot; &quot;t&quot; ## ## [[4]] ## [,1] [,2] [,3] ## [1,] &quot;store&#39;s&quot; &quot;store&quot; &quot;s&quot; ## ## [[5]] ## [,1] [,2] [,3] ## [1,] &quot;workmen&#39;s&quot; &quot;workmen&quot; &quot;s&quot; ## ## [[6]] ## [,1] [,2] [,3] ## [1,] &quot;Let&#39;s&quot; &quot;Let&quot; &quot;s&quot; ## ## [[7]] ## [,1] [,2] [,3] ## [1,] &quot;sun&#39;s&quot; &quot;sun&quot; &quot;s&quot; ## ## [[8]] ## [,1] [,2] [,3] ## [1,] &quot;child&#39;s&quot; &quot;child&quot; &quot;s&quot; ## ## [[9]] ## [,1] [,2] [,3] ## [1,] &quot;king&#39;s&quot; &quot;king&quot; &quot;s&quot; ## ## [[10]] ## [,1] [,2] [,3] ## [1,] &quot;It&#39;s&quot; &quot;It&quot; &quot;s&quot; ## ## [[11]] ## [,1] [,2] [,3] ## [1,] &quot;don&#39;t&quot; &quot;don&quot; &quot;t&quot; ## ## [[12]] ## [,1] [,2] [,3] ## [1,] &quot;queen&#39;s&quot; &quot;queen&quot; &quot;s&quot; ## ## [[13]] ## [,1] [,2] [,3] ## [1,] &quot;don&#39;t&quot; &quot;don&quot; &quot;t&quot; ## ## [[14]] ## [,1] [,2] [,3] ## [1,] &quot;pirate&#39;s&quot; &quot;pirate&quot; &quot;s&quot; ## ## [[15]] ## [,1] [,2] [,3] ## [1,] &quot;neighbor&#39;s&quot; &quot;neighbor&quot; &quot;s&quot; 14.4.5.1 Replace all forward slashes in a string with backslashes. x &lt;- c(&quot;test/dklsk/&quot;) str_replace_all(x, &quot;/&quot;, &quot;\\\\\\\\&quot;) %&gt;% writeLines() ## test\\dklsk\\ Implement a simple version of str_to_lower() using replace_all(). x &lt;- c(&quot;BIdklsKOS&quot;) str_replace_all(x, &quot;([A-Z])&quot;, tolower) ## [1] &quot;bidklskos&quot; Switch the first and last letters in words. Which of those strings are still words? str_replace(words, &quot;(^.)(.*)(.$)&quot;, &quot;\\\\3\\\\2\\\\1&quot;) Any words that start and end with the same letter, e.g. ‘treat’, as well as a few other examples like, war –&gt; raw . 14.4.6.1 Split up a string like \"apples, pears, and bananas\" into individual components. x &lt;- &quot;apples, pears, and bananas&quot; str_split(x, &quot;,* &quot;) #note that regular expression works to handle commas as well ## [[1]] ## [1] &quot;apples&quot; &quot;pears&quot; &quot;and&quot; &quot;bananas&quot; Why is it better to split up by boundary(\"word\") than \" \"? Handles commas and punctuation32. str_split(x, boundary(&quot;word&quot;)) ## [[1]] ## [1] &quot;apples&quot; &quot;pears&quot; &quot;and&quot; &quot;bananas&quot; What does splitting with an empty string (\"\") do? Experiment, and then read the documentation. Splitting by an empty string splits up each character. str_split(x,&quot;&quot;) ## [[1]] ## [1] &quot;a&quot; &quot;p&quot; &quot;p&quot; &quot;l&quot; &quot;e&quot; &quot;s&quot; &quot;,&quot; &quot; &quot; &quot;p&quot; &quot;e&quot; &quot;a&quot; &quot;r&quot; &quot;s&quot; &quot;,&quot; &quot; &quot; &quot;a&quot; &quot;n&quot; ## [18] &quot;d&quot; &quot; &quot; &quot;b&quot; &quot;a&quot; &quot;n&quot; &quot;a&quot; &quot;n&quot; &quot;a&quot; &quot;s&quot; splits each character into an individual element (and creates elements for spaces between strings) 14.5: Other types of patterns regex args to know: ignore_case = TRUE allows characters to match either their uppercase or lowercase forms. This always uses the current locale. multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string. comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: \"\\\\ \". dotall = TRUE allows . to match everything, including \\n. Alternatives to regex(): * fixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. * coll(): compare strings using standard collation rules. This is useful for doing case insensitive matching. Note that coll() takes a locale parameter that controls which rules are used for comparing characters. 14.5.1 How would you find all strings containing \\ with regex() vs. with fixed()? would be \\\\ instead of \\\\\\\\ str_view_all(&quot;so \\\\ the party is on\\\\ right?&quot;, fixed(&quot;\\\\&quot;)) What are the five most common words in sentences? str_extract_all(sentences, boundary(&quot;word&quot;), simplify = TRUE) %&gt;% as_tibble() %&gt;% gather(V1:V12, value = &quot;words&quot;, key = &quot;order&quot;) %&gt;% mutate(words = str_to_lower(words)) %&gt;% filter(!words == &quot;&quot;) %&gt;% count(words, sort = TRUE) %&gt;% head(5) ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. ## # A tibble: 5 x 2 ## words n ## &lt;chr&gt; &lt;int&gt; ## 1 the 751 ## 2 a 202 ## 3 of 132 ## 4 to 123 ## 5 and 118 14.7: stringi Other functions: apropos searches all objects available from the global environment–useful if you can’t remember fun name. Check those that start with replace: apropos(&quot;^(replace)&quot;) ## [1] &quot;replace&quot; &quot;replace_na&quot; Check those that start with str, but not stri apropos(&quot;^(str)[^i]&quot;) ## [1] &quot;str_c&quot; &quot;str_conv&quot; &quot;str_count&quot; ## [4] &quot;str_detect&quot; &quot;str_dup&quot; &quot;str_extract&quot; ## [7] &quot;str_extract_all&quot; &quot;str_flatten&quot; &quot;str_glue&quot; ## [10] &quot;str_glue_data&quot; &quot;str_interp&quot; &quot;str_length&quot; ## [13] &quot;str_locate&quot; &quot;str_locate_all&quot; &quot;str_match&quot; ## [16] &quot;str_match_all&quot; &quot;str_order&quot; &quot;str_pad&quot; ## [19] &quot;str_remove&quot; &quot;str_remove_all&quot; &quot;str_replace&quot; ## [22] &quot;str_replace_all&quot; &quot;str_replace_na&quot; &quot;str_sort&quot; ## [25] &quot;str_split&quot; &quot;str_split_fixed&quot; &quot;str_squish&quot; ## [28] &quot;str_sub&quot; &quot;str_sub&lt;-&quot; &quot;str_subset&quot; ## [31] &quot;str_to_lower&quot; &quot;str_to_title&quot; &quot;str_to_upper&quot; ## [34] &quot;str_trim&quot; &quot;str_trunc&quot; &quot;str_view&quot; ## [37] &quot;str_view_all&quot; &quot;str_which&quot; &quot;str_wrap&quot; ## [40] &quot;strcapture&quot; &quot;strftime&quot; &quot;strheight&quot; ## [43] &quot;strOptions&quot; &quot;strptime&quot; &quot;strrep&quot; ## [46] &quot;strsplit&quot; &quot;strtoi&quot; &quot;strtrim&quot; ## [49] &quot;StructTS&quot; &quot;structure&quot; &quot;strwidth&quot; ## [52] &quot;strwrap&quot; 14.7.1 Find the stringi functions that: Count the number of words. – stri_count Find duplicated strings. – stri_duplicated Generate random text. – str_rand_strings How do you control the language that stri_sort() uses for sorting? The decreasing argument Appendix 14.4.2.3 One way of doing this using iteration methods: vowels &lt;- c(&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;) tibble(vowels = vowels, words = list(words)) %&gt;% mutate(detect_vowels = purrr::map2(words, vowels, str_detect)) %&gt;% spread(key = vowels, value = detect_vowels) %&gt;% unnest() %&gt;% mutate(unique_vowels = rowSums(.[2:6])) %&gt;% arrange(desc(unique_vowels)) ## # A tibble: 980 x 7 ## words a e i o u unique_vowels ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 absolute TRUE TRUE FALSE TRUE TRUE 4 ## 2 appropriate TRUE TRUE TRUE TRUE FALSE 4 ## 3 associate TRUE TRUE TRUE TRUE FALSE 4 ## 4 authority TRUE FALSE TRUE TRUE TRUE 4 ## 5 colleague TRUE TRUE FALSE TRUE TRUE 4 ## 6 continue FALSE TRUE TRUE TRUE TRUE 4 ## 7 encourage TRUE TRUE FALSE TRUE TRUE 4 ## 8 introduce FALSE TRUE TRUE TRUE TRUE 4 ## 9 organize TRUE TRUE TRUE TRUE FALSE 4 ## 10 previous FALSE TRUE TRUE TRUE TRUE 4 ## # ... with 970 more rows #seems that nothing gets over 4 I still sometimes prefer to use patterns where possible over boundary function. Regex is more generally applicabale as well outside of R.↩ "],
["15-factors.html", "Ch. 15: Factors 15.4: General Social Survey 15.4: Modifying factor order 15.5: Modifying factor levels Appendix", " Ch. 15: Factors Key questions: 15.3.1. #1, 3 (make the visualization and table) 15.5.1. #1 Functions and notes: factor make variable a factor based on levels provided fct_rev reverses order of factors fct_infreq orders levels in increasing frequency fct_relevel lets you move levels to front of order fct_inorder orders existing factor by order values show-up in in data fct_reorder orders input factors by other specified variables value (median by default), 3 inputs: f: factor to modify, x: input var to order by, fun: function to use on x, also have desc option fct_reorder2 orders input factor by max of other specified variable (good for making legends align as expected) fct_recode lets you change value of each level fct_collapse is variant of fct_recode that allows you to provide multiple old levels as a vector fct_lump allows you to lump together small groups, use n to specify number of groups to end with Create factors by order they come-in: Avoiding dropping levels with drop = FALSE gss_cat %&gt;% ggplot(aes(race))+ geom_bar()+ scale_x_discrete(drop = FALSE) 15.4: General Social Survey 15.3.1 Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot? Default bar chart has categories across the x-asix, I flipped these to be across the y-axis Also, have highest values at the bottom rather than at the top and have different version of NA showing-up at both top and bottom, all should be on one side In bar_prep, I used reg expressions to extract the numeric values, arrange by that, and then set factor levels according to the new order Solution is probably unnecessarily complicated… bar_prep &lt;- gss_cat %&gt;% tidyr::extract(col = rincome, into =c(&quot;dollars1&quot;, &quot;dollars2&quot;), &quot;([0-9]+)[^0-9]*([0-9]*)&quot;, remove = FALSE) %&gt;% mutate_at(c(&quot;dollars1&quot;, &quot;dollars2&quot;), ~ifelse(is.na(.) | . == &quot;&quot;, 0, as.numeric(.))) %&gt;% arrange(dollars1, dollars2) %&gt;% mutate(rincome = fct_inorder(rincome)) bar_prep %&gt;% ggplot(aes(x = rincome)) + geom_bar() + scale_x_discrete(drop = FALSE) + coord_flip() What is the most common relig in this survey? What’s the most common partyid? gss_cat %&gt;% count(relig, sort = TRUE) ## # A tibble: 15 x 2 ## relig n ## &lt;fct&gt; &lt;int&gt; ## 1 Protestant 10846 ## 2 Catholic 5124 ## 3 None 3523 ## 4 Christian 689 ## 5 Jewish 388 ## 6 Other 224 ## 7 Buddhism 147 ## 8 Inter-nondenominational 109 ## 9 Moslem/islam 104 ## 10 Orthodox-christian 95 ## 11 No answer 93 ## 12 Hinduism 71 ## 13 Other eastern 32 ## 14 Native american 23 ## 15 Don&#39;t know 15 gss_cat %&gt;% count(partyid, sort = TRUE) ## # A tibble: 10 x 2 ## partyid n ## &lt;fct&gt; &lt;int&gt; ## 1 Independent 4119 ## 2 Not str democrat 3690 ## 3 Strong democrat 3490 ## 4 Not str republican 3032 ## 5 Ind,near dem 2499 ## 6 Strong republican 2314 ## 7 Ind,near rep 1791 ## 8 Other party 393 ## 9 No answer 154 ## 10 Don&#39;t know 1 relig most common – Protestant, 10846, partyid most common – Independent, 4119 Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualisation? With visualization: gss_cat %&gt;% ggplot(aes(x=relig, fill=denom))+ geom_bar()+ coord_flip() Notice which have the widest variety of colours – are protestant, and Christian slightly With table: gss_cat %&gt;% count(relig, denom) %&gt;% count(relig, sort = TRUE) ## # A tibble: 15 x 2 ## relig n ## &lt;fct&gt; &lt;int&gt; ## 1 Protestant 29 ## 2 Christian 4 ## 3 Other 2 ## 4 No answer 1 ## 5 Don&#39;t know 1 ## 6 Inter-nondenominational 1 ## 7 Native american 1 ## 8 Orthodox-christian 1 ## 9 Moslem/islam 1 ## 10 Other eastern 1 ## 11 Hinduism 1 ## 12 Buddhism 1 ## 13 None 1 ## 14 Jewish 1 ## 15 Catholic 1 15.4: Modifying factor order 15.4.1 There are some suspiciously high numbers in tvhours. Is the mean a good summary? gss_cat %&gt;% mutate(tvhours_fct = factor(tvhours)) %&gt;% ggplot(aes(x = tvhours_fct)) + geom_bar() Distribution is reasonably skewed with some values showing-up as 24 hours which seems impossible, in addition to this we have a lot of NA values, this may skew results Given high number of missing values, tvhours may also just not be reliable, do NAs associate with other variables? – Perhaps could try and impute these NAs For each factor in gss_cat identify whether the order of the levels is arbitrary or principled. gss_cat %&gt;% purrr::keep(is.factor) %&gt;% purrr::map(levels) ## $marital ## [1] &quot;No answer&quot; &quot;Never married&quot; &quot;Separated&quot; &quot;Divorced&quot; ## [5] &quot;Widowed&quot; &quot;Married&quot; ## ## $race ## [1] &quot;Other&quot; &quot;Black&quot; &quot;White&quot; &quot;Not applicable&quot; ## ## $rincome ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; &quot;$25000 or more&quot; ## [5] &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; &quot;$8000 to 9999&quot; ## [9] &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; &quot;$4000 to 4999&quot; ## [13] &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; &quot;Not applicable&quot; ## ## $partyid ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Other party&quot; ## [4] &quot;Strong republican&quot; &quot;Not str republican&quot; &quot;Ind,near rep&quot; ## [7] &quot;Independent&quot; &quot;Ind,near dem&quot; &quot;Not str democrat&quot; ## [10] &quot;Strong democrat&quot; ## ## $relig ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; ## [3] &quot;Inter-nondenominational&quot; &quot;Native american&quot; ## [5] &quot;Christian&quot; &quot;Orthodox-christian&quot; ## [7] &quot;Moslem/islam&quot; &quot;Other eastern&quot; ## [9] &quot;Hinduism&quot; &quot;Buddhism&quot; ## [11] &quot;Other&quot; &quot;None&quot; ## [13] &quot;Jewish&quot; &quot;Catholic&quot; ## [15] &quot;Protestant&quot; &quot;Not applicable&quot; ## ## $denom ## [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;No denomination&quot; ## [4] &quot;Other&quot; &quot;Episcopal&quot; &quot;Presbyterian-dk wh&quot; ## [7] &quot;Presbyterian, merged&quot; &quot;Other presbyterian&quot; &quot;United pres ch in us&quot; ## [10] &quot;Presbyterian c in us&quot; &quot;Lutheran-dk which&quot; &quot;Evangelical luth&quot; ## [13] &quot;Other lutheran&quot; &quot;Wi evan luth synod&quot; &quot;Lutheran-mo synod&quot; ## [16] &quot;Luth ch in america&quot; &quot;Am lutheran&quot; &quot;Methodist-dk which&quot; ## [19] &quot;Other methodist&quot; &quot;United methodist&quot; &quot;Afr meth ep zion&quot; ## [22] &quot;Afr meth episcopal&quot; &quot;Baptist-dk which&quot; &quot;Other baptists&quot; ## [25] &quot;Southern baptist&quot; &quot;Nat bapt conv usa&quot; &quot;Nat bapt conv of am&quot; ## [28] &quot;Am bapt ch in usa&quot; &quot;Am baptist asso&quot; &quot;Not applicable&quot; rincome is principaled, rest are arbitrary Why did moving “Not applicable” to the front of the levels move it to the bottom of the plot? Becuase is moving this factor to be first in order 15.5: Modifying factor levels Example with fct_recode gss_cat %&gt;% mutate(partyid = fct_recode(partyid, &quot;Republican, strong&quot; = &quot;Strong republican&quot;, &quot;Republican, weak&quot; = &quot;Not str republican&quot;, &quot;Independent, near rep&quot; = &quot;Ind,near rep&quot;, &quot;Independent, near dem&quot; = &quot;Ind,near dem&quot;, &quot;Democrat, weak&quot; = &quot;Not str democrat&quot;, &quot;Democrat, strong&quot; = &quot;Strong democrat&quot; )) %&gt;% count(partyid) ## # A tibble: 10 x 2 ## partyid n ## &lt;fct&gt; &lt;int&gt; ## 1 No answer 154 ## 2 Don&#39;t know 1 ## 3 Other party 393 ## 4 Republican, strong 2314 ## 5 Republican, weak 3032 ## 6 Independent, near rep 1791 ## 7 Independent 4119 ## 8 Independent, near dem 2499 ## 9 Democrat, weak 3690 ## 10 Democrat, strong 3490 15.5.1 How have the proportions of people identifying as Democrat, Republican, and Independent changed over time? As a line plot: gss_cat %&gt;% mutate(partyid = fct_collapse( partyid, other = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Other party&quot;), rep = c(&quot;Strong republican&quot;, &quot;Not str republican&quot;), ind = c(&quot;Ind,near rep&quot;, &quot;Independent&quot;, &quot;Ind,near dem&quot;), dem = c(&quot;Not str democrat&quot;, &quot;Strong democrat&quot;) )) %&gt;% count(year, partyid) %&gt;% group_by(year) %&gt;% mutate(prop = n / sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes( x = year, y = prop, colour = fct_reorder2(partyid, year, prop) )) + geom_line() + labs(colour = &quot;partyid&quot;) As a bar plot: gss_cat %&gt;% mutate(partyid = fct_collapse( partyid, other = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Other party&quot;), rep = c(&quot;Strong republican&quot;, &quot;Not str republican&quot;), ind = c(&quot;Ind,near rep&quot;, &quot;Independent&quot;, &quot;Ind,near dem&quot;), dem = c(&quot;Not str democrat&quot;, &quot;Strong democrat&quot;) )) %&gt;% count(year, partyid) %&gt;% group_by(year) %&gt;% mutate(prop = n / sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes( x = year, y = prop, fill = fct_reorder2(partyid, year, prop) )) + geom_col() + labs(colour = &quot;partyid&quot;) Suggests proportion of republicans has gone down with independents and other going up. How could you collapse rincome into a small set of categories? other = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Refused&quot;, &quot;Not applicable&quot;) high = c(&quot;$25000 or more&quot;, &quot;$20000 - 24999&quot;, &quot;$15000 - 19999&quot;, &quot;$10000 - 14999&quot;) med = c(&quot;$8000 to 9999&quot;, &quot;$7000 to 7999&quot;, &quot;$6000 to 6999&quot;, &quot;$5000 to 5999&quot;) low = c(&quot;$4000 to 4999&quot;, &quot;$3000 to 3999&quot;, &quot;$1000 to 2999&quot;, &quot;Lt $1000&quot;) mutate(gss_cat, rincome = fct_collapse( rincome, other = other, high = high, med = med, low = low )) %&gt;% count(rincome) ## # A tibble: 4 x 2 ## rincome n ## &lt;fct&gt; &lt;int&gt; ## 1 other 8468 ## 2 high 10862 ## 3 med 970 ## 4 low 1183 Appendix Viewing all levels A few ways to get an initial look at the levels or counts across a dataset gss_cat %&gt;% purrr::map(unique) gss_cat %&gt;% purrr::map(table) gss_cat %&gt;% purrr::map(table) %&gt;% purrr::map(plot) gss_cat %&gt;% mutate_if(is.factor, ~fct_lump(., 14)) %&gt;% sample_n(1000) %&gt;% GGally::ggpairs() Percentage NA each level: gss_cat %&gt;% purrr::map(~(sum(is.na(.x)) / length(.x))) %&gt;% as_tibble() # essentially equivalent... gss_cat %&gt;% summarise_all(~(sum(is.na(.)) / length(.))) Print all levels of tibble: gss_cat %&gt;% count(age) %&gt;% print(n = Inf) "],
["16-dates-and-times.html", "Ch. 16: Dates and times 16.2: Creating date/times 16.3: Date-time components 16.4: Time spans Appendix", " Ch. 16: Dates and times Key questions: 16.2.4. #3 16.3.4. #1, 4, 5 16.4.5. #4 Functions and notes: today get current date now get current date-time ymd_hms one example of straight-forward set-of of functions that take either strings or unquoted numbers and output dates or date-times make_datetime create date-time from individual components, e.g. make_datetime(year, month, day, hour, minute) as_date_time and as_date let you switch between date-time and dates, e.g. as_datetime(today()) or as_date(now()) Accessor functions let you pull out components from an existing date-time: year, month, mday, yday, wday, hour, minute, second month and wday have label = TRUE to pull the abbreviated name rather than the number, and pull full name with abbr = FALSE You can also use these to set particular components year(datetime) &lt;- 2020 update allows you to specify multiple values at one time, e.g. update(datetime, year = 2020, month = 2, mday = 2, hour = 2) When values are too big they roll-over e.g. update(ymd(\"2015-02-01\"), mday = 30) will become ‘2015-03-02’ Rounding functions to nearest unit of time floor_date, round_date, ceiling_date as.duration convert diff-time to a duration Durations (can add and multiply): dseconds, dhours, ddays, dweeks, dyears Periods (can add and multiply), more likely to do what you expect than duration: seconds, minutes, hours, days, weeks, months Interval is a duration with a starting point, making it precise and possible to determine EXACT length e.g. (today() %--% next_year) / ddays(1) to find exact duration Sys.timezone to see what R thinks your current time zone is tz = arg in ymd_hms let’s you change printing behavior (not underlying value, as assumes UTC unless changed) with_tz allows you to print an existing date-time object to a specific other timezone force_tz when have an object that’s been labeled with wrong time-zone and need to fix it 16.2: Creating date/times Note that 1 in date-times is treated as 1 - second in numeric contexts, so example below sets binwidth = 86400 to specify 1 day make_datetime_100 &lt;- function(year, month, day, time) { make_datetime(year, month, day, time %/% 100, time %% 100) } flights_dt &lt;- flights %&gt;% filter(!is.na(dep_time), !is.na(arr_time)) %&gt;% mutate_at(c(&quot;dep_time&quot;, &quot;arr_time&quot;, &quot;sched_dep_time&quot;, &quot;sched_arr_time&quot;), ~make_datetime_100(year, month, day, .)) %&gt;% select(origin, dest, ends_with(&quot;delay&quot;), ends_with(&quot;time&quot;)) flights_dt %&gt;% ggplot(aes(dep_time)) + geom_freqpoly(binwidth = 86400) 16.2.4 What happens if you parse a string that contains invalid dates? ymd(c(&quot;2010-10-10&quot;, &quot;bananas&quot;)) ## Warning: 1 failed to parse. ## [1] &quot;2010-10-10&quot; NA Outputs an NA and sends warning of number that failed to parse What does the tzone argument to today() do? Why is it important? Let’s you specify timezones, may be different days depending on location today(tzone = &quot;MST&quot;) ## [1] &quot;2019-06-05&quot; now(tzone = &quot;MST&quot;) ## [1] &quot;2019-06-05 16:27:06 MST&quot; Use the appropriate lubridate function to parse each of the following dates: d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 mdy(d1) ## [1] &quot;2010-01-01&quot; ymd(d2) ## [1] &quot;2015-03-07&quot; dmy(d3) ## [1] &quot;2017-06-06&quot; mdy(d4) ## [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; mdy(d5) ## [1] &quot;2014-12-30&quot; 16.3: Date-time components This allows you to plot the number of flights per week flights_dt %&gt;% count(week = floor_date(dep_time, &quot;week&quot;)) %&gt;% ggplot(aes(week, n)) + geom_line() 16.3.4 How does the distribution of flight times within a day change over the course of the year? Median flight time by day flights_dt %&gt;% transmute(quarter_dep = quarter(dep_time) %&gt;% factor(), day_dep = as_date(dep_time), dep_time = as.hms(dep_time)) %&gt;% group_by(quarter_dep, day_dep) %&gt;% summarise(day_median = median(dep_time)) %&gt;% ungroup() %&gt;% ggplot(aes(x = day_dep, y = day_median)) + geom_line(aes(colour = quarter_dep, group = 1)) + labs(title = &quot;Median flight times by day, coloured by quarter&quot;, subtitle = &quot;Typical flight times change with daylight savings times&quot;)+ geom_vline(xintercept = ymd(&quot;20130310&quot;), linetype = 2)+ geom_vline(xintercept = ymd(&quot;20131103&quot;), linetype = 2) First couple and last couple months tend to have slightly earlier start times Quantiles of flight times by month flights_dt %&gt;% transmute(month_dep = month(dep_time, label = TRUE), quarter_dep = quarter(dep_time) %&gt;% factor(), wk_dep = week(dep_time), dep_time = as.hms(dep_time)) %&gt;% group_by(month_dep, wk_dep) %&gt;% ungroup() %&gt;% ggplot(aes(x = month_dep, y = dep_time, group = month_dep)) + geom_boxplot() Reinforces prior plot, shows that first couple and last couple months of year tend to have slightly higher proportion of flights earlier in day Last week of the year have a lower proportion of late flights, and a higher proportion of morning flights See 16.3.4.1 for a few other plots I looked at. Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings. flights_dt %&gt;% mutate(dep_delay_check = (dep_time - sched_dep_time) / dminutes(1), same = dep_delay == dep_delay_check, difference = dep_delay_check - dep_delay) %&gt;% filter(abs(difference) &gt; 0) ## # A tibble: 1,205 x 12 ## origin dest dep_delay arr_delay dep_time sched_dep_time ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 JFK BWI 853 851 2013-01-01 08:48:00 2013-01-01 18:35:00 ## 2 JFK SJU 43 36 2013-01-02 00:42:00 2013-01-02 23:59:00 ## 3 JFK SYR 156 154 2013-01-02 01:26:00 2013-01-02 22:50:00 ## 4 JFK SJU 33 22 2013-01-03 00:32:00 2013-01-03 23:59:00 ## 5 JFK BUF 185 172 2013-01-03 00:50:00 2013-01-03 21:45:00 ## 6 JFK BQN 156 143 2013-01-03 02:35:00 2013-01-03 23:59:00 ## 7 JFK SJU 26 23 2013-01-04 00:25:00 2013-01-04 23:59:00 ## 8 JFK PWM 141 125 2013-01-04 01:06:00 2013-01-04 22:45:00 ## 9 JFK PSE 15 18 2013-01-05 00:14:00 2013-01-05 23:59:00 ## 10 JFK FLL 127 130 2013-01-05 00:37:00 2013-01-05 22:30:00 ## # ... with 1,195 more rows, and 6 more variables: arr_time &lt;dttm&gt;, ## # sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;, dep_delay_check &lt;dbl&gt;, ## # same &lt;lgl&gt;, difference &lt;dbl&gt; They are except in the case when it goes over a day, the day is not pushed forward so it counts it as being 24 hours off Compare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.) flights_dt %&gt;% mutate(air_time_check = (arr_time - dep_time) / dminutes(1)) %&gt;% select(air_time_check, air_time, dep_time, arr_time, everything()) ## # A tibble: 328,063 x 10 ## air_time_check air_time dep_time arr_time origin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 193 227 2013-01-01 05:17:00 2013-01-01 08:30:00 EWR ## 2 197 227 2013-01-01 05:33:00 2013-01-01 08:50:00 LGA ## 3 221 160 2013-01-01 05:42:00 2013-01-01 09:23:00 JFK ## 4 260 183 2013-01-01 05:44:00 2013-01-01 10:04:00 JFK ## 5 138 116 2013-01-01 05:54:00 2013-01-01 08:12:00 LGA ## 6 106 150 2013-01-01 05:54:00 2013-01-01 07:40:00 EWR ## 7 198 158 2013-01-01 05:55:00 2013-01-01 09:13:00 EWR ## 8 72 53 2013-01-01 05:57:00 2013-01-01 07:09:00 LGA ## 9 161 140 2013-01-01 05:57:00 2013-01-01 08:38:00 JFK ## 10 115 138 2013-01-01 05:58:00 2013-01-01 07:53:00 LGA ## # ... with 328,053 more rows, and 5 more variables: dest &lt;chr&gt;, ## # dep_delay &lt;dbl&gt;, arr_delay &lt;dbl&gt;, sched_dep_time &lt;dttm&gt;, ## # sched_arr_time &lt;dttm&gt; Initial check is off, so need to take into account the time-zone and difference from NYC, so join timezone document flights_dt %&gt;% left_join(select(nycflights13::airports, dest = faa, tz), by = &quot;dest&quot;) %&gt;% mutate(arr_time_new = arr_time - dhours(tz + 5)) %&gt;% mutate(air_time_tz = (arr_time_new - dep_time) / dminutes(1), diff_Airtime = air_time_tz - air_time) %&gt;% select( origin, dest, tz, contains(&quot;time&quot;), -(contains(&quot;sched&quot;))) ## # A tibble: 328,063 x 9 ## origin dest tz dep_time arr_time air_time ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 EWR IAH -6 2013-01-01 05:17:00 2013-01-01 08:30:00 227 ## 2 LGA IAH -6 2013-01-01 05:33:00 2013-01-01 08:50:00 227 ## 3 JFK MIA -5 2013-01-01 05:42:00 2013-01-01 09:23:00 160 ## 4 JFK BQN NA 2013-01-01 05:44:00 2013-01-01 10:04:00 183 ## 5 LGA ATL -5 2013-01-01 05:54:00 2013-01-01 08:12:00 116 ## 6 EWR ORD -6 2013-01-01 05:54:00 2013-01-01 07:40:00 150 ## 7 EWR FLL -5 2013-01-01 05:55:00 2013-01-01 09:13:00 158 ## 8 LGA IAD -5 2013-01-01 05:57:00 2013-01-01 07:09:00 53 ## 9 JFK MCO -5 2013-01-01 05:57:00 2013-01-01 08:38:00 140 ## 10 LGA ORD -6 2013-01-01 05:58:00 2013-01-01 07:53:00 138 ## # ... with 328,053 more rows, and 3 more variables: arr_time_new &lt;dttm&gt;, ## # air_time_tz &lt;dbl&gt;, diff_Airtime &lt;dbl&gt; Is closer but still off. In chapter 5, problem 5.5.2.1 I go further into this In Appendix section 16.3.4.3 filter to NAs How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why? flights_dt %&gt;% mutate(sched_dep_time = as.hms(floor_date(sched_dep_time, &quot;30 mins&quot;))) %&gt;% group_by(sched_dep_time) %&gt;% summarise(delay_mean = mean(arr_delay, na.rm = TRUE), n = n(), n_na = sum(is.na(arr_delay)) / n, delay_median = median(arr_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = sched_dep_time, y = delay_mean, size = n)) + geom_point() It goes-up throughout the day Use sched_dep_time because it has the correct day On what day of the week should you leave if you want to minimise the chance of a delay? flights_dt %&gt;% mutate(weekday = wday(sched_dep_time, label = TRUE)) %&gt;% group_by(weekday) %&gt;% summarise(prop_delay = sum(dep_delay &gt; 0) / n()) ## # A tibble: 7 x 2 ## weekday prop_delay ## &lt;ord&gt; &lt;dbl&gt; ## 1 Sun 0.383 ## 2 Mon 0.401 ## 3 Tue 0.364 ## 4 Wed 0.372 ## 5 Thu 0.431 ## 6 Fri 0.425 ## 7 Sat 0.348 wknd has a slightly lower proportion of flights delayed (Thursday has the worst) What makes the distribution of diamonds$carat and flights$sched_dep_time similar? ggplot(diamonds, aes(x = carat)) + geom_histogram(bins = 500)+ labs(title = &quot;Distribution of carat in diamonds dataset&quot;) ggplot(flights, aes(x = as.hms(sched_dep_time))) + geom_histogram(bins = 24*6)+ labs(title = &quot;Distribution of scheduled departure times in flights dataset&quot;) Both have gaps and peaks at ‘attractive’ values Confirm my hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed. mutate(flights_dt, mins_dep = minute(dep_time), mins_sched = minute(sched_dep_time), delayed = dep_delay &gt; 0) %&gt;% group_by(mins_dep) %&gt;% summarise(prop_delayed = sum(delayed) / n()) %&gt;% ggplot(aes(x = mins_dep, y = prop_delayed)) + geom_line() Consistent with above hypothesis 16.4: Time spans durations, which represent an exact number of seconds. periods, which represent human units like weeks and months. intervals, which represent a starting and ending point. Permitted arithmetic operations between different data types Periods example, using durations to fix oddity of problem when flight arrives overnight flights_dt &lt;- flights_dt %&gt;% mutate( overnight = arr_time &lt; dep_time, arr_time = arr_time + days(overnight * 1), sched_arr_time = sched_arr_time + days(overnight * 1) ) Intervals example to get precise number of days dependent on specific time next_year &lt;- today() + years(1) (today() %--% next_year) / ddays(1) ## [1] 366 To find out how many periods fall in an interval, need to use integer division (today() %--% next_year) %/% days(1) ## Note: method with signature &#39;Timespan#Timespan&#39; chosen for function &#39;%/%&#39;, ## target signature &#39;Interval#Period&#39;. ## &quot;Interval#ANY&quot;, &quot;ANY#Period&quot; would also be valid ## [1] 366 16.4.5 Why is there months() but no dmonths()? the duration varies from month to month Explain days(overnight * 1) to someone who has just started learning R. How does it work? this used in the example above makes it such that if overnight is TRUE, it will return the same time period but one day ahead, if false, does not change (as is adding 0 days) Create a vector of dates giving the first day of every month in 2015. x &lt;- ymd(&quot;2015-01-01&quot;) mons &lt;- c(0:11) (x + months(mons)) %&gt;% wday(label = TRUE) ## [1] Thu Sun Sun Wed Fri Mon Wed Sat Tue Thu Sun Tue ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat Create a vector of dates giving the first day of every month in the current year. x &lt;- today() %&gt;% update(month = 1, mday = 1) mons &lt;- c(0:11) (x + months(mons)) %&gt;% wday(label=TRUE) ## [1] Tue Fri Fri Mon Wed Sat Mon Thu Sun Tue Fri Sun ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat Write a function that given your birthday (as a date), returns how old you are in years. birthday_age &lt;- function(birthday) { (ymd(birthday) %--% today()) %/% years(1) } birthday_age(&quot;1989-09-07&quot;) ## [1] 29 Why can’t (today() %--% (today() + years(1)) / months(1) work? Can’t add and subtract intervals Appendix 16.3.4.1 Weekly flight proportions by 4 hour blocks flights_dt %&gt;% transmute(month_dep = month(dep_time, label = TRUE), wk_dep = week(dep_time), dep_time_4hrs = floor_date(dep_time, &quot;4 hours&quot;), hour_dep_4hrs = hour(dep_time_4hrs) %&gt;% factor) %&gt;% count(wk_dep, hour_dep_4hrs) %&gt;% group_by(wk_dep) %&gt;% mutate(wk_tot = sum(n), wk_prop = round(n / wk_tot, 3)) %&gt;% ungroup() %&gt;% ggplot(aes(x = wk_dep, y = wk_prop)) + geom_col(aes(fill = hour_dep_4hrs)) Weekly median fight time flights_dt %&gt;% transmute(quarter_dep = quarter(dep_time) %&gt;% factor(), day_dep = as_date(dep_time), wk_dep = floor_date(dep_time, &quot;1 week&quot;) %&gt;% as_date, dep_time = as.hms(dep_time)) %&gt;% group_by(quarter_dep, wk_dep) %&gt;% summarise(wk_median = median(dep_time)) %&gt;% ungroup() %&gt;% mutate(wk_median = as.hms(wk_median)) %&gt;% ggplot(aes(x = wk_dep, y = wk_median)) + geom_line(aes(colour = quarter_dep, group = 1)) Proportion of flights in each hour, by quarter flights_dt %&gt;% transmute(quarter_dep = quarter(dep_time) %&gt;% factor(), hour_dep = hour(dep_time)) %&gt;% count(quarter_dep, hour_dep) %&gt;% group_by(quarter_dep) %&gt;% mutate(quarter_tot = sum(n), quarter_prop = round(n / quarter_tot, 3)) %&gt;% ungroup() %&gt;% ggplot(aes(x = hour_dep, y = quarter_prop)) + geom_line(aes(colour = quarter_dep)) Q1 seems to be a little more extreme at the local maximas Look at proportion of flights by hour faceted by each month flights_dt %&gt;% transmute(month_dep = month(dep_time, label = TRUE), hour_dep = hour(dep_time)) %&gt;% count(month_dep, hour_dep) %&gt;% group_by(month_dep) %&gt;% mutate(month_tot = sum(n), month_prop = round(n / month_tot, 3)) %&gt;% ungroup() %&gt;% ggplot(aes(x = hour_dep, y = month_prop)) + geom_line() + facet_wrap( ~ month_dep) 16.3.4.3 Perhaps these are flights where landed in different location… flights_dt %&gt;% mutate(arr_delay_test = (arr_time - sched_arr_time) / dminutes(1)) %&gt;% select( origin, dest, dep_delay, arr_delay, arr_delay_test, contains(&quot;time&quot;)) %&gt;% filter(is.na(arr_delay)) ## # A tibble: 717 x 10 ## origin dest dep_delay arr_delay arr_delay_test dep_time ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 LGA XNA -5 NA 89 2013-01-01 15:25:00 ## 2 EWR STL 29 NA 195 2013-01-01 15:28:00 ## 3 LGA XNA -5 NA 98 2013-01-01 17:40:00 ## 4 EWR SAN 29 NA 108 2013-01-01 18:07:00 ## 5 JFK DFW 59 NA -1282 2013-01-01 19:39:00 ## 6 EWR TUL 22 NA 111 2013-01-01 19:52:00 ## 7 EWR XNA 43 NA 148 2013-01-02 09:05:00 ## 8 LGA GRR 120 NA 179 2013-01-02 11:25:00 ## 9 JFK DFW 8 NA 102 2013-01-02 18:48:00 ## 10 EWR MCI 85 NA 177 2013-01-02 18:49:00 ## # ... with 707 more rows, and 4 more variables: sched_dep_time &lt;dttm&gt;, ## # arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt; 16.3.4.4 Below started looking at proportions… mutate(flights_dt, dep_old = dep_time, sched_old = sched_dep_time, dep_time = floor_date(dep_time, &quot;5 minutes&quot;), sched_dep_time = floor_date(sched_dep_time, &quot;5 minutes&quot;), mins_dep = minute(dep_time), mins_sched = minute(sched_dep_time), delayed = dep_delay &gt; 0) %&gt;% group_by(mins_dep, mins_sched) %&gt;% summarise(num_delayed = sum(delayed), num = n(), prop_delayed = num_delayed / num) %&gt;% group_by(mins_dep) %&gt;% mutate(num_tot = sum(num), prop_sched = num / num_tot, sched_dep_diff = mins_dep - mins_sched) %&gt;% ungroup() %&gt;% ggplot(aes(x = mins_dep, y = prop_sched, fill = factor(mins_sched))) + geom_col()+ labs(title = &quot;Proportion of early flights by minute scheduled v. minute departed&quot;) mutate(flights_dt, dep_old = dep_time, sched_old = sched_dep_time, # dep_time = floor_date(dep_time, &quot;5 minutes&quot;), # sched_dep_time = floor_date(sched_dep_time, &quot;5 minutes&quot;), mins_dep = minute(dep_time), mins_sched = minute(sched_dep_time), early_less10 = dep_delay &gt;= -10) %&gt;% filter(dep_delay &lt; 0) %&gt;% group_by(mins_dep) %&gt;% summarise(num = n(), sum_recent10 = sum(early_less10), prop_recent10 = sum_recent10 / num) %&gt;% ungroup() %&gt;% ggplot(aes(x = mins_dep, y = prop_recent10)) + geom_line()+ labs(title = &quot;proportion of early flights that were scheduled to leave within 10 mins of when they did&quot;) "],
["18-pipes.html", "Ch. 18: Pipes (notes only)", " Ch. 18: Pipes (notes only) pryr::object_size gives the memory occupied by all of its arguments (note that built-in object.size does not allow measuring multiple objects so can’t see shared space). This function is actually shown in chapter 18: Pipes Some functions do not work naturally with the pipe. If you want to use assign with the pipe, you must be explicit about the environment env &lt;- environment() assign(&quot;x&quot;, 100, envir = env) try, tryCatch, suppressMessages, and suppressWarnings from base R all also do not work well Other pipes = ‘T pipe’, %T&gt;% that returns left-hand side rather than right. Will let the plot output, but then continues. Notice that this doesn’t work quite the same way for ggplot as ggplot does output something rnorm(100) %&gt;% matrix(ncol = 2) %T&gt;% plot() %&gt;% str() ## num [1:50, 1:2] -0.25 -0.924 0.894 0.298 0.347 ... iris %&gt;% select(Sepal.Length, Sepal.Width) %T&gt;% plot() %&gt;% select(Sepal.Length) %&gt;% head(10) ## Sepal.Length ## 1 5.1 ## 2 4.9 ## 3 4.7 ## 4 4.6 ## 5 5.0 ## 6 5.4 ## 7 4.6 ## 8 5.0 ## 9 4.4 ## 10 4.9 %$% allows you to blow out the names of the arguments, I personally prefer using the with() function for this instead as I find it to be a little more readable… The two examples below are equivalent mtcars %$% cor(disp, mpg) ## [1] -0.8475514 mtcars %&gt;% with(cor(disp, mpg)) ## [1] -0.8475514 "],
["19-functions.html", "Ch. 19: Functions 19.2: When should you write a function? 19.3: Functions are for humans and computers 19.4: Conditional execution 19.5: Function arguments 19.6: Return values Appendix", " Ch. 19: Functions Key questions: 19.3.1. #1 19.4.4. #2, 3 19.5.5. #2 (actually make a function that fixes this) Functions and notes: function_name &lt;- function(input1, input2) {} if () {} else if () {} else {} || (or) , &amp;&amp; (and) – used to combine multiple logical expressions | and &amp; are vectorized operations not to be used with if statements any checks if any values in vector are TRUE all checks if all values in vector are TRUE identical strict check for if values are the same dplyr::near for more lenient comparison and typically better than identical as unaffected by floating point numbers and fine with different types switch evaluate selected code based on position or name (good for replacing long chain of if statements). e.g. Operation_Times2 &lt;- function(x, y, op){ first_op &lt;- switch(op, plus = x + y, minus = x - y, times = x * y, divide = x / y, stop(&quot;Unknown op!&quot;) ) first_op * 2 } Operation_Times2(5, 7, &quot;plus&quot;) ## [1] 24 stop stops expression and executes error action, note that .call default is FALSE warning generates a warning that corresponds with arguments, suppressWarnings may also be useful to no stopifnot test multiple args and will produce error message if any are not true – compromise that prevents you from tedious work of putting multiple if(){} else stop() statements ... useful catch-all if your function primarily wraps another function (note that misspelled arguments will not raise an error), e.g. commas &lt;- function(...) stringr::str_c(..., collapse = &quot;, &quot;) commas(letters[1:10]) ## [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; cut can be used to discretize continuous variables (also saves long if statements) return allows you to return the function early, typically reserve use for when the function can return early with a simpler function cat function to print label in output invisible input does not get printed out 19.2: When should you write a function? 19.2.1 Why is TRUE not a parameter to rescale01()? What would happen if x contained a single missing value, and na.rm was FALSE? TRUE doesn’t change between uses. The output would be NA In the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1. rescale01_inf &lt;- function(x){ rng &lt;- range(x, na.rm = TRUE, finite = TRUE) x_scaled &lt;- (x - rng[1]) / (rng[2] - rng[1]) is_inf &lt;- is.infinite(x) is_less0 &lt;- x &lt; 0 x_scaled[is_inf &amp; is_less0] &lt;- 0 x_scaled[is_inf &amp; (!is_less0)] &lt;- 1 x_scaled } x &lt;- c(Inf, -Inf, 0, 3, -5) rescale01_inf(x) ## [1] 1.000 0.000 0.625 1.000 0.000 Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative? mean(is.na(x)) x / sum(x, na.rm = TRUE) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) See solutions below: x &lt;- c(1, 4, 2, 0, NA, 3, NA) #mean(is.na(x)) perc_na &lt;- function(x) { is.na(x) %&gt;% mean() } perc_na(x) ## [1] 0.2857143 #x / sum(x, na.rm = TRUE) prop_weighted &lt;- function(x) { x / sum(x, na.rm = TRUE) } prop_weighted(x) ## [1] 0.1 0.4 0.2 0.0 NA 0.3 NA #sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) CoefficientOfVariation &lt;- function(x) { sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) } CoefficientOfVariation(x) ## [1] 0.7905694 Follow http://nicercode.github.io/intro/writing-functions.html to write your own functions to compute the variance and skew of a numeric vector. Re-do below to write measures for skew and variance (e.g. kurtosis, etc.) var_bry &lt;- function(x){ sum((x - mean(x)) ^ 2) / (length(x) - 1) } skewness_bry &lt;- function(x) { mean((x - mean(x)) ^ 3) / var_bry(x) ^ (3 / 2) } Let’s create some samples of distributions – normal, t (with 7 degrees of freedom), unifrom, poisson (with lambda of 2). Note that an example with a cauchy distribution and looking at difference in kurtosis between that and a normal distribution has been moved to the Appendix section 19.2.1.4. nd &lt;- rnorm(10000) td_df7 &lt;- rt(10000, df = 7) ud &lt;- runif(10000) pd_l2 &lt;- rpois(10000, 2) Verify that these functions match with established functions dplyr::near(skewness_bry(pd_l2), e1071::skewness(pd_l2, type = 3)) ## [1] TRUE dplyr::near(var_bry(pd_l2), var(pd_l2)) ## [1] TRUE Let’s look at the distributions as well as their variance an skewness distributions_df &lt;- tibble(normal_dist = nd, t_7df_dist = td_df7, uniform_dist = ud, poisson_dist = pd_l2) distributions_df %&gt;% gather(normal_dist:poisson_dist, value = &quot;sample&quot;, key = &quot;dist_type&quot;) %&gt;% mutate(dist_type = factor(forcats::fct_inorder(dist_type))) %&gt;% ggplot(aes(x = sample))+ geom_histogram()+ facet_wrap(~ dist_type, scales = &quot;free&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. tibble(dist_type = names(distributions_df), skewness = purrr::map_dbl(distributions_df, skewness_bry), variance = purrr::map_dbl(distributions_df, var_bry)) ## # A tibble: 4 x 3 ## dist_type skewness variance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 normal_dist 0.0561 0.988 ## 2 t_7df_dist -0.0263 1.40 ## 3 uniform_dist -0.00678 0.0833 ## 4 poisson_dist 0.680 1.98 excellent video explaining intuition behind skewness: https://www.youtube.com/watch?v=z3XaFUP1rAM Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors. both_na &lt;- function(x, y) { if (length(x) == length(y)) { sum(is.na(x) &amp; is.na(y)) } else stop(&quot;Vectors are not equal length&quot;) } x &lt;- c(4, NA, 7, NA, 3) y &lt;- c(NA, NA, 5, NA, 0) z &lt;- c(NA, 4) both_na(x, y) ## [1] 2 both_na(x, z) ## Error in both_na(x, z): Vectors are not equal length What do the following functions do? Why are they useful even though they are so short? is_directory &lt;- function(x) file.info(x)$isdir is_readable &lt;- function(x) file.access(x, 4) == 0 first checks if what is being referred to is actually a directory second checks if a specific file is readable Read the complete lyrics to “Little Bunny Foo Foo”. There’s a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication. 19.3: Functions are for humans and computers Recommends snake_case over camelCase, but just choose one and be consistent When functions have a link, common prefix over suffix (i.e. input_select, input_text over, select_input, text_input) ctrl + shift + r creates section breaks in R scripts like below # test label -------------------------------------------------------------- (though these cannot be made in markdown documents) 19.3.1 Read the source code for each of the following three functions, puzzle out what they do, and then brainstorm better names. f1 &lt;- function(string, prefix) { substr(string, 1, nchar(prefix)) == prefix } f2 &lt;- function(x) { if (length(x) &lt;= 1) return(NULL) x[-length(x)] } f3 &lt;- function(x, y) { rep(y, length.out = length(x)) } f1: check_prefix f2: return_not_last f3: repeat_for_length Take a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments. done seperately Compare and contrast rnorm() and MASS::mvrnorm(). How could you make them more consistent? uses mu = and Sigma = instead of mean = and sd = , and has extra parameters like tol, empirical, EISPACK Similar in that both are pulling samples from gaussian distribution mvrnorm is multivariate though, could change name to rnorm_mv Make a case for why norm_r(), norm_d() etc would be better than rnorm(), dnorm(). Make a case for the opposite. norm_* would show the commonality of them being from the same distribution. One could argue the important commonality though may be more related to it being either a random sample or a density distribution, in which case the r* or d* coming first may make more sense. To me, the fact that the help pages has all of the ‘normal distribution’ functions on the same page suggests the former may make more sense. However, I actually like having it be set-up the way it is, because I am more likely to forget the name of the distribution type I want over the fact that I want a random sample, so it’s easier to type r and then do ctrl + space and have autocomplete help me find the specific distribution I want, e.g. rnorm, runif, rpois, rbinom… 19.4: Conditional execution Function example that uses if statement: has_name &lt;- function(x) { nms &lt;- names(x) if (is.null(nms)) { rep(FALSE, length(x)) } else { !is.na(nms) &amp; nms != &quot;&quot; } } note that if all names are blank, it returns the one-unit vector value NULL, hence the need for the if statement here…33 19.4.4. What’s the difference between if and ifelse()? Carefully read the help and construct three examples that illustrate the key differences. ifelse is vectorized, if is not Typically use if in functions when giving conditional options for how to evaluate Typically use ifelse when changing specific values in a vector If you supply if with a vector of length &gt; 1, it will use the first value x &lt;- c(3, 4, 6) y &lt;- c(&quot;5&quot;, &quot;c&quot;, &quot;9&quot;) # Use `ifelse` simple transformations of values ifelse(x &lt; 5, 0, x) ## [1] 0 0 6 # Use `if` for single condition tests cutoff_make0 &lt;- function(x, cutoff = 0){ if(is.numeric(x)){ ifelse(x &lt; cutoff, 0, x) } else stop(&quot;The input provided is not a numeric vector&quot;) } cutoff_make0(x, cutoff = 4) ## [1] 0 4 6 cutoff_make0(y, cutoff = 4) ## Error in cutoff_make0(y, cutoff = 4): The input provided is not a numeric vector Write a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.) greeting &lt;- function(when) { time &lt;- hour(when) if (time &lt; 12 &amp;&amp; time &gt; 4) { greating &lt;- &quot;good morning&quot; } else if (time &lt; 17 &amp;&amp; time &gt;= 12) { greeting &lt;- &quot;good afternoon&quot; } else greeting &lt;- &quot;good evening&quot; when_char &lt;- as.character(when) mid &lt;- &quot;, it is: &quot; cat(greeting, mid, when_char, sep = &quot;&quot;) } greeting(now()) ## good evening, it is: 2019-06-05 19:28:02 Implement a fizzbuzz function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function. fizzbuzz &lt;- function(x){ if(is.numeric(x) &amp;&amp; length(x) == 1){ y &lt;- &quot;&quot; if (x %% 5 == 0) y &lt;- str_c(y, &quot;fizz&quot;) if (x %% 3 == 0) y &lt;- str_c(y, &quot;buzz&quot;) if (str_length(y) == 0) { print(x) } else print(y) } else stop(&quot;Input is not a numeric vector with length 1&quot;) } fizzbuzz(4) ## [1] 4 fizzbuzz(10) ## [1] &quot;fizz&quot; fizzbuzz(6) ## [1] &quot;buzz&quot; fizzbuzz(30) ## [1] &quot;fizzbuzz&quot; fizzbuzz(c(34, 21)) ## Error in fizzbuzz(c(34, 21)): Input is not a numeric vector with length 1 How could you use cut() to simplify this set of nested if-else statements? if (temp &lt;= 0) { &quot;freezing&quot; } else if (temp &lt;= 10) { &quot;cold&quot; } else if (temp &lt;= 20) { &quot;cool&quot; } else if (temp &lt;= 30) { &quot;warm&quot; } else { &quot;hot&quot; } Below is example of fix temp &lt;- seq(-10, 50, 5) cut(temp, breaks = c(-Inf, 0, 10, 20, 30, Inf), #need to include negative and positive infiniity labels = c(&quot;freezing&quot;, &quot;cold&quot;, &quot;cool&quot;, &quot;warm&quot;, &quot;hot&quot;), right = TRUE, oredered_result = TRUE) ## [1] freezing freezing freezing cold cold cool cool ## [8] warm warm hot hot hot hot ## Levels: freezing cold cool warm hot How would you change the call to cut() if I’d used &lt; instead of &lt;=? What is the other chief advantage of cut() for this problem? (Hint: what happens if you have many values in temp?) See below change to right argument cut(temp, breaks = c(-Inf, 0, 10, 20, 30, Inf), #need to include negative and positive infiniity labels = c(&quot;freezing&quot;, &quot;cold&quot;, &quot;cool&quot;, &quot;warm&quot;, &quot;hot&quot;), right = FALSE, oredered_result = TRUE) ## [1] freezing freezing cold cold cool cool warm ## [8] warm hot hot hot hot hot ## Levels: freezing cold cool warm hot What happens if you use switch() with numeric values? It will return the index of the argument. In example below, I input ‘3’ into switch value so it does the times argument math_operation &lt;- function(x, y, op){ switch(op, plus = x + y, minus = x - y, times = x * y, divide = x / y, stop(&quot;Unknown op!&quot;) ) } math_operation(5, 4, 3) ## [1] 20 What does this switch() call do? What happens if x is “e”? x &lt;- &quot;e&quot; switch(x, a = , b = &quot;ab&quot;, c = , d = &quot;cd&quot; ) Experiment, then carefully read the documentation. If x is ‘e’ nothing will be outputted. If x is ‘c’ or ‘d’ then ‘cd’ is outputted. If ‘a’ or ‘b’ then ‘ab’ is outputeed. If blank it will continue down list until reaching an argument to output. 19.5: Function arguments Common non-descriptive short argument names: x, y, z: vectors. w: a vector of weights. df: a data frame. i, j: numeric indices (typically rows and columns). n: length, or number of rows. p: number of columns. 19.5.5. What does commas(letters, collapse = \"-\") do? Why? commas function is below commas &lt;- function(...) stringr::str_c(..., collapse = &quot;, &quot;) commas(letters[1:10]) ## [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; commas(letters[1:10], collapse = &quot;-&quot;) ## Error in stringr::str_c(..., collapse = &quot;, &quot;): formal argument &quot;collapse&quot; matched by multiple actual arguments The above fails because are essentially specifying two different values for the collapse argument Takes in vector of mulitple strings and outputs one-unit character string with items concatenated together and seperated by columns Is able to do this via use of ... that turns this into a wrapper on stringr::str_c with the collapse value specified It’d be nice if you could supply multiple characters to the pad argument, e.g. rule(\"Title\", pad = \"-+\"). Why doesn’t this currently work? How could you fix it? current rule function is below rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 cat(title, &quot; &quot;, stringr::str_dup(pad, width), &quot;\\n&quot;, sep = &quot;&quot;) } # Note that `cat` is used instead of `paste` because paste would output it as a character vector, whereas `cat` is focused on just ouptut, could also have used `print`, though print does more conversion than cat does (apparently) rule(&quot;Tis the season&quot;,&quot; to be jolly&quot;) ## Tis the season to be jolly -------------------------------------------- doesn’t work because pad ends-up being too many characters in this situation rule(&quot;Tis the season&quot;,&quot; to be jolly&quot;, pad=&quot;+-&quot;) ## Tis the season to be jolly +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+- instead would need to make the number of times pad is duplicated dependent on its length, see below for fix rule_pad_fix &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 width_fix &lt;- width %/% stringr::str_length(pad) cat(title, &quot; &quot;, stringr::str_dup(pad, width_fix), &quot;\\n&quot;, sep = &quot;&quot;) } rule_pad_fix(&quot;Tis the season&quot;,&quot; to be jolly&quot;, pad=&quot;+-&quot;) ## Tis the season to be jolly +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+- What does the trim argument to mean() do? When might you use it? trim specifies proportion of data to take off from both ends, good with outliers mean(c(-1000, 1:100, 100000), trim = .025) ## [1] 50.5 The default value for the method argument to cor() is c(\"pearson\", \"kendall\", \"spearman\"). What does that mean? What value is used by default? is showing that you can choose from any of these, will default to use pearson (value in first position) 19.6: Return values show_missings &lt;- function(df) { n &lt;- sum(is.na(df)) cat(&quot;Missing values: &quot;, n, &quot;\\n&quot;, sep = &quot;&quot;) invisible(df) } x &lt;- show_missings(mtcars) ## Missing values: 0 str(x) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... can still use in pipes mtcars %&gt;% show_missings() %&gt;% mutate(mpg = ifelse(mpg &lt; 20, NA, mpg)) %&gt;% show_missings() ## Missing values: 0 ## Missing values: 18 Appendix 19.2.1.4 Function for Standard Error: x &lt;- c(5, -2, 8, 6, 9) sd(x, na.rm = TRUE) / sqrt(sum(!is.na(x))) ## [1] 1.933908 sample_se &lt;- function(x) { sd(x, na.rm = TRUE) / sqrt(sum(!is.na(x)) - 1) } #sqrt(var(x)/sum(!is.na(x))) sample_se(x) ## [1] 2.162175 Function for kurtosis: kurtosis_type3 &lt;- function(x){ sum((x - mean(x)) ^ 4) / length(x) / sd(x) ^ 4 - 3 } Notice differences between cauchy and normal distribution set.seed(1235) norm_exp &lt;- rnorm(10000) set.seed(1235) cauchy_exp &lt;- rcauchy(10000) hist(norm_exp) hist(cauchy_exp) kurtosis kurtosis_type3(norm_exp) ## [1] 0.06382172 kurtosis_type3(cauchy_exp) ## [1] 1197.052 19.2.3.5 position_both_na &lt;- function(x, y) { if (length(x) == length(y)) { (c(1:length(x)))[(is.na(x) &amp; is.na(y))] } else stop(&quot;Vectors are not equal length&quot;) } x &lt;- c(4, NA, 7, NA, 3) y &lt;- c(NA, NA, 5, NA, 0) z &lt;- c(NA, 4) both_na(x, y) ## [1] 2 both_na(x, z) ## Error in both_na(x, z): Vectors are not equal length specifies position where both are NA second example shows returning of ‘stop’ argument NULL types are not vectors but only single length elements of a different type. is.null is not vectorized in the way that is.na. E.g. it’s job is to return TRUE if given a NULL. For example, a list of NULLs is a list type (not a NULL type) therefore the following: is.null(list(NULL, NULL)) would be FALSE – to return a list of TRUE values you would need to run map(list(NULL, NULL), is.null).↩ "],
["20-vectors.html", "Ch. 20: Vectors 20.3: Important types of atomic vector 20.4: Using atomic vectors 20.5: Recursive vectors (lists) 20.7: Augmented vectors Appendix", " Ch. 20: Vectors Key questions: 20.3.5, #1 20.4.6. #1, 4, 5 Functions and notes: Types of vectors, not including augmented types: Check special value types: is.finite, is.infinite, is.na, is.nan typeof retruns type of vector length returns length of vector pryr::object_size view size of object stored specific NA values can be defined explicitly with NA_integer_, NA_real_, NA_character_ (usually don’t need to know) explicitly differentiate integers from doubles with 10L v 10 explicit coersion functions: as.logical, as.integer, as.double, as.character, or use col_[types] when reading in so that coersion done at source test functions from purrr package that are more consistent than base R’s Purrr versions for testing types: set_names lets you set names after the fact, e.g. set_names(1:3, c(\"a\", \"b\", \"c\")) For more details on subsetting: http://adv-r.had.co.nz/Subsetting.html#applications str checks structure (excellent for use with lists) attr and attributes get and set attributes main types of attributes: Names, Dimensions/dims, Class. Class is important for object oriented programming which is covered in more detail here: http://adv-r.had.co.nz/OO-essentials.html#s3 Used together to investigate details of code for functions useMethod in function syntax indicates it is a generic function methods lists all methods within a generic getS3method to show specific implementation of a method as.Date ## function (x, ...) ## UseMethod(&quot;as.Date&quot;) ## &lt;bytecode: 0x000000001d6f1aa0&gt; ## &lt;environment: namespace:base&gt; methods(&quot;as.Date&quot;) ## [1] as.Date.character as.Date.default as.Date.factor as.Date.numeric ## [5] as.Date.POSIXct as.Date.POSIXlt ## see &#39;?methods&#39; for accessing help and source code getS3method(&quot;as.Date&quot;, &quot;default&quot;) ## function (x, ...) ## { ## if (inherits(x, &quot;Date&quot;)) ## x ## else if (is.logical(x) &amp;&amp; all(is.na(x))) ## .Date(as.numeric(x)) ## else stop(gettextf(&quot;do not know how to convert &#39;%s&#39; to class %s&quot;, ## deparse(substitute(x)), dQuote(&quot;Date&quot;)), domain = NA) ## } ## &lt;bytecode: 0x0000000019e1f0e8&gt; ## &lt;environment: namespace:base&gt; Some tidyverse functions are not always easy to unpack with just the above34 Augmented vectors: vectors with additional attributes, e.g. factors (levels, class = factors), dates and datetimes (tzone, class = (POSIXct, POSIXt)), POSIXlt (names, class = (POSIXLt, POSIXt)), tibbles (names, class = (tbl_df, tbl, data.frame), row.names) – in the integer, double and double, list, list types. data frames only have class data.frame, whereas tibbles have tbl_df, and tbl as well class get or set class attribute unclass returns copy with ‘class’ attribute removed 20.3: Important types of atomic vector 20.3.5 Describe the difference between is.finite(x) and !is.infinite(x). is.finite and is.infinite return FALSE for NA or NaN values, therefore these values become TRUE when negated as in the latter case, e.g.: is.finite(c(6,11,-Inf, NA, NaN)) ## [1] TRUE TRUE FALSE FALSE FALSE !is.infinite(c(6,11,-Inf, NA, NaN)) ## [1] TRUE TRUE FALSE TRUE TRUE Read the source code for dplyr::near() (Hint: to see the source code, drop the ()). How does it work? safer way to test equality of floating point numbers (as has some tolerance for differences caused by rounding) it checks if the difference between the value is within tol which by default is .Machine$double.eps^0.5 A logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research. Part of the point here is that it’s not ‘infinite’ like someone may be tempted to answer – it’s constrained by memory of the machine For integer it is 2 * 2 * 10^9 For double it is 2 * 2 * 10^308 Brainstorm at least four functions that allow you to convert a double to an integer. How do they differ? Be precise. as.integer, as.factor (technically is going to a factor – but this class is built on top of integers), round, floor, ceiling, these last 3 though do not change the type, which would remain a type double What functions from the readr package allow you to turn a string into logical, integer, and double vector? The appropriate parse_* or col_* functions 20.4: Using atomic vectors 20.4.6 What does mean(is.na(x)) tell you about a vector x? What about sum(!is.finite(x))? percentage that are NA or NaN number that are either Inf, -Inf, NA or NaN Carefully read the documentation of is.vector(). What does it actually test for? Why does is.atomic() not agree with the definition of atomic vectors above? is.vector tests if it is a specific type of vector with no attributes other than names. This second requirement means that any augmented vectors such as factors, dates, tibbles all would return false. is.atomic returns TRUE to is.atomic(NULL) despite this representing the empty set. Compare and contrast setNames() with purrr::set_names(). both assign names after fact purrr::set_names is stricter and returns an error in situations like the following where as setNames does not: setNames(1:4, c(&quot;a&quot;)) ## a &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 1 2 3 4 set_names(1:4, c(&quot;a&quot;)) ## Error: `nm` must be `NULL` or a character vector the same length as `x` Create functions that take a vector as input and returns: x &lt;- c(-3:14, NA, Inf, NaN) The last value. Should you use [ or [[? return_last &lt;- function(x) x[[length(x)]] return_last(x) ## [1] NaN The elements at even numbered positions. return_even &lt;- function(x) x[((1:length(x)) %% 2 == 0)] return_even(x) ## [1] -2 0 2 4 6 8 10 12 14 Inf Every element except the last value. return_not_last &lt;- function(x) x[-length(x)] return_not_last(x) ## [1] -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 ## [18] 14 NA Inf Only even numbers (and no missing values). #only even and not na return_even_no_na &lt;- function(x) x[((1:length(x)) %% 2 == 0) &amp; !is.na(x)] return_even_no_na(x) ## [1] -2 0 2 4 6 8 10 12 14 Inf Why is x[-which(x &gt; 0)] not the same as x[x &lt;= 0]? x[-which(x &gt; 0)] #which only reports the indices of the matches, so specifies all to be removed ## [1] -3 -2 -1 0 NA NaN x[x &lt;= 0] #This method reports T/F&#39;sNaN is converted into NA ## [1] -3 -2 -1 0 NA NA in the 2nd instance, NaNs will get converted to NA What happens when you subset with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist? In both cases you get back an NA (though it seems to take longer in the case when subsetting by a name that doesn’t exist). 20.5: Recursive vectors (lists) Example of subsetting items from a list: a &lt;- list(a = 1:3, b = &quot;a string&quot;, c = pi, d = list(c(-1,-2), -5)) a[[4]][[1]] ## [1] -1 -2 # equivalent alternatives: # a$d[[1]] # a[4][[1]][[1]] 3 ways of subsetting [], [[]], $ 20.5.4. a &lt;- list(a = 1:3, b = &quot;a string&quot;, c = pi, d = list(-1, -5)) Draw the following lists as nested sets: list(a, b, list(c, d), list(e, f)) list(list(list(list(list(list(a)))))) I did not conform with Hadley’s square v rounded syntax, but hopefully this gives a sense of what the above are: drawings 1 and 2 for 20.5.4. What happens if you subset a tibble as if you’re subsetting a list? What are the key differences between a list and a tibble? Dataframe is just a list of vectors (columns) – with the restriction that each column has the same number of elements whereas lists do not have this requirement Dataframe structure better connects elements by row structure, making subsetting by the qualities of these values much easier 20.7: Augmented vectors 20.7.4 What does hms::hms(3600) return? x &lt;- hms::hms(3600) How does it print? print(x) ## 01:00:00 What primitive type is the augmented vector built on top of? typeof(x) ## [1] &quot;double&quot; What attributes does it use? attributes(x) ## $class ## [1] &quot;hms&quot; &quot;difftime&quot; ## ## $units ## [1] &quot;secs&quot; Try and make a tibble that has columns with different lengths. What happens? if the column is length one it will repeat for the length of the other column(s), otherwise if it is not the same length it will return an error tibble(x = 1:4, y = 5:6) #error ## Error: Tibble columns must have consistent lengths, only values of length one are recycled: ## * Length 2: Column `y` ## * Length 4: Column `x` tibble(x = 1:5, y = 6) #can have length 1 that repeats ## # A tibble: 5 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 6 ## 2 2 6 ## 3 3 6 ## 4 4 6 ## 5 5 6 Based on the definition above, is it ok to have a list as a column of a tibble? Yes, as long as the number of elements align with the other length of the other columns – this will come-up a lot in the modeling chapters. Appendix Subsetting nested lists x &lt;- list(&quot;a&quot;, list(list(&quot;c&quot;, &quot;d&quot;), &quot;e2&quot;), list(&quot;e&quot;, &quot;f&quot;)) x ## [[1]] ## [1] &quot;a&quot; ## ## [[2]] ## [[2]][[1]] ## [[2]][[1]][[1]] ## [1] &quot;c&quot; ## ## [[2]][[1]][[2]] ## [1] &quot;d&quot; ## ## ## [[2]][[2]] ## [1] &quot;e2&quot; ## ## ## [[3]] ## [[3]][[1]] ## [1] &quot;e&quot; ## ## [[3]][[2]] ## [1] &quot;f&quot; It can be confusing how to subset items in a nested list, lists output helps tell you the subsetting needed to extract particular items. For example, to output list(\"c\", \"d\") requires x[[2]][[1]], to output just d requires x[[2]][[1]][[2]] Subset nested lists: "],
["21-iteration.html", "Ch. 21: Iteration 21.2: For loops 21.3 For loop variations 21.4: For loops vs. functionals 21.5: The map functions 21.9 Other patterns of for loops Appendix", " Ch. 21: Iteration Key questions: 21.2.1. #1, 2 21.3.5. #1, 3 21.4.1. #2 21.5.3. #1 21.9.4. #2 Functions and notes: Common for loop template: output &lt;- vector(&quot;double&quot;, ncol(df)) # common for loop style for (i in seq_len(length(df))){ output[[i]] &lt;- fun(df[[i]]) } Common while loop template: i &lt;- 1 while (i &lt;= length(x)){ # body i &lt;- i + 1 } seq_along(df) does essentially same as seq_len(length(df)) unlist flatten list of vectors into single vector flaten_dbl is stricter alternative dplyr::bind_rows save output in a list of dfs and then append all at end rather than sequential rbinding sample(c(\"T\", \"H\"), 1) sapply is wrapper around lapply that automatically simplifies output – problematic in that never know what ouptut will be vapply is safe alternative to sapply e.g. for logical vapply(df, is.numeric, logical(1)), but map_lgl(df, is.numeric) is more simple map() makes a list. map_lgl() makes a logical vector. map_int() makes an integer vector. map_dbl() makes a double vector. map_chr() makes a character vector. shortcuts for applying functions in map: models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(function(df) lm(mpg ~ wt, data = df)) models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~lm(mpg ~ wt, data = .)) extracting by named elements from map: models %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) extracting by positions from map x &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9)) x %&gt;% map_dbl(2) map2 let’s you iterate through two components at once pmap allows you to iterate over p components – works well to hold inputs in a dataframe safely takes funciton returns two parts, result and error object similar to try but more consistent possibly similar to safely, but provide it a default value to return for errors quietly is similar to safely but captures all printed output messages and warnings purrr::transpose allows you to do things like get all 2nd elements in list, e.g. show later invoke_map let’s you iterate over both the functions and the parameters, have an f and a param input, e.g. f &lt;- c(&quot;runif&quot;, &quot;rnorm&quot;, &quot;rpois&quot;) param &lt;- list( list(min = -1, max = 1), list(sd = 5), list(lambda = 10) ) invoke_map(f, param, n = 5) %&gt;% str() walk is alternative to map that you call for side effects. Also have walk2 and pwalk that are generally more useful all invisibly return `.x (the first argument) so can used in the middle of pipelines keep and discard keep or discard elements in the input based off if TRUE to predicate some and every determine if the predicte is true for any or for all of our elements detect finds the first element where the predicate is true, detect_index returns its position head_while and tail_while take elements from the start or end of a vector while a predicate is true reduce is good for applying two table rule repeatedly, e.g. joins accumulate is similar but keeps all the interim results 21.2: For loops 21.2.1 Write for loops to (think about the output, sequence, and body before you start writing the loop): Compute the mean of every column in mtcars. output &lt;- vector(&quot;double&quot;, length(mtcars)) for (i in seq_along(mtcars)){ output[[i]] &lt;- mean(mtcars[[i]]) } output ## [1] 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 ## [7] 17.848750 0.437500 0.406250 3.687500 2.812500 Determine the type of each column in nycflights13::flights. output &lt;- vector(&quot;character&quot;, length(flights)) for (i in seq_along(flights)){ output[[i]] &lt;- typeof(flights[[i]]) } output ## [1] &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## [6] &quot;double&quot; &quot;integer&quot; &quot;integer&quot; &quot;double&quot; &quot;character&quot; ## [11] &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;double&quot; ## [16] &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; Compute the number of unique values in each column of iris. output &lt;- vector(&quot;integer&quot;, length(iris)) for (i in seq_along(iris)){ output[[i]] &lt;- unique(iris[[i]]) %&gt;% length() } output ## [1] 35 23 43 22 3 Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). output &lt;- vector(&quot;list&quot;, 4) input_means &lt;- c(-10, 0, 10, 100) for (i in seq_along(output)){ output[[i]] &lt;- rnorm(10, mean = input_means[[i]]) } output ## [[1]] ## [1] -11.371326 -10.118467 -10.582961 -10.324829 -7.604983 -9.300232 ## [7] -9.840124 -9.719733 -9.784274 -10.338814 ## ## [[2]] ## [1] -1.04951842 -0.68385670 0.17893523 0.07338463 -1.18028235 ## [6] -1.00777188 0.91491408 -0.14041984 -0.25074297 -0.50055019 ## ## [[3]] ## [1] 11.013913 9.790495 10.631115 10.325991 10.608040 9.463515 11.265961 ## [8] 10.630382 10.436201 8.907654 ## ## [[4]] ## [1] 99.37012 100.31396 99.06230 98.00350 100.31506 99.67347 101.02248 ## [8] 98.32484 98.62669 100.28487 Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors: example: out &lt;- &quot;&quot; for (x in letters) { out &lt;- stringr::str_c(out, x) } out collabse letters into length-one character vector with all characters concatenated str_c(letters, collapse = &quot;&quot;) ## [1] &quot;abcdefghijklmnopqrstuvwxyz&quot; example: x &lt;- sample(100) sd &lt;- 0 for (i in seq_along(x)) { sd &lt;- sd + (x[i] - mean(x)) ^ 2 } sd &lt;- sqrt(sd / (length(x) - 1)) sd ## [1] 29.01149 calculate standard deviaiton of x sd(x) ## [1] 29.01149 example: x &lt;- runif(100) out &lt;- vector(&quot;numeric&quot;, length(x)) out[1] &lt;- x[1] for (i in 2:length(x)) { out[i] &lt;- out[i - 1] + x[i] } out ## [1] 0.1543797 0.5168570 1.4323513 1.4861995 1.7440626 2.3503876 ## [7] 2.7033856 3.4933038 3.8878801 4.8166162 4.8404351 5.0134399 ## [13] 5.8128633 5.9002886 6.4672338 7.3249551 7.4813311 7.9067374 ## [19] 7.9143362 8.6500421 9.4114592 9.8109883 10.6637337 11.5345437 ## [25] 11.8881403 12.8609933 13.0060893 13.1121490 13.2820768 13.7832678 ## [31] 14.0103818 14.8921300 15.8878166 16.3724888 17.2897726 17.6764167 ## [37] 18.3759822 18.5914902 18.7581008 19.3126850 20.0314901 20.9729033 ## [43] 21.5123325 22.1361972 22.9338153 23.9220106 23.9905409 24.1247463 ## [49] 24.3690186 24.6778073 25.1676470 25.6649358 26.0152919 26.3936317 ## [55] 26.6769802 26.7589431 27.4933689 28.3744835 28.8274173 29.5040112 ## [61] 30.4625068 31.1908181 31.5785996 32.0691594 32.4015008 33.1859971 ## [67] 34.0973779 34.4118215 34.6828655 34.9383821 35.5988994 35.9820211 ## [73] 36.7825814 37.5402040 37.9568733 38.5686788 38.6336509 39.0451422 ## [79] 39.1208101 39.8826954 40.4989736 41.2877620 41.4204198 41.8790701 ## [85] 42.8085235 43.2102977 43.4620636 43.9427926 44.7306195 45.4886119 ## [91] 46.0891834 46.4679661 47.0817039 47.6331389 48.1357901 48.3671822 ## [97] 48.8290107 49.8198761 50.6520274 50.6527903 calculate cumulative sum cumsum(x) ## [1] 0.1543797 0.5168570 1.4323513 1.4861995 1.7440626 2.3503876 ## [7] 2.7033856 3.4933038 3.8878801 4.8166162 4.8404351 5.0134399 ## [13] 5.8128633 5.9002886 6.4672338 7.3249551 7.4813311 7.9067374 ## [19] 7.9143362 8.6500421 9.4114592 9.8109883 10.6637337 11.5345437 ## [25] 11.8881403 12.8609933 13.0060893 13.1121490 13.2820768 13.7832678 ## [31] 14.0103818 14.8921300 15.8878166 16.3724888 17.2897726 17.6764167 ## [37] 18.3759822 18.5914902 18.7581008 19.3126850 20.0314901 20.9729033 ## [43] 21.5123325 22.1361972 22.9338153 23.9220106 23.9905409 24.1247463 ## [49] 24.3690186 24.6778073 25.1676470 25.6649358 26.0152919 26.3936317 ## [55] 26.6769802 26.7589431 27.4933689 28.3744835 28.8274173 29.5040112 ## [61] 30.4625068 31.1908181 31.5785996 32.0691594 32.4015008 33.1859971 ## [67] 34.0973779 34.4118215 34.6828655 34.9383821 35.5988994 35.9820211 ## [73] 36.7825814 37.5402040 37.9568733 38.5686788 38.6336509 39.0451422 ## [79] 39.1208101 39.8826954 40.4989736 41.2877620 41.4204198 41.8790701 ## [85] 42.8085235 43.2102977 43.4620636 43.9427926 44.7306195 45.4886119 ## [91] 46.0891834 46.4679661 47.0817039 47.6331389 48.1357901 48.3671822 ## [97] 48.8290107 49.8198761 50.6520274 50.6527903 Combine your function writing and for loop skills: Write a for loop that prints() the lyrics to the children’s song “Alice the camel”. num_humps &lt;- c(&quot;five&quot;, &quot;four&quot;, &quot;three&quot;, &quot;two&quot;, &quot;one&quot;, &quot;no&quot;) for (i in seq_along(num_humps)){ paste0(&quot;Alice the camel has &quot;, num_humps[[i]], &quot; humps.&quot;) %&gt;% rep(3) %&gt;% writeLines() writeLines(&quot;So go, Alice, go.\\n&quot;) } ## Alice the camel has five humps. ## Alice the camel has five humps. ## Alice the camel has five humps. ## So go, Alice, go. ## ## Alice the camel has four humps. ## Alice the camel has four humps. ## Alice the camel has four humps. ## So go, Alice, go. ## ## Alice the camel has three humps. ## Alice the camel has three humps. ## Alice the camel has three humps. ## So go, Alice, go. ## ## Alice the camel has two humps. ## Alice the camel has two humps. ## Alice the camel has two humps. ## So go, Alice, go. ## ## Alice the camel has one humps. ## Alice the camel has one humps. ## Alice the camel has one humps. ## So go, Alice, go. ## ## Alice the camel has no humps. ## Alice the camel has no humps. ## Alice the camel has no humps. ## So go, Alice, go. Convert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure. nursery_bed &lt;- function(num, y) { output &lt;- vector(&quot;character&quot;, num) for (i in seq_along(output)) { output[[i]] &lt;- str_replace_all( &#39;There were x in the _y\\n And the little one said, \\n&quot;Roll over! Roll over!&quot;\\n So they all rolled over and\\n one fell out.&#39;, c(&quot;x&quot; = (length(output) - i + 1), &quot;_y&quot; = y)) } str_c(output, collapse = &quot;\\n\\n&quot;) %&gt;% writeLines() } nursery_bed(3, &quot;asteroid&quot;) ## There were 3 in the asteroid ## And the little one said, ## &quot;Roll over! Roll over!&quot; ## So they all rolled over and ## one fell out. ## ## There were 2 in the asteroid ## And the little one said, ## &quot;Roll over! Roll over!&quot; ## So they all rolled over and ## one fell out. ## ## There were 1 in the asteroid ## And the little one said, ## &quot;Roll over! Roll over!&quot; ## So they all rolled over and ## one fell out. Convert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface. This is a little bit of a lazy version… beer_rhyme &lt;- function(x, y, z){ output &lt;- vector(&quot;character&quot;, x) for (i in seq_along(output)){ output[i] &lt;- str_replace_all(&quot;x bottles of y on the z.\\n One fell off...&quot;, c( &quot;x&quot; = (x - i + 1), &quot;y&quot; = y, &quot;z&quot; = z )) } output &lt;- (str_c(output, collapse = &quot;\\n&quot;) %&gt;% str_c(&quot;\\nNo more bottles...&quot;, collapse = &quot;&quot;)) writeLines(output) } beer_rhyme(4, &quot;soda&quot;, &quot;toilet&quot;) ## 4 bottles of soda on the toilet. ## One fell off... ## 3 bottles of soda on the toilet. ## One fell off... ## 2 bottles of soda on the toilet. ## One fell off... ## 1 bottles of soda on the toilet. ## One fell off... ## No more bottles... It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step. How does this affect performance? Design and execute an experiment. preallocate &lt;- function(){ x &lt;- vector(&quot;double&quot;, 100) for (i in seq_along(x)){ x[i] &lt;- rnorm(1) } } growing &lt;- function(){ x &lt;- c(0) for (i in 1:100){ x[i] &lt;- rnorm(1) } } microbenchmark::microbenchmark( space = preallocate(), no_space = growing(), times = 20 ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## space 178.0 183.6 523.665 308.05 342.65 4991.2 20 a ## no_space 213.6 222.2 531.440 344.50 429.05 4081.9 20 a see roughly 35% better performance when creating ahead of time note: if you can do these operations with vectorized approach though – they’re often much faster microbenchmark::microbenchmark( space = preallocate(), no_space = growing(), vector = rnorm(100), times = 20 ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## space 155.8 161.45 177.075 163.50 166.20 349.6 20 b ## no_space 185.8 193.65 202.390 199.55 205.25 234.5 20 c ## vector 8.8 9.45 9.795 9.70 10.10 11.0 20 a vectorized was &gt; 10x faster 21.3 For loop variations 21.3.5 Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files &lt;- dir(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame. To start this problem, I first created a file directory, and then wrote in 26 csvs each with the most popular name from each year since 1880 for a particular letter35. Next I read these into a single dataframe with a for loop What happens if you use for (nm in names(x)) and x has no names? x &lt;- list(1:10, 11:18, 19:25) for (nm in names(x)) { print(x[[nm]]) } each iteration produces an error, so nothing is written What if only some of the elements are named? x &lt;- list(a = 1:10, 11:18, c = 19:25) for (nm in names(x)) { print(x[[nm]]) } ## [1] 1 2 3 4 5 6 7 8 9 10 ## NULL ## [1] 19 20 21 22 23 24 25 you have output for those with names and NULL for those without What if the names are not unique? x &lt;- list(a = 1:10, a = 11:18, c = 19:25) for (nm in names(x)) { print(x[[nm]]) } ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 19 20 21 22 23 24 25 it prints the first position with the name Write a function that prints the mean of each numeric column in a data frame, along with its name. For example, show_mean(iris) would print: show_mean(iris) #&gt; Sepal.Length: 5.84 #&gt; Sepal.Width: 3.06 #&gt; Petal.Length: 3.76 #&gt; Petal.Width: 1.20 (Extra challenge: what function did I use to make sure that the numbers lined up nicely, even though the variable names had different lengths?) show_mean &lt;- function(df){ # select just cols that are numeric out &lt;- vector(&quot;logical&quot;, length(df)) for (i in seq_along(df)) { out[[i]] &lt;- is.numeric(df[[i]]) } df_select &lt;- df[out] # keep/discard funs would have made this easy # make list of values w/ mean means &lt;- vector(&quot;list&quot;, length(df_select)) names(means) &lt;- names(df_select) for (i in seq_along(df_select)){ means[[i]] &lt;- mean(df_select[[i]], na.rm = TRUE) %&gt;% round(digits = 2) } # print out, use method to identify max chars for vars printed means_names &lt;- names(means) chars_max &lt;- (str_count(means_names) + str_count(as.character(means))) %&gt;% max() chars_pad &lt;- chars_max - (str_count(means_names) + str_count(as.character(means))) names(chars_pad) &lt;- means_names str_c(means_names, &quot;: &quot;, str_dup(&quot; &quot;, chars_pad), means) %&gt;% writeLines() } show_mean(flights) ## year: 2013 ## month: 6.55 ## day: 15.71 ## dep_time: 1349.11 ## sched_dep_time: 1344.25 ## dep_delay: 12.64 ## arr_time: 1502.05 ## sched_arr_time: 1536.38 ## arr_delay: 6.9 ## flight: 1971.92 ## air_time: 150.69 ## distance: 1039.91 ## hour: 13.18 ## minute: 26.23 What does this code do? How does it work? trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) { factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) } ) for (var in names(trans)) { mtcars[[var]] &lt;- trans[[var]](mtcars[[var]]) } mtcars first part builds list of functions, 2nd applies those to a dataset are storing the data transformations as a function and then applying this to a dataframe36 21.4: For loops vs. functionals 21.4.1 Read the documentation for apply(). In the 2d case, what two for loops does it generalise? It allows you to input either 1 or 2 for the MARGIN argument, which corresponds with looping over either the rows or the columns. Adapt col_summary() so that it only applies to numeric columns You might want to start with an is_numeric() function that returns a logical vector that has a TRUE corresponding to each numeric column. col_summary_gen &lt;- function(df, fun, ...) { #find cols that are numeric out &lt;- vector(&quot;logical&quot;, length(df)) for (i in seq_along(df)) { out[[i]] &lt;- is.numeric(df[[i]]) } #make list of values w/ mean df_select &lt;- df[out] output &lt;- vector(&quot;list&quot;, length(df_select)) names(output) &lt;- names(df_select) for (nm in names(output)) { output[[nm]] &lt;- fun(df_select[[nm]], ...) %&gt;% round(digits = 2) } as_tibble(output) } col_summary_gen(flights, fun = median, na.rm = TRUE) %&gt;% gather() # trick to gather all easily ## # A tibble: 14 x 2 ## key value ## &lt;chr&gt; &lt;dbl&gt; ## 1 year 2013 ## 2 month 7 ## 3 day 16 ## 4 dep_time 1401 ## 5 sched_dep_time 1359 ## 6 dep_delay -2 ## 7 arr_time 1535 ## 8 sched_arr_time 1556 ## 9 arr_delay -5 ## 10 flight 1496 ## 11 air_time 129 ## 12 distance 872 ## 13 hour 13 ## 14 minute 29 the ... makes this so you can add arguments to the functions. 21.5: The map functions 21.5.3 Write code that uses one of the map functions to: Compute the mean of every column in mtcars. purrr::map_dbl(mtcars, mean) ## mpg cyl disp hp drat wt ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 ## qsec vs am gear carb ## 17.848750 0.437500 0.406250 3.687500 2.812500 Determine the type of each column in nycflights13::flights. purrr::map_chr(flights, typeof) ## year month day dep_time sched_dep_time ## &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## dep_delay arr_time sched_arr_time arr_delay carrier ## &quot;double&quot; &quot;integer&quot; &quot;integer&quot; &quot;double&quot; &quot;character&quot; ## flight tailnum origin dest air_time ## &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;double&quot; ## distance hour minute time_hour ## &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; Compute the number of unique values in each column of iris. purrr::map(iris, unique) %&gt;% map_dbl(length) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 35 23 43 22 3 Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). purrr::map(c(-10, 0, 10, 100), rnorm, n = 10) ## [[1]] ## [1] -11.668016 -10.174630 -9.873417 -9.935144 -9.549267 -9.989001 ## [7] -9.991157 -9.490583 -9.020713 -11.215907 ## ## [[2]] ## [1] -1.3330518 1.7970408 -0.7859694 -1.5184894 0.4544287 0.2134496 ## [7] -1.0761067 0.1600194 -0.1258518 -0.6974829 ## ## [[3]] ## [1] 10.334081 9.523160 9.730305 10.855434 10.899334 11.522520 9.532049 ## [8] 9.778320 10.276128 9.939547 ## ## [[4]] ## [1] 98.63699 100.57597 100.23664 99.65274 100.66985 99.86635 99.79877 ## [8] 98.84634 101.00019 99.09162 # purrr::map_dbl(flights, ~mean(is.na(.x))) How can you create a single vector that for each column in a data frame indicates whether or not it’s a factor? purrr::map_lgl(iris, is.factor) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## FALSE FALSE FALSE FALSE TRUE What happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why? purrr::map(1:5, rnorm) ## [[1]] ## [1] 0.26078 ## ## [[2]] ## [1] 0.39670324 0.03106982 ## ## [[3]] ## [1] 1.0644632 -0.1632358 -1.0353975 ## ## [[4]] ## [1] -0.3556528 -0.5027896 2.0659595 -0.1360896 ## ## [[5]] ## [1] 0.50936851 0.16219258 -1.53746908 -0.04141543 -0.79950355 It runs on each item in the vector. map() runs on each element item within the input, i.e .x[[1]], .x[[2]], .x[[n]]. The elements of a numeric vector are scalars (or technically length 1 numeric vectors) In this case then it is passing the values 1, 2, 3, 4, 5 into the first argument of rnorm for each run, hence pattern above. What does map(-2:2, rnorm, n = 5) do? Why? map(-2:2, rnorm, n = 5) ## [[1]] ## [1] -1.829446 -3.357986 -3.582975 -2.039341 -2.087265 ## ## [[2]] ## [1] -0.6831658 -0.8729133 -0.3192894 -1.3425364 0.2383131 ## ## [[3]] ## [1] 0.43215278 -0.07629132 -0.14400722 1.85870258 0.13472292 ## ## [[4]] ## [1] -0.22256104 2.00645188 -0.06027834 1.44273092 0.69404413 ## ## [[5]] ## [1] 1.642268 2.233247 2.021023 1.988244 2.798515 It makes 5 vectors each of length 5 with the values centered at the means of -2,-1, 0, 1, 2 respectively. The reason is that the default filling of the first argument is already named by the defined input of ‘n = 5’, therefore, the inputs are instead going to the 2nd argument, and hence become the mean of the different rnorm calls. Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function. mtcars %&gt;% purrr::map( ~ lm(mpg ~ wt, data = .)) 21.9 Other patterns of for loops 21.9.3 Implement your own version of every() using a for loop. Compare it with purrr::every(). What does purrr’s version do that your version doesn’t? every_loop &lt;- function(x, fun, ...) { output &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { output[[i]] &lt;- fun(x[[i]]) } total &lt;- flatten_lgl(output) sum(total) == length(x) } x &lt;- list(flights, mtcars, iris) every_loop(x, is.data.frame) ## [1] TRUE every(x, is.data.frame) ## [1] TRUE Create an enhanced col_sum() that applies a summary function to every numeric column in a data frame. col_summary_enh &lt;- function(x,fun){ x %&gt;% keep(is.numeric) %&gt;% purrr::map_dbl(fun) } col_summary_enh(mtcars, median) ## mpg cyl disp hp drat wt qsec vs am ## 19.200 6.000 196.300 123.000 3.695 3.325 17.710 0.000 0.000 ## gear carb ## 4.000 2.000 A possible base R equivalent of col_sum() is: col_sum3 &lt;- function(df, f) { is_num &lt;- sapply(df, is.numeric) df_num &lt;- df[, is_num] sapply(df_num, f) } But it has a number of bugs as illustrated with the following inputs: df &lt;- tibble( x = 1:3, y = 3:1, z = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ) # OK col_sum3(df, mean) # Has problems: don&#39;t always return numeric vector col_sum3(df[1:2], mean) col_sum3(df[1], mean) col_sum3(df[0], mean) What causes the bugs? The vector output is not always consistent in it’s output type. Also, returns error when inputting an empty list due to indexing issue. Appendix 21.3.5.1 Using map outputted_csv &lt;- files_example %&gt;% mutate(csv_data = map(file_paths, read_csv)) outputted_csv &lt;- files_example %&gt;% mutate(csv_data = map(file_paths, safely(read_csv))) Plot of names Below is a plot of the proportion of individuals named the most popular letter in each year. This suggests that the top names by letter do not have as large of a proportion of the population ocmpared to historically. names_appended %&gt;% ggplot(aes(x = year, y = prop, colour = first_letter))+ geom_line() csv other example The code below might be used to read csvs from a shared drive. I added on the ‘file_path_pull’ and ‘files_example’ components to add in information on the file paths and other details that were relevant. You might also add this data into a new column on the output… files_path_pull &lt;- dir(&quot;//companydomain.com/directory/&quot;, pattern = &quot;csv$&quot;, full.names = TRUE) files_example &lt;- tibble(file_paths = files_path_pull[1:2]) %&gt;% extract(file_paths, into = c(&quot;path&quot;, &quot;name&quot;), regex = &quot;(.*)([0-9]{4}-[0-9]{2}-[0-9]{2})&quot;, remove = FALSE) read_dir &lt;- function(dir){ #input vector of file paths name and output appended file out &lt;- vector(&quot;list&quot;, length(dir)) for (i in seq_along(out)){ out[[i]] &lt;- read_csv(dir[[i]]) } out &lt;- bind_rows(out) out } read_dir(files_example$file_paths) 21.3.5.2 (with purrr) purrr::map_lgl(iris, is.factor) %&gt;% tibble::enframe() ## # A tibble: 5 x 2 ## name value ## &lt;chr&gt; &lt;lgl&gt; ## 1 Sepal.Length FALSE ## 2 Sepal.Width FALSE ## 3 Petal.Length FALSE ## 4 Petal.Width FALSE ## 5 Species TRUE Slightly less attractive printing show_mean2 &lt;- function(df) { df %&gt;% keep(is.numeric) %&gt;% map_dbl(mean, na.rm = TRUE) } show_mean2(flights) ## year month day dep_time sched_dep_time ## 2013.000000 6.548510 15.710787 1349.109947 1344.254840 ## dep_delay arr_time sched_arr_time arr_delay flight ## 12.639070 1502.054999 1536.380220 6.895377 1971.923620 ## air_time distance hour minute ## 150.686460 1039.912604 13.180247 26.230100 Maybe slightly better printing and in df show_mean3 &lt;- function(df){ df %&gt;% keep(is.numeric) %&gt;% map_dbl(mean, na.rm = TRUE) %&gt;% as_tibble() %&gt;% mutate(names = row.names(.)) } show_mean3(flights) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead. ## This warning is displayed once per session. ## # A tibble: 14 x 2 ## value names ## &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 ## 2 6.55 2 ## 3 15.7 3 ## 4 1349. 4 ## 5 1344. 5 ## 6 12.6 6 ## 7 1502. 7 ## 8 1536. 8 ## 9 6.90 9 ## 10 1972. 10 ## 11 151. 11 ## 12 1040. 12 ## 13 13.2 13 ## 14 26.2 14 Other method is to take advantage of the gather() function flights %&gt;% keep(is.numeric) %&gt;% map(mean, na.rm = TRUE) %&gt;% as_tibble() %&gt;% gather() ## # A tibble: 14 x 2 ## key value ## &lt;chr&gt; &lt;dbl&gt; ## 1 year 2013 ## 2 month 6.55 ## 3 day 15.7 ## 4 dep_time 1349. ## 5 sched_dep_time 1344. ## 6 dep_delay 12.6 ## 7 arr_time 1502. ## 8 sched_arr_time 1536. ## 9 arr_delay 6.90 ## 10 flight 1972. ## 11 air_time 151. ## 12 distance 1040. ## 13 hour 13.2 ## 14 minute 26.2 21.9.3.1 mine can’t handle shortcut formulas or new functions z &lt;- sample(10) z %&gt;% every( ~ . &lt; 11) ## [1] TRUE # e.g. below would fail # z %&gt;% # every_loop( ~ . &lt; 11) 21.9 mirroring keep below is one method for passing multiple, more complex arguments through keep, though you can also use function shortcuts (~) in keep and discard ##how to pass multiple functions through keep? #can use map to subset columns by multiple criteria and then subset at end flights %&gt;% purrr::map(is.na) %&gt;% purrr::map_dbl(sum) %&gt;% purrr::map_lgl(~.&gt;10) %&gt;% flights[.] ## # A tibble: 336,776 x 6 ## dep_time dep_delay arr_time arr_delay tailnum air_time ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 517 2 830 11 N14228 227 ## 2 533 4 850 20 N24211 227 ## 3 542 2 923 33 N619AA 160 ## 4 544 -1 1004 -18 N804JB 183 ## 5 554 -6 812 -25 N668DN 116 ## 6 554 -4 740 12 N39463 150 ## 7 555 -5 913 19 N516JB 158 ## 8 557 -3 709 -14 N829AS 53 ## 9 557 -3 838 -8 N593JB 140 ## 10 558 -2 753 8 N3ALAA 138 ## # ... with 336,766 more rows invoke examples Let’s change the example to be with quantile… invoke(runif, n = 10) ## [1] 0.775555937 0.328805817 0.920314980 0.176599637 0.210958651 ## [6] 0.890200325 0.456075735 0.498955991 0.148438198 0.001021321 list(&quot;01a&quot;, &quot;01b&quot;) %&gt;% invoke(paste, ., sep = &quot;-&quot;) ## [1] &quot;01a-01b&quot; set.seed(123) invoke_map(list(runif, rnorm), list(list(n = 10), list(n = 5))) ## [[1]] ## [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055 ## [8] 0.8924190 0.5514350 0.4566147 ## ## [[2]] ## [1] 1.7150650 0.4609162 -1.2650612 -0.6868529 -0.4456620 set.seed(123) invoke_map(list(runif, rnorm), list(list(n = 10), list(5, 50))) ## [[1]] ## [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055 ## [8] 0.8924190 0.5514350 0.4566147 ## ## [[2]] ## [1] 51.71506 50.46092 48.73494 49.31315 49.55434 list(m1 = mean, m2 = median) %&gt;% invoke_map(x = rcauchy(100)) ## $m1 ## [1] 0.7316016 ## ## $m2 ## [1] 0.1690467 rcauchy(100) ## [1] -1.99514216 1.57378677 1.44901985 0.82604308 2.30072052 ## [6] -0.04961749 0.52626840 0.29408692 0.47790231 -1.47138470 ## [11] -2.54305059 -0.35508248 -1.65511601 -1.08467708 -15.03813728 ## [16] -1.82118206 -0.62669137 -0.79456204 -0.06347636 5.19179251 ## [21] 1.48851593 3.42095041 0.03289526 0.65171559 -0.53864091 ## [26] 0.88812626 0.93375555 0.24570517 0.97348569 -1.11905466 ## [31] -0.51964526 128.72537963 2.72138263 0.97793363 0.36391811 ## [36] 2.77745450 -4.34935786 0.81096079 5.70518746 0.81669440 ## [41] -138.41947905 2.02359725 -1.96283674 2.40809060 2.04850398 ## [46] -9.41347275 -1.06265274 0.83312509 3.55625549 1.10375978 ## [51] -2.31140048 0.65162145 -0.45665528 -1.02179975 -1.71189590 ## [56] -2.57239721 2.35617831 -10.63750166 -0.41538322 -3.80770683 ## [61] -0.55070513 1.49607830 -1.30359005 1.09910916 -3.27457763 ## [66] 16.99304208 1.09921270 -4.86030197 -0.27969649 -0.31842181 ## [71] 1.16466121 1.59209243 -0.04514112 -2.52586678 -0.19951960 ## [76] 9.47599952 3.31841045 -1.82945785 0.51884667 -4.29179059 ## [81] 0.93155898 -0.11880720 -3.03333758 -21.16294537 3.16450655 ## [86] -0.39503234 2.19801293 1.27457150 0.59413768 0.60064481 ## [91] 17.70703023 1.01880490 0.80764382 -1.63905090 0.15086898 ## [96] -1.36865319 1.99173761 3.39988162 -0.63043489 -0.26058630 Let’s store everything in a dataframe… set.seed(123) tibble(funs = list(rn = &quot;rnorm&quot;, rp = &quot;rpois&quot;, ru = &quot;runif&quot;), params = list(list(n = 20, mean = 10), list(n = 20, lambda = 3), list(n = 20, min = -1, max = 1))) %&gt;% with(invoke_map_df(funs, params)) ## # A tibble: 20 x 3 ## rn rp ru ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9.44 1 0.330 ## 2 9.77 2 -0.810 ## 3 11.6 2 -0.232 ## 4 10.1 2 -0.451 ## 5 10.1 1 0.629 ## 6 11.7 1 -0.103 ## 7 10.5 2 0.620 ## 8 8.73 3 0.625 ## 9 9.31 2 0.589 ## 10 9.55 5 -0.120 ## 11 11.2 0 0.509 ## 12 10.4 3 0.258 ## 13 10.4 4 0.420 ## 14 10.1 1 -0.999 ## 15 9.44 3 -0.0494 ## 16 11.8 2 -0.560 ## 17 10.5 1 -0.240 ## 18 8.03 4 0.226 ## 19 10.7 5 -0.296 ## 20 9.53 2 -0.778 map_df(iris, ~.x*2) ## Warning in Ops.factor(.x, 2): &#39;*&#39; not meaningful for factors ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 10.2 7 2.8 0.4 NA ## 2 9.8 6 2.8 0.4 NA ## 3 9.4 6.4 2.6 0.4 NA ## 4 9.2 6.2 3 0.4 NA ## 5 10 7.2 2.8 0.4 NA ## 6 10.8 7.8 3.4 0.8 NA ## 7 9.2 6.8 2.8 0.6 NA ## 8 10 6.8 3 0.4 NA ## 9 8.8 5.8 2.8 0.4 NA ## 10 9.8 6.2 3 0.2 NA ## # ... with 140 more rows select(iris, -Species) %&gt;% flatten_dbl() %&gt;% mean() ## [1] 3.4645 mean.and.median &lt;- function(x){ list(mean = mean(x, na.rm = TRUE), median = median(x, na.rm = TRUE)) } Difference between dfr and dfc, taken from here: https://bio304-class.github.io/bio304-fall2017/control-flow-in-R.html iris %&gt;% select(-Species) %&gt;% map_dfr(mean.and.median) %&gt;% bind_cols(tibble(names = names(select(iris, -Species)))) ## # A tibble: 4 x 3 ## mean median names ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.84 5.8 Sepal.Length ## 2 3.06 3 Sepal.Width ## 3 3.76 4.35 Petal.Length ## 4 1.20 1.3 Petal.Width iris %&gt;% select(-Species) %&gt;% map_dfr(mean.and.median) %&gt;% bind_cols(tibble(names = names(select(iris, -Species)))) ## # A tibble: 4 x 3 ## mean median names ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.84 5.8 Sepal.Length ## 2 3.06 3 Sepal.Width ## 3 3.76 4.35 Petal.Length ## 4 1.20 1.3 Petal.Width iris %&gt;% select(-Species) %&gt;% map_dfc(mean.and.median) ## # A tibble: 1 x 8 ## mean median mean1 median1 mean2 median2 mean3 median3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 5.8 3.06 3 3.76 4.35 1.20 1.3 indexing nms caution When creating your empty list, use indexes rather than names if you are creating values, otherwise you are creating new values on the list. E.g. in the example below I the output ends up being length 6 because you have the 3 NULL values plus the 3 newly created named positions. x &lt;- list(a = 1:10, b = 11:18, c = 19:25) output &lt;- vector(&quot;list&quot;, length(x)) for (nm in names(x)) { output[[nm]] &lt;- x[[nm]] * 3 } output ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## $a ## [1] 3 6 9 12 15 18 21 24 27 30 ## ## $b ## [1] 33 36 39 42 45 48 51 54 ## ## $c ## [1] 57 60 63 66 69 72 75 in-class notes the map_* functions are essentially like running a flatten_* after running map. E.g. the two things below are equivalent map(flights, typeof) %&gt;% flatten_chr() ## [1] &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## [6] &quot;double&quot; &quot;integer&quot; &quot;integer&quot; &quot;double&quot; &quot;character&quot; ## [11] &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;double&quot; ## [16] &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; map_chr(flights, typeof) ## year month day dep_time sched_dep_time ## &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; &quot;integer&quot; ## dep_delay arr_time sched_arr_time arr_delay carrier ## &quot;double&quot; &quot;integer&quot; &quot;integer&quot; &quot;double&quot; &quot;character&quot; ## flight tailnum origin dest air_time ## &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;double&quot; ## distance hour minute time_hour ## &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; Calculate the number of unique values for each level iris %&gt;% map(unique) %&gt;% map_dbl(length) map_int(iris, ~length(unique(.x))) Iterate through different min and max values min_params &lt;- c(-1, 0, -10) max_params &lt;- c(11:13) map2(.x = min_params, .y = max_params, ~runif(n = 10, min = .x, max = .y)) ## [[1]] ## [1] 1.9234337 7.0166670 4.0117614 8.4583500 0.2343757 4.2187129 ## [7] 10.8194838 9.7166134 9.6376287 1.1006318 ## ## [[2]] ## [1] 1.568348 7.837223 4.122198 7.881098 3.844479 2.252293 9.387532 ## [8] 1.123140 5.601348 6.138066 ## ## [[3]] ## [1] 3.7997461 -2.3450586 1.2380998 11.9528980 1.1067551 10.4780551 ## [7] 11.0320783 4.0009046 -0.5541351 -6.6168221 When using pmap it’s often best to keep the parameters in a dataframe min_df_params &lt;- tibble(n = c(10, 15, 20, 50 ), min = c(-1, 0, 1, 2), max = c(0, 1, 2, 3)) pmap(min_df_params, runif) ## [[1]] ## [1] -0.06470020 -0.69877110 -0.93927943 -0.05227306 -0.27940373 ## [6] -0.85770570 -0.45071534 -0.04590876 -0.41451665 -0.59548972 ## ## [[2]] ## [1] 0.6478935 0.3198206 0.3077200 0.2197676 0.3694889 0.9842192 0.1542023 ## [8] 0.0910440 0.1419069 0.6900071 0.6192565 0.8913941 0.6729991 0.7370777 ## [15] 0.5211357 ## ## [[3]] ## [1] 1.659838 1.821805 1.786282 1.979822 1.439432 1.311702 1.409475 ## [8] 1.010467 1.183850 1.842729 1.231162 1.239100 1.076691 1.245724 ## [15] 1.732135 1.847453 1.497527 1.387909 1.246449 1.111096 ## ## [[4]] ## [1] 2.389994 2.571935 2.216893 2.444768 2.217991 2.502300 2.353905 ## [8] 2.649985 2.374714 2.355445 2.533688 2.740334 2.221103 2.412746 ## [15] 2.265687 2.629973 2.183828 2.863644 2.746568 2.668285 2.618018 ## [22] 2.372238 2.529836 2.874682 2.581750 2.839768 2.312448 2.708290 ## [29] 2.265018 2.594343 2.481290 2.265033 2.564590 2.913188 2.901874 ## [36] 2.274167 2.321483 2.985641 2.619993 2.937314 2.466533 2.406833 ## [43] 2.659230 2.152347 2.572867 2.238726 2.962359 2.601366 2.515030 ## [50] 2.402573 You can often use map a bunch of output that can then be stored in a tibble tibble(type = map_chr(mtcars, typeof), means = map_dbl(mtcars, mean), median = map_dbl(mtcars, median), names = names(mtcars)) ## # A tibble: 11 x 4 ## type means median names ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 double 20.1 19.2 mpg ## 2 double 6.19 6 cyl ## 3 double 231. 196. disp ## 4 double 147. 123 hp ## 5 double 3.60 3.70 drat ## 6 double 3.22 3.32 wt ## 7 double 17.8 17.7 qsec ## 8 double 0.438 0 vs ## 9 double 0.406 0 am ## 10 double 3.69 4 gear ## 11 double 2.81 2 carb Provide the number of unique values for all columns excluding columns with numeric types or date types. num_unique &lt;- function(df) { df %&gt;% keep(~is_character(.x) | is.factor(.x)) %&gt;% map(~length(unique(.x))) %&gt;% as_tibble() %&gt;% gather() %&gt;% rename(field_name = key, num_unique = value) } num_unique(flights) ## # A tibble: 4 x 2 ## field_name num_unique ## &lt;chr&gt; &lt;int&gt; ## 1 carrier 16 ## 2 tailnum 4044 ## 3 origin 3 ## 4 dest 105 num_unique(iris) ## # A tibble: 1 x 2 ## field_name num_unique ## &lt;chr&gt; &lt;int&gt; ## 1 Species 3 num_unique(mpg) ## # A tibble: 6 x 2 ## field_name num_unique ## &lt;chr&gt; &lt;int&gt; ## 1 manufacturer 15 ## 2 model 38 ## 3 trans 10 ## 4 drv 3 ## 5 fl 5 ## 6 class 7 This is a very powerful practice because it allows you to save / keep track of your manipulations and apply them at other locations, while keeping the logic very well organized – go and use this for documenting your work / transformations↩ "],
["23-model-basics.html", "Ch. 23: Model basics 23.2: A simple model 23.3: Visualising models 23.4: Formulas and model families Appendix", " Ch. 23: Model basics WARNING many of the solutions and notes in this chapter use various modelr::sim* datasets but do not explicitly refer to these coming from the modelr package. Key questions: 23.2.1. #2 23.3.3. #1, 4 23.4.5. #4 Functions and notes: geom_abline create line or lines given intercept and slopes, e.g. geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) optim general purpose function for optimization using Newton-Raphson search coef function for extracting coefficients from a linear model modelr::data_grid, can be used to generate an evenly spaced grid of values that covers region where data lies37 First arg is dataframe, next args are variable names to build grid from when using with continuous data, seq_range or similar can be a good complement, see38 for an example where this is used. seq_range(x1, 5) takes 5 values evenly spaced within this set. Two other useful args: pretty = TRUE : will generate a “pretty sequence” seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE) , 0, 0.2, 0.4, 0.6, 0.8, 1 trim = 0.1 will trim off 10% of tail values (useful if vars have long tailed distribution and you want to focus on values near center) *`expand = 0.1 is kind of the opposite and expands the range by 10% modelr::add_predictions takes a data frame and modle an adds predictions from the model to a new column modelr::add_residuals is similar to above but requires actual value be in dataframe so that residuals can be calculated modelr::spread_residuals and modelr::gather_residuals allow you to do this for multiple models at once. equivalent are avaialble for *_predictions as well. use model_matrix to see what equation is being fitted To include all 2-way interactions can do something like this model_matrix(sim3, y ~ (x1 + x2 + x3)^2) or model_matrix(sim3, y ~ (.)^2) Use I() in I(x2^2) to incorporate squared term or any transformation that includes +, -, *, or ^ Use poly to include 1, 2, … n order terms associated with a variable, e.g. model_matrix(df, y ~ poly(x, 3)) splines::ns() represent safer alternative to poly that is less likely to become extreme, e.g.Interesting spline note39. nobs(mod) see how many observations were used in model building (assuming ‘mod’ represents a model) make dropping of missing values explicit with options(na.action = na.warn) to make silent in specific models use e.g. mod &lt;- lm(y ~ x, data = df, na.action = na.exclude) 23.2: A simple model 23.2.1 One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model? sim1a &lt;- tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2) ) generate n number of datasets that fit characteristics of sim1a sim1a_mult &lt;- tibble(num = 1:500) %&gt;% rowwise() %&gt;% mutate(data = list(tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2) ))) %&gt;% #undoes rowwise (used to have much more of workflow with rowwise, but have #changed to use more of map) ungroup() plots_prep &lt;- sim1a_mult %&gt;% mutate(mods = map(data, ~lm(y ~ x, data = .x))) %&gt;% mutate(preds = map2(data, mods, modelr::add_predictions), rmse = map2_dbl(mods, data, modelr::rmse), mae = map2_dbl(mods, data, modelr::mae)) plots_prep %&gt;% ggplot(aes(x = &quot;rmse&quot;, y = rmse))+ ggbeeswarm::geom_beeswarm() # # Other option for visualizing # plots_prep %&gt;% # ggplot(aes(x = &quot;rmse&quot;, y = rmse))+ # geom_violin() as a metric it tends to be more suseptible to outliers, than say mae One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance: measure_distance &lt;- function(mod, data) { diff &lt;- data$y - make_prediction(mod, data) mean(abs(diff)) } Use optim() to fit this model to the simulated data above and compare it to the linear model. model_1df &lt;- function(betas, x1 = sim1$x) { betas[1] + x1 * betas[2] } measure_mae &lt;- function(mod, data) { diff &lt;- data$y - model_1df(betas = mod, data$x) mean(abs(diff)) } measure_rmse &lt;- function(mod, data) { diff &lt;- data$y - model_1df(betas = mod, data$x) sqrt(mean(diff^2)) } best_mae_sims &lt;- map(sim1a_mult$data, ~optim(c(0,0), measure_mae, data = .x)) best_rmse_sims &lt;- map(sim1a_mult$data, ~optim(c(0,0), measure_rmse, data = .x)) mae_df &lt;- best_mae_sims %&gt;% map(&quot;value&quot;) %&gt;% transpose() %&gt;% set_names(c(&quot;error&quot;)) %&gt;% as_tibble() %&gt;% unnest() %&gt;% mutate(error_type = &quot;mae&quot;, row_n = row_number()) rmse_df &lt;- best_rmse_sims %&gt;% map(&quot;value&quot;) %&gt;% transpose() %&gt;% set_names(c(&quot;error&quot;)) %&gt;% as_tibble() %&gt;% unnest() %&gt;% mutate(error_type = &quot;rmse&quot;, row_n = row_number()) bind_rows(rmse_df, mae_df) %&gt;% ggplot(aes(x = error_type, colour = error_type))+ ggbeeswarm::geom_beeswarm(aes(y = error))+ facet_wrap(~error_type, scales = &quot;free_x&quot;) you can see the error for rmse seems to have more extreme examples One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this? model1 &lt;- function(a, data) { a[1] + data$x * a[2] + a[3] } the problem is that is that there are multiple “best” solutions in this example. a[1] and a[3] together represent the intercept here. models_two &lt;- vector(&quot;list&quot;, 2) model1 &lt;- function(a, data) { a[1] + data$x * a[2] + a[3] } models_two[[1]] &lt;- optim(c(0, 0, 0), measure_rmse, data = sim1) models_two[[1]]$par ## [1] 4.219814 2.051678 -3.049197 model1 &lt;- function(a, data) { a[1] + data$x * a[2] } models_two[[2]] &lt;- optim(c(0, 0), measure_rmse, data = sim1) models_two ## [[1]] ## [[1]]$par ## [1] 4.219814 2.051678 -3.049197 ## ## [[1]]$value ## [1] 2.128181 ## ## [[1]]$counts ## function gradient ## 110 NA ## ## [[1]]$convergence ## [1] 0 ## ## [[1]]$message ## NULL ## ## ## [[2]] ## [[2]]$par ## [1] 4.222248 2.051204 ## ## [[2]]$value ## [1] 2.128181 ## ## [[2]]$counts ## function gradient ## 77 NA ## ## [[2]]$convergence ## [1] 0 ## ## [[2]]$message ## NULL a1 and a3 are essentially equivalent, so optimizes somewhat arbitrarily, in this case can see the a1+a3 in the 1st (when there are 3 parameters) is equal to a1 in the 2nd (when there are only two parameters)…40 23.3: Visualising models 23.3.3 Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()? sim1_mod &lt;- lm(y ~ x, data = sim1) sim1_mod_loess &lt;- loess(y ~ x, data = sim1) #Look at plot of points sim1 %&gt;% add_predictions(sim1_mod, var = &quot;pred_lin&quot;) %&gt;% add_predictions(sim1_mod_loess) %&gt;% add_residuals(sim1_mod_loess) %&gt;% ggplot()+ geom_point(aes(x = x, y = y))+ geom_smooth(aes(x = x, y = y), se = FALSE)+ geom_line(aes(x = x, y = pred_lin, colour = &quot;pred_lin&quot;), alpha = 0.3, size = 2.5)+ geom_line(aes(x = x, y = pred, colour = &quot;pred_loess&quot;), alpha = 0.3, size = 2.5) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; For sim1, the default value for geom_smooth is to use loess, so it is the exact same. geom_smooth will sometimes use gam or other methods depending on data, note that there is also a weight argument that can be useful this relationship looks pretty solidly linear Plot of the resids: sim1 %&gt;% add_predictions(sim1_mod_loess, var = &quot;pred_loess&quot;) %&gt;% add_residuals(sim1_mod_loess, var = &quot;resid_loess&quot;) %&gt;% add_predictions(sim1_mod, var = &quot;pred_lin&quot;) %&gt;% add_residuals(sim1_mod, var = &quot;resid_lin&quot;) %&gt;% # mutate(row_n = row_number) %&gt;% ggplot()+ geom_ref_line(h = 0)+ geom_point(aes(x = x, y = resid_lin, colour = &quot;pred_lin&quot;))+ geom_point(aes(x = x, y = resid_loess, colour = &quot;pred_loess&quot;)) Distribution of residuals: sim1 %&gt;% gather_residuals(sim1_mod, sim1_mod_loess) %&gt;% mutate(model = ifelse(model == &quot;sim1_mod&quot;, &quot;sim1_mod_lin&quot;, model)) %&gt;% ggplot()+ geom_histogram(aes(x = resid, fill = model))+ facet_wrap(~model) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ? spread_predictions() adds a new pred for each model included gather_predictions() adds 2 columns model and pred for each model and repeats the input rows for each model (seems like it would work well with facets) sim1 %&gt;% spread_predictions(sim1_mod, sim1_mod_loess) ## # A tibble: 30 x 4 ## x y sim1_mod sim1_mod_loess ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4.20 6.27 5.34 ## 2 1 7.51 6.27 5.34 ## 3 1 2.13 6.27 5.34 ## 4 2 8.99 8.32 8.27 ## 5 2 10.2 8.32 8.27 ## 6 2 11.3 8.32 8.27 ## 7 3 7.36 10.4 10.8 ## 8 3 10.5 10.4 10.8 ## 9 3 10.5 10.4 10.8 ## 10 4 12.4 12.4 12.8 ## # ... with 20 more rows What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important? It comes from ggplot2 and shows either a geom_hline or a geom_vline, depending on whether you specify h or v. ggplot2::geom_ref_line Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals? may be good for situations when you have TONS of residuals, and is hard to look at?… pros are it may be easier to get sense of count, cons are that you can’t plot it against something like x so patterns associated with residuals will not be picked-up, e.g. heteroskedasticity, or more simply, signs that the model could be improved by incorporating other vars in the model 23.4: Formulas and model families 23.4.5 What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions? mod2 &lt;- lm(y ~ x, data = sim2) mod2_NoInt &lt;- lm(y ~ x - 1, data = sim2) mod2 ## ## Call: ## lm(formula = y ~ x, data = sim2) ## ## Coefficients: ## (Intercept) xb xc xd ## 1.1522 6.9639 4.9750 0.7588 mod2_NoInt ## ## Call: ## lm(formula = y ~ x - 1, data = sim2) ## ## Coefficients: ## xa xb xc xd ## 1.152 8.116 6.127 1.911 you have an ANOVA analysis, one of the variables takes on the value of the intercept, the others all have the value of the intercept added to them. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction? model_matrix(y ~ x1 * x2, data = sim3) ## # A tibble: 120 x 8 ## `(Intercept)` x1 x2b x2c x2d `x1:x2b` `x1:x2c` `x1:x2d` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 0 0 0 0 ## 2 1 1 0 0 0 0 0 0 ## 3 1 1 0 0 0 0 0 0 ## 4 1 1 1 0 0 1 0 0 ## 5 1 1 1 0 0 1 0 0 ## 6 1 1 1 0 0 1 0 0 ## 7 1 1 0 1 0 0 1 0 ## 8 1 1 0 1 0 0 1 0 ## 9 1 1 0 1 0 0 1 0 ## 10 1 1 0 0 1 0 0 1 ## # ... with 110 more rows model_matrix(y ~ x1 * x2, data = sim4) ## # A tibble: 300 x 4 ## `(Intercept)` x1 x2 `x1:x2` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -1 -1 1 ## 2 1 -1 -1 1 ## 3 1 -1 -1 1 ## 4 1 -1 -0.778 0.778 ## 5 1 -1 -0.778 0.778 ## 6 1 -1 -0.778 0.778 ## 7 1 -1 -0.556 0.556 ## 8 1 -1 -0.556 0.556 ## 9 1 -1 -0.556 0.556 ## 10 1 -1 -0.333 0.333 ## # ... with 290 more rows because each of the levels are multiplied by one another (just don’t have to write in the design variables) Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.) mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) not completed For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim? ```r mod1 &lt;- lm(y ~ x1 + x2, data = sim4) mod2 &lt;- lm(y ~ x1 * x2, data = sim4) grid &lt;- modelr::seq_range(sim4$x1, n = 3, pretty = TRUE) sim4 %&gt;% gather_residuals(mod1, mod2) %&gt;% mutate(resid_abs = (resid)^2) %&gt;% group_by(model) %&gt;% summarise(rmse = sqrt(mean(resid_abs))) ``` ``` ## # A tibble: 2 x 2 ## model rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 2.10 ## 2 mod2 2.06 ``` * The aggregate `rmse` between the two models is nearly the same. ```r sim4 %&gt;% gather_residuals(mod1, mod2) %&gt;% ggplot(aes(x = resid, fill = model, group = model))+ geom_density(alpha = 0.3) ``` &lt;img src=&quot;23-model-basics_files/figure-html/unnamed-chunk-19-1.png&quot; width=&quot;672&quot; /&gt; * any difference in resids is pretty subtle... *Let&#39;s plot them though and see how their predictions differ* ```r sim4 %&gt;% spread_residuals(mod1, mod2) %&gt;% gather_predictions(mod1, mod2) %&gt;% ggplot(aes(x1, pred, colour = x2, group = x2))+ geom_line()+ geom_point(aes(y = y), alpha = 0.3)+ facet_wrap(~model) ``` &lt;img src=&quot;23-model-basics_files/figure-html/unnamed-chunk-20-1.png&quot; width=&quot;672&quot; /&gt; * notice subtle difference where for mod2 as x2 decreases, the predicted value for x1 also decreases, this is because the interaciton term between these is positive * the values near where x2 and x1 are most near 0 should be where the residuals are most similar *Plot difference in residuals* ```r sim4 %&gt;% spread_residuals(mod1, mod2) %&gt;% mutate(mod2_less_mod1 = mod2 - mod1) %&gt;% group_by(x1, x2) %&gt;% summarise(mod2_less_mod1 = mean(mod2_less_mod1) ) %&gt;% ungroup() %&gt;% ggplot(aes(x = x1, y = x2))+ geom_tile(aes(fill = mod2_less_mod1))+ geom_text(aes(label = round(mod2_less_mod1, 1)), size = 3)+ scale_fill_gradient2() ``` &lt;img src=&quot;23-model-basics_files/figure-html/unnamed-chunk-21-1.png&quot; width=&quot;672&quot; /&gt; * This shows how `mod2` has higher valued predictions when x1 and x2 are opposite signs compared to the predictions from `mod1` *Plot difference in distance from 0 between `mod1` and `mod1` resids* ```r sim4 %&gt;% spread_residuals(mod1, mod2) %&gt;% mutate_at(c(&quot;mod1&quot;, &quot;mod2&quot;), abs) %&gt;% mutate(mod2_less_mod1 = mod2 - mod1) %&gt;% group_by(x1, x2) %&gt;% summarise(mod2_less_mod1 = mean(mod2_less_mod1) ) %&gt;% ungroup() %&gt;% ggplot(aes(x = x1, y = x2))+ geom_tile(aes(fill = mod2_less_mod1))+ geom_text(aes(label = round(mod2_less_mod1, 1)), size = 3)+ scale_fill_gradient2() ``` &lt;img src=&quot;23-model-basics_files/figure-html/unnamed-chunk-22-1.png&quot; width=&quot;672&quot; /&gt; * see slightly more red than blue indicating that `mod2` may, in general, have slightly smaller residuals on a wider range of locations * however little and `mod1` has advantage of simplicity Appendix Other model families Generalised linear models, e.g. stats::glm(). Linear models assume that the response is continuous and the error has a normal distribution. Generalised linear models extend linear models to include non-continuous responses (e.g. binary data or counts). They work by defining a distance metric based on the statistical idea of likelihood. Generalised additive models, e.g. mgcv::gam(), extend generalised linear models to incorporate arbitrary smooth functions. That means you can write a formula like y ~ s(x) which becomes an equation like y = f(x) and let gam() estimate what that function is (subject to some smoothness constraints to make the problem tractable). Penalised linear models, e.g. glmnet::glmnet(), add a penalty term to the distance that penalises complex models (as defined by the distance between the parameter vector and the origin). This tends to make models that generalise better to new datasets from the same population. Robust linear models, e.g. MASS:rlm(), tweak the distance to downweight points that are very far away. This makes them less sensitive to the presence of outliers, at the cost of being not quite as good when there are no outliers. Trees, e.g. rpart::rpart(), attack the problem in a completely different way than linear models. They fit a piece-wise constant model, splitting the data into progressively smaller and smaller pieces. Trees aren’t terribly effective by themselves, but they are very powerful when used in aggregate by models like random forests (e.g. randomForest::randomForest()) or gradient boosting machines (e.g. xgboost::xgboost.) 23.2 book example models &lt;- tibble( a1 = runif(250, -20, 40), a2 = runif(250, -5, 5) ) ggplot(sim1, aes(x,y))+ geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 0.25)+ geom_point() Next, lets set-up a way to calculate the distance between predicted value and each point. models_error &lt;- models %&gt;% mutate(preds = map2(.y = a1, .x = a2, ~mutate(sim1, pred = .x*x + .y, resid = y - pred, error_squared = (y - pred)^2, error_abs = abs(y - pred))), rmse = map_dbl(preds, ~(with(.x, mean(error_squared)) %&gt;% sqrt(.))), mae = map_dbl(preds, ~with(.x, mean(error_abs))), rank_rmse = min_rank(rmse)) ggplot(sim1, aes(x, y))+ geom_abline(aes(intercept = a1, slope = a2, colour = -rmse), data = filter(models_error, rank_rmse &lt;= 10))+ geom_point() Could instead plot this as a model of a1 vs a2 and whichever does the best models_error %&gt;% ggplot(aes(x = a1, y = a2))+ geom_point(colour = &quot;red&quot;, size = 4, data = filter(models_error, rank_rmse &lt; 15))+ geom_point(aes(colour = -rmse)) Could be more methodical and use Grid Search. Let’s use the min and max points of the top 15 to set. #need helper function because distance function expects the model as a numeric vector of length 2 sim1_rmse &lt;- function(b0, b1, df = sim1, x = &quot;x&quot;, y = &quot;y&quot;){ ((b0 + b1*df[[x]]) - df[[y]])^2 %&gt;% mean() %&gt;% sqrt() } sim1_rmse(2,3) ## [1] 4.574414 grid_space &lt;- models_error %&gt;% filter(rank_rmse &lt; 15) %&gt;% summarise(min_x = min(a1), max_x = max(a1), min_y = min(a2), max_y = max(a2)) grid_models &lt;- data_grid(grid_space, a1 = seq(min_x, max_x, length = 25), a2 = seq(min_y, max_y, length = 25) ) %&gt;% mutate(rmse = map2_dbl(a1, a2, sim1_rmse, df = sim1)) grid_models %&gt;% ggplot(aes(x = a1, y = a2))+ geom_point(colour = &quot;red&quot;, size = 4, data = filter(grid_models, min_rank(rmse) &lt; 15))+ geom_point(aes(colour = -rmse)) In the future could add-in a grid-search that would have used PCA to first rotate axes and then do min and max values. Could instead use Newton-Raphson search with optim model_1df &lt;- function(betas, x1 = sim1$x) { betas[1] + x1 * betas[2] } measure_rmse &lt;- function(mod, data) { diff &lt;- data$y - model_1df(betas = mod, data$x) sqrt(mean(diff^2)) } best_rmse &lt;- optim(c(0,0), measure_rmse, data = sim1) best_rmse$par ## [1] 4.222248 2.051204 best_rmse$value ## [1] 2.128181 Above produces roughly equal to R’s lm() function sim1_mod &lt;- lm(y ~ x, data = sim1) sim1_mod %&gt;% coef ## (Intercept) x ## 4.220822 2.051533 rmse(sim1_mod, sim1) ## [1] 2.128181 Notice are slightly different, perhaps due to number of steps optim() will take E.g. could build a function for optimizing upon MAE instead and still works measure_mae &lt;- function(mod, data) { diff &lt;- data$y - model_1df(betas = mod, data$x) mean(abs(diff)) } best_mae &lt;- optim(c(0,0), measure_mae, data = sim1) best_mae$par ## [1] 4.364852 2.048918 23.2.1.1 *Let&#39;s look at differences in the coefficients produced:* ```r mae_df &lt;- best_mae_sims %&gt;% map(&quot;par&quot;) %&gt;% transpose() %&gt;% set_names(c(&quot;bo&quot;, &quot;b1&quot;)) %&gt;% as_tibble() %&gt;% unnest() %&gt;% mutate(error_type = &quot;mae&quot;, row_n = row_number()) rmse_df &lt;- best_rmse_sims %&gt;% map(&quot;par&quot;) %&gt;% transpose() %&gt;% set_names(c(&quot;bo&quot;, &quot;b1&quot;)) %&gt;% as_tibble() %&gt;% unnest() %&gt;% mutate(error_type = &quot;rmse&quot;, row_n = row_number()) bind_rows(rmse_df, mae_df) %&gt;% ggplot(aes(x = bo, colour = error_type))+ geom_point(aes(y = b1)) ``` &lt;img src=&quot;23-model-basics_files/figure-html/unnamed-chunk-29-1.png&quot; width=&quot;672&quot; /&gt; * see more variability in the b1 * another way of visualizing the variability in coefficients is below ```r left_join(rmse_df, mae_df, by = &quot;row_n&quot;, suffix = c(&quot;_rmse&quot;, &quot;_mae&quot;)) %&gt;% ggplot(aes(x = b1_rmse, y = b1_mae))+ geom_point()+ coord_fixed() ``` &lt;img src=&quot;23-model-basics_files/figure-html/unnamed-chunk-30-1.png&quot; width=&quot;672&quot; /&gt; 23.3.3.2 #How can I add a prefix when using spread_predictions() ? -- could use the #method below sim1 %&gt;% gather_predictions(sim1_mod, sim1_mod_loess) %&gt;% mutate(model = str_c(model, &quot;_pred&quot;)) %&gt;% spread(key = model, value = pred) ## # A tibble: 30 x 4 ## x y sim1_mod_loess_pred sim1_mod_pred ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.13 5.34 6.27 ## 2 1 4.20 5.34 6.27 ## 3 1 7.51 5.34 6.27 ## 4 2 8.99 8.27 8.32 ## 5 2 10.2 8.27 8.32 ## 6 2 11.3 8.27 8.32 ## 7 3 7.36 10.8 10.4 ## 8 3 10.5 10.8 10.4 ## 9 3 10.5 10.8 10.4 ## 10 4 11.9 12.8 12.4 ## # ... with 20 more rows #now could add a spread_residuals() without it breaking... sim1 %&gt;% gather_predictions(sim1_mod, sim1_mod_loess) %&gt;% ggplot()+ geom_point(aes(x = x, y = y))+ geom_line(aes(x = x, y = pred))+ facet_wrap(~model) tidy grid_space Below is a pseudo-tidy way of creating the grid_space var from above, it actually took more effort to create this probably, so didn’t use. However you could imagine if you had to search across A LOT of values it could be worth doing something similar to this (note caret or tidymodels are resources for effectively building search spaces for hyper paramter tuning and other related modeling activities). funs_names &lt;- tibble(funs = c(rep(&quot;min&quot;, 2), rep(&quot;max&quot;, 2)), coord = rep(c(&quot;x&quot;, &quot;y&quot;), 2), field_names = str_c(funs, &quot;_&quot;, coord)) grid_space &lt;- models_error %&gt;% filter(rank_rmse &lt; 15) %&gt;% select(a1, a2) %&gt;% rep(2) %&gt;% invoke_map(.f = funs_names$funs, .x = .) %&gt;% set_names(funs_names$field_names) %&gt;% as_tibble() grid_space ## # A tibble: 1 x 4 ## min_x min_y max_x max_y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -7.10 0.319 14.9 3.89 23.4.5.4 Rather than geom_density or geom_freqpoly let’s look at histogram with values overlaid rather than stacked. sim4 %&gt;% gather_residuals(mod1, mod2) %&gt;% ggplot(aes(x = resid, y = ..density.., fill = model))+ geom_histogram(position = &quot;identity&quot;, alpha = 0.3) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. expand.grid is very similar to data_grid↩ Would be nice perhaps if this spit out a message or warning…↩ "],
["24-model-building.html", "Ch. 24: Model building 24.2: Why are low quality diamonds more expensive? 24.3 What affects the number of daily flights? Appendix", " Ch. 24: Model building Key questions: 24.2.3. #1, 2, 4 Functions and notes: data_grid, argument: .model: if the model needs variables that haven’t been supplied explicitly, will auto-fill them with “typical” values; continuous –&gt; median; categorical –&gt; mode MASS:rlm robust linear model that uses “M estimation by default” warning: MASS::select will clash with dplyr::select, so I usually won’t load in MASS explicitly 24.2: Why are low quality diamonds more expensive? # model for only small diamonds diamonds2 &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% mutate(lprice = log2(price), lcarat = log2(carat)) mod_diamond &lt;- lm(lprice ~ lcarat, data = diamonds2) mod_diamond2 &lt;- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2) diamonds2 &lt;- diamonds2 %&gt;% add_residuals(mod_diamond2, &quot;resid_lg&quot;) 24.2.3 In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent? plot_lc_lp &lt;- diamonds2 %&gt;% ggplot(aes(lcarat, lprice))+ geom_hex(show.legend = FALSE) plot_lp_lc &lt;- diamonds2 %&gt;% ggplot(aes(lprice, lcarat))+ geom_hex(show.legend = FALSE) plot_lp &lt;- diamonds2 %&gt;% ggplot(aes(lprice))+ geom_histogram(binwidth = .1) plot_lc &lt;- diamonds2 %&gt;% ggplot(aes(lcarat))+ geom_histogram(binwidth = 0.1) gridExtra::grid.arrange(plot_lc_lp, plot_lc, plot_lp + coord_flip()) The vertical bands correspond with clumps of carat_lg values falling across a range of price_lg values Histogram of carat values: diamonds2 %&gt;% ggplot(aes(carat))+ geom_histogram(binwidth = 0.01)+ scale_x_continuous(breaks = seq(0, 2, 0.1)) The chart above shows spikes in carat values at 0.3, 0.4, 0.41, 0.5, 0.7, 0.9, 1.0, 1.01, 1.2, 1.5, 1.7 and 2.0, each distribution spikes at that value and then decreases until hitting the next spike This suggests there is a preference for round numbers ending on tenths It’s curious why you don’t see really see spikes at 0.6, 0.8, 0.9, 1.1, 1.3, 1.4, 1.6, 1.8, 1.9, it suggests either there is something special about those paricular values – perhaps diamonds just tend to develop near those sizes so are more available in sizes of 0.7 than say 0.8 this article also found similar spikes: https://www.diamdb.com/carat-weight-vs-face-up-size/ as did this: https://www.pricescope.com/wiki/diamonds/diamond-carat-weight , which use different data sets (though they do not explain the spike at 0.9 but no spike at 1.4) If log(price) = a_0 + a_1 * log(carat), what does that say about the relationship between price and carat? because we’re using a natural log it means that an a_1 percentage change in carat corresponds with an a_1 percentage increase in the price if you had used a log base 2 it has a different interpretation that can be thought of in terms of relationship of doubling Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors? extreme_vals &lt;- diamonds2 %&gt;% mutate(extreme_value = (abs(resid_lg) &gt; 1)) %&gt;% filter(extreme_value) %&gt;% add_predictions(mod_diamond2, &quot;pred_lg&quot;) %&gt;% mutate(price_pred = 2^(pred_lg)) #graph extreme points as well as line of pred diamonds2 %&gt;% add_predictions(mod_diamond2) %&gt;% # mutate(extreme_value = (abs(resid_lg) &gt; 1)) %&gt;% # filter(!extreme_value) %&gt;% ggplot(aes(carat, price))+ geom_hex(bins = 50)+ geom_point(aes(carat, price), data = extreme_vals, color = &quot;orange&quot;) It’s possible some of these these were mislabeled or errors, e.g. an error in typing e.g. 200 miswritten as 2000, though given the wide range in pricing this does not seem like that extreme of a variation. diamonds2 %&gt;% add_predictions(mod_diamond2) %&gt;% mutate(extreme_value = (abs(resid_lg) &gt; 1), price_pred = 2^pred) %&gt;% filter(extreme_value) %&gt;% mutate(multiple = price / price_pred) %&gt;% arrange(desc(multiple)) %&gt;% select(price, price_pred, multiple) ## # A tibble: 16 x 3 ## price price_pred multiple ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2160 314. 6.88 ## 2 1776 412. 4.31 ## 3 1186 284. 4.17 ## 4 1186 284. 4.17 ## 5 1013 264. 3.83 ## 6 2366 774. 3.05 ## 7 1715 576. 2.98 ## 8 4368 1705. 2.56 ## 9 10011 4048. 2.47 ## 10 3807 1540. 2.47 ## 11 3360 1373. 2.45 ## 12 3920 1705. 2.30 ## 13 1415 639. 2.21 ## 14 1415 639. 2.21 ## 15 1262 2644. 0.477 ## 16 10470 23622. 0.443 If the mislabeling were an issue of e.g. 200 to 2000, you would expect that some of the actual values were ~1/10th or 10x the value of the predicted value. Though none of them appear to have this issue, except for maybe the item that was priced at 2160 but has a price of 314, which is the closest error where the actual value was ~1/10th the value of the prediction Does the final model, mod_diamonds2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond? perc_unexplained &lt;- diamonds2 %&gt;% add_predictions(mod_diamond2, &quot;pred&quot;) %&gt;% mutate(pred_2 = 2^pred, mean_price = mean(price), error_deviation = (price - pred_2)^2, reg_deviation = (pred_2 - mean_price)^2, tot_deviation = (price - mean_price)^2) %&gt;% summarise(R_squared = sum(error_deviation) / sum(tot_deviation)) %&gt;% flatten_dbl() 1 - perc_unexplained ## [1] 0.9653255 ~96.5% of variance is explained by model which seems pretty solid, though is relative to each situation See 24.2.3.3 for other considerations, though even this is very incomplete. Would want to check a variety of other metrics to further evaluate trust. 24.3 What affects the number of daily flights? Some useful notes copied from this section: daily &lt;- flights %&gt;% mutate(date = make_date(year, month, day)) %&gt;% count(date) daily &lt;- daily %&gt;% mutate(month = month(date, label = TRUE)) daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) term &lt;- function(date) { cut(date, breaks = ymd(20130101, 20130605, 20130825, 20140101), labels = c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;) ) } daily &lt;- daily %&gt;% mutate(term = term(date)) daily %&gt;% filter(wday == &quot;Sat&quot;) %&gt;% ggplot(aes(date, n, colour = term))+ geom_point(alpha = .3)+ geom_line()+ scale_x_date(NULL, date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) 24.3.5 Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year? January 20th was the day for MLK day41 May 26th was the day before Memorial Day weekend September 1st was the day before labor day Based on the above, it seems a variable representing “holiday” or “holiday weekend” may be valuable. What do the three days with high positive residuals represent? How would these days generalise to another year? daily &lt;- flights %&gt;% mutate(date = make_date(year, month, day)) %&gt;% count(date) daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) mod &lt;- lm(n ~ wday, data = daily) daily &lt;- daily %&gt;% add_residuals(mod) daily %&gt;% top_n(3, resid) ## # A tibble: 3 x 4 ## date n wday resid ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 2013-11-30 857 Sat 112. ## 2 2013-12-01 987 Sun 95.5 ## 3 2013-12-28 814 Sat 69.4 these days correspond with the Saturday and Sunday of Thanksgiving, as well as the Saturday after Christmas these days can fall on different days of the week each year so would vary from year to year depending on which day they fell on ideally you would include some “holiday” variable to help capture the impact of these / better generalise between years Check the absolute values daily %&gt;% top_n(3, abs(resid)) ## # A tibble: 3 x 4 ## date n wday resid ## &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 2013-11-28 634 Thu -332. ## 2 2013-11-29 661 Fri -306. ## 3 2013-12-25 719 Wed -244. The days with the greatest magnitude for residuals were on Christmast Day, Thanksgiving Day, and the day after Thanksgiving Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e. it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall. How does this model compare with the model with every combination of wday and term? term &lt;- function(date) { cut(date, breaks = ymd(20130101, 20130605, 20130825, 20140101), labels = c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;) ) } daily &lt;- daily %&gt;% mutate(term = term(date)) # example with wday_mod Example_term_with_sat &lt;- daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% lm(n ~ wday_mod, data = .) # just wday wkday &lt;- daily %&gt;% lm(n ~ wday, data = .) # wday and term, no interaction... wkday_term &lt;- daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% lm(n ~ wday + term, data = .) # wday and term, interaction wkday_term_interaction &lt;- daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% lm(n ~ wday*term, data = .) daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% gather_predictions(Example_term_with_sat, wkday, wkday_term, wkday_term_interaction) %&gt;% ggplot(aes(date, pred, colour = wday))+ geom_point()+ geom_line()+ facet_wrap(~model, ncol = 1) In the example, saturday has different predicted number of flights in the summer when just including wkday you don’t see this differentiation when including wkday and term you see differentiation in the summer, though this difference is the same across all wdays, hence the increased number for Saturday’s is less than it shows-up as as compared to either the example (where the term is only interacted with for Saturday) or the wkday_term_interaction chart where the interaciton is allowed for each day of the week you see increases in flights across pretty much all wdays in summer, though you see the biggest difference in Saturday42 Residuals of these models daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% gather_residuals(Example_term_with_sat, wkday, wkday_term, wkday_term_interaction) %&gt;% ggplot(aes(date, resid, colour = wday))+ geom_point()+ geom_line()+ facet_wrap(~model, ncol = 1) The graphs with saturday term and interaction across terms do not show gross changes in residuals varying by season the way the models that included just weekday or weekday and term without an interaction do. note that you have a few days with large negative residuals43 these likely correspond with holidays Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like? Create dataset of federal holidays # holiday&#39;s that could have been added: Easter, black friday # consider adding a filter to remove Columbus day and perhaps Veteran&#39;s day holidays &lt;- tribble( ~HolidayName, ~HolidayDate, &quot;New Year&#39;s&quot;, &quot;2013-01-01&quot;, &quot;MLK&quot;, &quot;2013-01-21&quot;, &quot;President&#39;s Day&quot;, &quot;2013-02-18&quot;, &quot;Memorial Day&quot;, &quot;2013-05-27&quot;, &quot;Independene Day&quot;, &quot;2013-07-04&quot;, &quot;Labor Day&quot;, &quot;2013-09-02&quot;, &quot;Columbus Day&quot;, &quot;2013-10-14&quot;, &quot;Veteran&#39;s Day&quot;, &quot;2013-11-11&quot;, &quot;Thanksgiving&quot;, &quot;2013-11-28&quot;, &quot;Christmas Day&quot;, &quot;2013-12-25&quot; ) %&gt;% mutate(HolidayDate = ymd(HolidayDate)) Create model with Holiday variable Example_term_with_sat_holiday &lt;- daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% left_join(holidays, by = c(&quot;date&quot; = &quot;HolidayDate&quot;)) %&gt;% mutate(Holiday = !is.na(HolidayName)) %&gt;% lm(n ~ wday_mod + Holiday, data = .) Look at residuals of model daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% left_join(holidays, by = c(&quot;date&quot; = &quot;HolidayDate&quot;)) %&gt;% mutate(Holiday = !is.na(HolidayName)) %&gt;% gather_residuals(Example_term_with_sat_holiday, Example_term_with_sat) %&gt;% ggplot(aes(date, resid, colour = wday))+ geom_point()+ geom_line()+ facet_wrap(~model, ncol = 1) Notice the residuals for day’s like July 4th and Christas are closer to 0 now, though residuals for smaller holidays like MLK, President’s, Columbus, and Veteran’s Day are now positive when before they did not have such noticable abberations Suggests that just “holiday” is not enough to capture the relationship In 24.3.5.4 I show how to create a “near holiday” variable (though I do not add any new analysis after creating this) What happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful? Create model week_month &lt;- daily %&gt;% mutate(month = month(date) %&gt;% as.factor()) %&gt;% lm(n ~ wday * month, data = .) Graph predictions (with n ~ wday * term as the comparison) daily %&gt;% mutate(month = month(date) %&gt;% as.factor()) %&gt;% gather_predictions(wkday_term_interaction, week_month) %&gt;% ggplot(aes(date, pred, colour = wday))+ geom_point()+ geom_line()+ facet_wrap(~model, ncol = 1) This model has the most flexibility / inputs, though this makes the pattern harder to follow / interpret Certain decreases in the month to month model are difficult to explain, for example the decrease in the month of May Graph residuals (with n ~ wday * term as the comparison) daily %&gt;% mutate(month = month(date) %&gt;% as.factor()) %&gt;% gather_residuals(wkday_term_interaction, week_month) %&gt;% ggplot(aes(date, resid, colour = wday))+ geom_point()+ geom_line()+ facet_wrap(~model, ncol = 1) The residuals seem to partially explain some of these inexplicable ups / downs: For the model that incorporates an interaciton with month, you see the residuals in months with a holiday tend to cause the associated day of the week the holiday fell on to then have high residuals on the non-holiday days, an effect thta is less pronounced on the models interacted with term44 The reason for this is that for the monthly variables there are only 4-5 week days in each month, so a holiday on one of these can substantially impact the expected number of flights on the weekend in that month (i.e. the prediction is based just on 4-5 observations). For the term interaction you have more like 12 observations to get an expected value, so while there is still an aberration on that day, the other days predictions are less affected What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective? I would expect to see a similar overall pattern, but with more smoothed affects. Let’s check what these actually look like below. wkday_term_ns &lt;- daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% lm(n ~ wday + splines::ns(date, 5), data = .) wkday_term_interaction_ns &lt;- lm(n ~ wday * splines::ns(date, 5), data = daily) Look at predictions (light grey are actuals) daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% gather_predictions(wkday_term_ns, wkday_term_interaction_ns) %&gt;% ggplot(aes(date, pred, colour = wday))+ geom_point()+ geom_line(aes(x = date, y = n, group = wday), colour = &quot;grey&quot;, alpha = 0.5)+ geom_line()+ facet_wrap(~model, ncol = 1) Look at residuals (in light grey are actuals) daily %&gt;% mutate(wday_mod = ifelse(wday == &quot;Sat&quot;, paste(wday, &quot;_&quot;, term), wday)) %&gt;% gather_residuals(wkday_term_ns, wkday_term_interaction_ns) %&gt;% ggplot(aes(date, resid, colour = wday))+ geom_point()+ geom_line(aes(x = date, y = n, group = wday), colour = &quot;grey&quot;, alpha = 0.5)+ geom_line()+ facet_wrap(~model, ncol = 1) We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away. flights %&gt;% mutate(date = lubridate::make_date(year, month, day), wday = wday(date, label = TRUE)) %&gt;% select(date, wday, distance) %&gt;% filter(distance &lt; 3000) %&gt;% ggplot(aes(wday, distance))+ geom_boxplot() 25th and 75th percentiles aren’t visibly different, but median is a little higher the same is the case for Saturday travel which does not seem to fit into this hypothesis as neatly. The effect seems more general to the weekend than just Saturday, and there seem like there may be other potential explanations than “business travel” It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday. wday_modified &lt;- function(date){ date_order &lt;- (wday(date) + 5) %% 7 date &lt;- wday(date, label = TRUE) %&gt;% fct_reorder(date_order) date } flights %&gt;% mutate(date = lubridate::make_date(year, month, day), wday = wday_modified(date)) %&gt;% select(date, wday, distance) %&gt;% filter(distance &lt; 3000) %&gt;% ggplot(aes(wday, distance))+ geom_boxplot() Appendix 24.2.3.3 Plots of extreme values against a sample and colored by some of the key attributes Plots of extreme values against carat, price, clarity diamonds2 %&gt;% add_predictions(mod_diamond2) %&gt;% sample_n(5000) %&gt;% ggplot(aes(carat, price))+ geom_point(aes(carat, price, colour = clarity), alpha = 0.5)+ geom_point(aes(carat, price, colour = clarity), data = extreme_vals, size = 3) Plots of extreme values against carat, price, cut diamonds2 %&gt;% add_predictions(mod_diamond2) %&gt;% sample_n(5000) %&gt;% ggplot(aes(carat, price))+ # geom_hex(bins = 50)+ geom_point(aes(carat, price, colour = cut), alpha = 0.5)+ geom_point(aes(carat, price, colour = cut), data = extreme_vals, size = 3) 24.2.3.1 Visualization with horizontal stripes and lprice as the focus # horizontal stripes gridExtra::grid.arrange(plot_lp_lc, plot_lp, plot_lc + coord_flip()) same thing, just change orientation and highlight lprice with a histogram A few other graphs from this problem diamonds2 %&gt;% ggplot(aes(price))+ geom_histogram(binwidth = 50) diamonds2 %&gt;% ggplot(aes(carat))+ geom_histogram(binwidth = 0.01) Taking the log of price seems to have a bigger impact on the shape of the geom_hex graph diamonds2 %&gt;% ggplot(aes(carat, lprice))+ geom_hex(show.legend = FALSE) diamonds2 %&gt;% ggplot(aes(lcarat, price))+ geom_hex(show.legend = FALSE) 24.3.5.4 In this section I create a marker for days that are “near a holiday” near_holidays &lt;- holidays %&gt;% # This creates a series of helper variables to create the variable &#39;Holiday_IntervalDay&#39; that represents an interval that encloses the period between the holiday and the beginning or end of the most recent weekend mutate(HolidayWday = wday(HolidayDate, label = TRUE), HolidayWknd = lubridate::round_date(HolidayDate, unit = &quot;week&quot;), HolidayFloor = lubridate::floor_date(HolidayDate, unit = &quot;week&quot;), HolidayCeiling = lubridate::ceiling_date(HolidayDate, unit = &quot;week&quot;), Holiday_IntervalDay = case_when(HolidayWknd == HolidayFloor ~ (HolidayFloor - ddays(2)), TRUE ~ HolidayCeiling)) %&gt;% mutate(Holiday_Period = interval(pmin(HolidayDate, Holiday_IntervalDay), pmax(HolidayDate, Holiday_IntervalDay))) # This returns each day and whether it occurred near a holiday near_holiday &lt;- map(near_holidays$Holiday_Period, ~(seq.Date(ymd(&quot;2013-01-01&quot;), ymd(&quot;2013-12-31&quot;), by = &quot;day&quot;) %within% .x)) %&gt;% transpose() %&gt;% map_lgl(any) %&gt;% as_tibble() %&gt;% rename(NearHoliday = value) %&gt;% mutate(date = seq.Date(ymd(&quot;2013-01-01&quot;), ymd(&quot;2013-12-31&quot;), by = &quot;day&quot;)) near_holiday ## # A tibble: 365 x 2 ## NearHoliday date ## &lt;lgl&gt; &lt;date&gt; ## 1 TRUE 2013-01-01 ## 2 FALSE 2013-01-02 ## 3 FALSE 2013-01-03 ## 4 FALSE 2013-01-04 ## 5 FALSE 2013-01-05 ## 6 FALSE 2013-01-06 ## 7 FALSE 2013-01-07 ## 8 FALSE 2013-01-08 ## 9 FALSE 2013-01-09 ## 10 FALSE 2013-01-10 ## # ... with 355 more rows I ended-up not adding any additional analysis here, though the methodology for creating the “near holiday” seemed worth saving Could come back to add more in the future it was also the 2nd inauguration for Obama↩ Interactions facilitate encoding these types of conditional relationships, i.e. the impact of summer depends on the day of the week (/ vice versa)↩ Remember this corresponds with days where the predictions are higher than the actuals.↩ Balancing the right level of complexity of a model is generally referred to as the “bias variance tradeoff”.↩ "],
["25-many-models.html", "Ch. 25: Many models 25.2: gapminder 25.4: Creating list-columns 25.5: Simplifying list-columns Appendix", " Ch. 25: Many models Key questions: 25.2.5 #1, 2 Functions and notes: nest creates a list-column with default key value data. Each row value becomes a dataframe with all non-grouping columns and all rows corresponding with a particular group iris %&gt;% group_by(Species) %&gt;% nest() unnest unnest any list-column in your dataframe. Notes on unnest behavior: if the atomic components of the elements of the list column are length &gt; 1, the non-nested row columns will be duplicated when the list-column is unnested # atomic components of elements of list-col == 3 --&gt; (will see duplicates of `x`) tibble(x = 1:100) %&gt;% mutate(test1 = list(tibble(a = c(1, 2, 3)))) %&gt;% unnest(test1) # atomic components of elements of list-col == 1 --&gt; (will not see duplicates of `x`) tibble(x = 1:100) %&gt;% mutate(test1 = list(tibble(a = 1, b = 2))) %&gt;% unnest(test1) if there are multiple list-cols, specify the column to unnest or default behavior will be to unnest all when unnesting a single column but multiple list-cols exist, the default behavior is to drop the other list columns. To override this use .drop = FALSE.45 tibble(x = 1:100) %&gt;% mutate(test1 = list(c(1, 2)), test2 = list(c(3, 4))) %&gt;% unnest(test1, .drop = FALSE) # change to default, i.e. `.drop = TRUE` to drop `test2` column when unnesting multiple columns, all must be the same length or you will get an error, e.g. below fails: tibble(x = 1:100) %&gt;% mutate(test1 = list(c(1)), test2 = list(c(2,3))) %&gt;% unnest() ## Error: All nested columns must have the same number of elements. # # to successfully unnest this could have added another unnest, e.g.: # tibble(x = 1:100) %&gt;% # mutate(test1 = list(c(1)), # test2 = list(c(2,3))) %&gt;% # unnest(test1) %&gt;% # unnest(test2) Method for nesting individual vectors: group_by() %&gt;% summarise(), e.g.: iris %&gt;% group_by(Species) %&gt;% summarise_all(list) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 setosa &lt;dbl [50]&gt; &lt;dbl [50]&gt; &lt;dbl [50]&gt; &lt;dbl [50]&gt; ## 2 versicolor &lt;dbl [50]&gt; &lt;dbl [50]&gt; &lt;dbl [50]&gt; &lt;dbl [50]&gt; ## 3 virginica &lt;dbl [50]&gt; &lt;dbl [50]&gt; &lt;dbl [50]&gt; &lt;dbl [50]&gt; the above has the advantage of producing atomic vectors rather than dataframes as the types inside of the lists broom::glance takes a model as input and outputs a one row tibble with columns for each of several model evalation statistics (note that these metrics are geared towards evaluating the training) broom::tidy creates a tibble with columns term, estimate, std.error, statistic (t-statistic) and p.value. A new row is created for each term type, e.g. intercept, x1, x2, etc. ggtitle(), alternative to labs(title = \"type title here\") see 25.4.5 number 3 for a useful way of wrapping certain functions in list functions to take advantage of the list-col format 25.2: gapminder The set-up example Hadley goes through is important, below is a slightly altered copy of his example. Nested Data by_country &lt;- gapminder::gapminder %&gt;% group_by(country, continent) %&gt;% nest() List-columns country_model &lt;- function(df) { lm(lifeExp ~ year, data = df) } Want to apply this function over every data frame, the dataframes are in a list, so do this by: by_country2 &lt;- by_country %&gt;% mutate(model = purrr::map(data, country_model)) Advantage with keeping things in the dataframe is that when you filter, or move things around, everything stays in sync, as do new summary values you might add. by_country2 %&gt;% arrange(continent, country) ## # A tibble: 142 x 4 ## country continent data model ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 Algeria Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 2 Angola Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 3 Benin Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 4 Botswana Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 5 Burkina Faso Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 6 Burundi Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 7 Cameroon Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 8 Central African Republic Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 9 Chad Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## 10 Comoros Africa &lt;tibble [12 x 4]&gt; &lt;S3: lm&gt; ## # ... with 132 more rows by_country2 %&gt;% mutate(summaries = purrr::map(model, summary)) %&gt;% mutate(r_squared = purrr::map2_dbl(model, data, rsquare)) ## # A tibble: 142 x 6 ## country continent data model summaries r_squared ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Afghanistan Asia &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.948 ## 2 Albania Europe &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.911 ## 3 Algeria Africa &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.985 ## 4 Angola Africa &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.888 ## 5 Argentina Americas &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.996 ## 6 Australia Oceania &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.980 ## 7 Austria Europe &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.992 ## 8 Bahrain Asia &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.967 ## 9 Bangladesh Asia &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.989 ## 10 Belgium Europe &lt;tibble [12 x 4~ &lt;S3: l~ &lt;S3: summary.l~ 0.995 ## # ... with 132 more rows unnesting, another dataframe with the residuals included and then unnest by_country3 &lt;- by_country2 %&gt;% mutate(resids = purrr::map2(data, model, add_residuals)) resids &lt;- by_country3 %&gt;% unnest(resids) resids ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap resid ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. -1.11 ## 2 Afghanistan Asia 1957 30.3 9240934 821. -0.952 ## 3 Afghanistan Asia 1962 32.0 10267083 853. -0.664 ## 4 Afghanistan Asia 1967 34.0 11537966 836. -0.0172 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 0.674 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 1.65 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 1.69 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 1.28 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 0.754 ## 10 Afghanistan Asia 1997 41.8 22227415 635. -0.534 ## # ... with 1,694 more rows 25.2.5 A linear trend seems to be slightly too simple for the overall trend. Can you do better with a quadratic polynomial? How can you interpret the coefficients of the quadratic? (Hint you might want to transform year so that it has mean zero.) Create functions # funciton to center value center_value &lt;- function(df){ df %&gt;% mutate(year_cent = year - mean(year)) } # this function allows me to input any text to &quot;var&quot; to customize the inputs # to the model, default are a linear and quadratic term for year (centered) lm_quad_2 &lt;- function(df, var = &quot;year_cent + I(year_cent^2)&quot;){ lm(as.formula(paste(&quot;lifeExp ~ &quot;, var)), data = df) } Create dataframe with evaluation metrics by_country3_quad &lt;- by_country3 %&gt;% mutate( # create centered data data_cent = purrr::map(data, center_value), # create quadratic models mod_quad = purrr::map(data_cent, lm_quad_2), # get model evaluation stats from original model glance_mod = purrr::map(model, broom::glance), # get model evaluation stats from quadratic model glance_quad = purrr::map(mod_quad, broom::glance)) Create plots by_country3_quad %&gt;% unnest(glance_mod, glance_quad, .sep = &quot;_&quot;, .drop = TRUE) %&gt;% gather(glance_mod_r.squared, glance_quad_r.squared, key = &quot;order&quot;, value = &quot;r.squared&quot;) %&gt;% ggplot(aes(x = continent, y = r.squared, colour = continent)) + geom_boxplot() + facet_wrap(~order) The quadratic trend seems to do better –&gt; indicated by the distribution of the R^2 values being closer to one. The level of improvement seems especially pronounced for African countries. Let’s check this closer by looking at percentage point improvement in R^2 in chart below by_country3_quad %&gt;% mutate(quad_coefs = map(mod_quad, broom::tidy)) %&gt;% unnest(glance_mod, .sep = &quot;_&quot;) %&gt;% unnest(glance_quad) %&gt;% mutate(bad_fit = glance_mod_r.squared &lt; 0.25, R.squ_ppt_increase = r.squared - glance_mod_r.squared) %&gt;% ggplot(aes(x = continent, y = R.squ_ppt_increase))+ # geom_quasirandom(aes(alpha = bad_fit), colour = &quot;black&quot;)+ geom_boxplot(alpha = 0.1, colour = &quot;dark grey&quot;)+ geom_quasirandom(aes(colour = continent))+ labs(title = &quot;Percentage point (PPT) improvement in R squared value&quot;, subtitle = &quot;(When adding a quadratic term to the linear regression model)&quot;) View predictions from linear model with quadratic term (of countries where linear trend did not capture relationship) bad_fit &lt;- by_country3 %&gt;% mutate(glance = purrr::map(model, broom::glance)) %&gt;% unnest(glance, .drop = TRUE) %&gt;% filter(r.squared &lt; 0.25) #solve with join with bad_fit by_country3_quad %&gt;% semi_join(bad_fit, by = &quot;country&quot;) %&gt;% mutate(data_preds = purrr::map2(data_cent, mod_quad, add_predictions)) %&gt;% unnest(data_preds) %&gt;% ggplot(aes(x = year, group = country))+ geom_point(aes(y = lifeExp, colour = country))+ geom_line(aes(y = pred, colour = country))+ facet_wrap(~country)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) while the quadratic model does a better job fitting the model than a linear term does, I wouldn’t say it does a good job of fitting the model it looks like the trends are generally consistent rates of improvement and then there is a sudden drop-off associated with some event, hence an intervention variable may be a more appropriate method for modeling this pattern Quadratic model parameters by_country3_quad %&gt;% mutate(quad_coefs = map(mod_quad, broom::tidy)) %&gt;% unnest(glance_mod, .sep = &quot;_&quot;) %&gt;% unnest(glance_quad) %&gt;% unnest(quad_coefs) %&gt;% mutate(bad_fit = glance_mod_r.squared &lt; 0.25) %&gt;% ggplot(aes(x = continent, y = estimate, alpha = bad_fit))+ geom_boxplot(alpha = 0.1, colour = &quot;dark grey&quot;)+ geom_quasirandom(aes(colour = continent))+ facet_wrap(~term, scales = &quot;free&quot;)+ labs(caption = &quot;Note that &#39;bad fit&#39; represents a bad fit on the initial model \\nthat did not contain a quadratic term)&quot;)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) ## Warning: Using alpha for a discrete variable is not advised. The quadratic term (in a linear function, trained with the x-value centered at the mean, as in this dataset) has a few important notes related to interpretation If the coefficient is positive the output will be convex, if it is negative it will be concave (i.e. smile vs. frown shape) The value on the coefficient represents 1/2 the rate at which the relationship between lifeExp and year is changing for every one unit change from the mean / expected value of lifeExp in the dataset. Hence if the coefficient is near 0, that means the relationship between lifeExp and year does not change (or at least does not change at a constant rate) when moving in either direction from lifeExps mean value. To better understand this, let’s look look at a specific example. Excluding Rwanda, Botswana was the country that the linear model without the quadratic term performed the worst on. We’ll use this as our example for interpreting the coefficients. Plots of predicted and actual values for Botswanian life expectancy by year by_country3_quad %&gt;% filter(country == &quot;Botswana&quot;) %&gt;% mutate(data_preds = purrr::map2(data_cent, mod_quad, add_predictions)) %&gt;% unnest(data_preds) %&gt;% ggplot(aes(x = year, group = country))+ geom_point(aes(y = lifeExp))+ geom_line(aes(y = pred, colour = &quot;prediction&quot;))+ labs(title = &quot;Data and quadratic trend of predictions for Botswana&quot;) (note that the centered value for year in the ‘centered’ dataset is 1979.5) In the model for Botswana, coefficents are: Intercept: ~ 59.81 year (centered): ~ 0.0607 year (centered)^2: ~ -0.0175 Hence for every one year we move away from the central year (1979.5), the rate of change between year and price decreases by ~0.035. Below I show this graphically by plotting the lines tangent to the models output. botswana_coefs &lt;- by_country3_quad %&gt;% filter(country == &quot;Botswana&quot;) %&gt;% with(map(mod_quad, coef)) %&gt;% flatten_dbl() Helper functions to find tangent points find_slope &lt;- function(x){ 2*botswana_coefs[[3]]*x + botswana_coefs[[2]] } find_y1 &lt;- function(x){ botswana_coefs[[3]]*(x^2) + botswana_coefs[[2]]*x + botswana_coefs[[1]] } find_intercept &lt;- function(x, y, m){ y - x*m } tangent_lines &lt;- tibble(x1 = seq(-20, 20, 10)) %&gt;% mutate(slope = find_slope(x1), y1 = find_y1(x1), intercept = find_intercept(x1, y1, slope), slope_change = x1*2*botswana_coefs[[3]]) %&gt;% select(slope, intercept, everything()) by_country3_quad %&gt;% filter(country == &quot;Botswana&quot;) %&gt;% mutate(data_preds = purrr::map2(data_cent, mod_quad, add_predictions)) %&gt;% unnest(data_preds) %&gt;% ggplot(aes(x = year_cent))+ geom_line(aes(x = year_cent, y = pred), colour = &quot;red&quot;)+ geom_abline(aes(intercept = intercept, slope = slope), data = tangent_lines)+ coord_fixed() Below is the relevant output in a table. x1: represents the change in x value from 1979.5 slope: slope of the tangent line at particular x1 value slope_diff_central: the amount the slope is different from the slope of the tangent line at the central year select(tangent_lines, x1, slope, slope_diff_central = slope_change) ## # A tibble: 5 x 3 ## x1 slope slope_diff_central ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -20 0.760 0.700 ## 2 -10 0.411 0.350 ## 3 0 0.0607 0 ## 4 10 -0.289 -0.350 ## 5 20 -0.639 -0.700 notice that for every 10 year increase in x1 we see the slope of the tangent line has decreased by 0.35. If we’d looked at just one year we would have seen the change was 0.035, this correspondig with 2 multiplied by the coefficient on the quadratic term of our model. Explore other methods for visualising the distribution of \\(R^2\\) per continent. You might want to try the ggbeeswarm package, which provides similar methods for avoiding overlaps as jitter, but uses deterministic methods. visualisations of linear model by_country3_quad %&gt;% unnest(glance_mod) %&gt;% ggplot(aes(x = continent, y = r.squared, colour = continent))+ geom_boxplot(alpha = 0.1, colour = &quot;dark grey&quot;)+ ggbeeswarm::geom_quasirandom() I like geom_quasirandom() the best as an overlay on boxplot, it keeps things centered and doesn’t have the gravitational pull affect that makes geom_beeswarm() become a little misaligned, it also works well here over geom_jitter() as the points stay better around their true value To create the last plot (showing the data for the countries with the worst model fits), we needed two steps: we created a data frame with one row per country and then semi-joined it to the original dataset. It’s possible to avoid this join if we use unnest() instead of unnest(.drop = TRUE). How? #first filter by r.squared and then unnest by_country3_quad %&gt;% mutate(data_preds = purrr::map2(data_cent, mod_quad, add_predictions)) %&gt;% unnest(glance_mod) %&gt;% mutate(bad_fit = r.squared &lt; 0.25) %&gt;% filter(bad_fit) %&gt;% unnest(data_preds) %&gt;% ggplot(aes(x = year, group = country))+ geom_point(aes(y = lifeExp, colour = country))+ geom_line(aes(y = pred, colour = country))+ facet_wrap(~country)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) 25.4: Creating list-columns 25.4.5 List all the functions that you can think of that take an atomic vector and return a list. stringr::str_extract_all + other stringr functions (however the below can also take types that are not atomic and are probably not really what is being looked for) list tibble map / lapply Brainstorm useful summary functions that, like quantile(), return multiple values. summary range … What’s missing in the following data frame? How does quantile() return that missing piece? Why isn’t that helpful here? mtcars %&gt;% group_by(cyl) %&gt;% summarise(q = list(quantile(mpg))) %&gt;% unnest() ## # A tibble: 15 x 2 ## cyl q ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 21.4 ## 2 4 22.8 ## 3 4 26 ## 4 4 30.4 ## 5 4 33.9 ## 6 6 17.8 ## 7 6 18.6 ## 8 6 19.7 ## 9 6 21 ## 10 6 21.4 ## 11 8 10.4 ## 12 8 14.4 ## 13 8 15.2 ## 14 8 16.2 ## 15 8 19.2 need to capture probabilities of quantiles to make useful… probs &lt;- c(0.01, 0.25, 0.5, 0.75, 0.99) mtcars %&gt;% group_by(cyl) %&gt;% summarise(p = list(probs), q = list(quantile(mpg, probs))) %&gt;% unnest() ## # A tibble: 15 x 3 ## cyl p q ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0.01 21.4 ## 2 4 0.25 22.8 ## 3 4 0.5 26 ## 4 4 0.75 30.4 ## 5 4 0.99 33.8 ## 6 6 0.01 17.8 ## 7 6 0.25 18.6 ## 8 6 0.5 19.7 ## 9 6 0.75 21 ## 10 6 0.99 21.4 ## 11 8 0.01 10.4 ## 12 8 0.25 14.4 ## 13 8 0.5 15.2 ## 14 8 0.75 16.2 ## 15 8 0.99 19.1 see list(quantile()) examples for related method that captures names of quantiles (rather than requiring th user to manually input a vector of probabilities) What does this code do? Why might it be useful? mtcars %&gt;% select(1:3) %&gt;% group_by(cyl) %&gt;% summarise_all(funs(list)) It turns each row into an atomic vector grouped by the particular cyl value. It is different from nest in that each column creates a new list-column representing an atomic vector. If nest had been used, this would have created a single dataframe that all the values woudl have been in. Could be useful for running purr through particular columns… e.g. let’s say we want to find the number of unique items in each column for each grouping, we could do that like so mtcars %&gt;% group_by(cyl) %&gt;% select(1:5) %&gt;% summarise_all(funs(list)) %&gt;% mutate_all(funs(unique = map_int(., ~length(unique(.x))))) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## please use list() instead ## ## # Before: ## funs(name = f(.) ## ## # After: ## list(name = ~f(.)) ## This warning is displayed once per session. ## # A tibble: 3 x 10 ## cyl mpg disp hp drat cyl_unique mpg_unique disp_unique hp_unique ## &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 4 &lt;dbl~ &lt;dbl~ &lt;dbl~ &lt;dbl~ 1 9 11 10 ## 2 6 &lt;dbl~ &lt;dbl~ &lt;dbl~ &lt;dbl~ 1 6 5 4 ## 3 8 &lt;dbl~ &lt;dbl~ &lt;dbl~ &lt;dbl~ 1 12 11 9 ## # ... with 1 more variable: drat_unique &lt;int&gt; # we could also simply overwrite the values (rather than make new columns) mtcars %&gt;% group_by(cyl) %&gt;% select(1:5) %&gt;% summarise_all(funs(list)) %&gt;% mutate_all(funs(map_int(., ~length(unique(.x))))) ## # A tibble: 3 x 5 ## cyl mpg disp hp drat ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 9 11 10 10 ## 2 1 6 5 4 5 ## 3 1 12 11 9 11 25.5: Simplifying list-columns 25.5.3 Why might the lengths() function be useful for creating atomic vector columns from list-columns? perhaps you want to measure the number of elements (or unique elements) in an individual element of a list column mpg %&gt;% group_by(cyl) %&gt;% summarise(displ_list = list(displ)) %&gt;% mutate(num_unique = map_int(displ_list, ~unique(.x) %&gt;% length())) ## # A tibble: 4 x 3 ## cyl displ_list num_unique ## &lt;int&gt; &lt;list&gt; &lt;int&gt; ## 1 4 &lt;dbl [81]&gt; 8 ## 2 5 &lt;dbl [4]&gt; 1 ## 3 6 &lt;dbl [79]&gt; 14 ## 4 8 &lt;dbl [70]&gt; 17 List the most common types of vector found in a data frame. What makes lists different? the atomic types: char, int, double, factor, date are all more common, they are atomic, whereas lists are not atomic vectors and can contain any type of data within them (e.g. a list of atomic vectors, list of lists, etc.). Appendix Models in lists This is the more traditional way you might store models in a list models_countries &lt;- purrr::map(by_country$data, country_model) names(models_countries) &lt;- by_country$country models_countries[1:3] ## $Afghanistan ## ## Call: ## lm(formula = lifeExp ~ year, data = df) ## ## Coefficients: ## (Intercept) year ## -507.5343 0.2753 ## ## ## $Albania ## ## Call: ## lm(formula = lifeExp ~ year, data = df) ## ## Coefficients: ## (Intercept) year ## -594.0725 0.3347 ## ## ## $Algeria ## ## Call: ## lm(formula = lifeExp ~ year, data = df) ## ## Coefficients: ## (Intercept) year ## -1067.8590 0.5693 List-columns for sampling say you want to sample all the flights on 50 days out of the year. List-cols can be used to generate a sample like this: flights %&gt;% mutate(create_date = make_date(year, month, day)) %&gt;% select(create_date, 5:8) %&gt;% group_by(create_date) %&gt;% nest() %&gt;% sample_n(50) %&gt;% unnest() ## # A tibble: 45,640 x 5 ## create_date sched_dep_time dep_delay arr_time sched_arr_time ## &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013-02-09 900 1 1242 1227 ## 2 2013-02-09 1130 30 1434 1430 ## 3 2013-02-09 900 186 1814 1540 ## 4 2013-02-09 1220 2 1545 1532 ## 5 2013-02-09 1240 -3 1414 1444 ## 6 2013-02-09 1245 -5 1528 1600 ## 7 2013-02-09 1250 0 1526 1550 ## 8 2013-02-09 1259 -4 1535 1555 ## 9 2013-02-09 1300 -2 1540 1605 ## 10 2013-02-09 1300 3 1626 1608 ## # ... with 45,630 more rows Alternatively you could use a semi_join(), e.g. flights_samp &lt;- flights %&gt;% mutate(create_date = make_date(year, month, day)) %&gt;% distinct(create_date) %&gt;% sample_n(50) flights %&gt;% mutate(create_date = make_date(year, month, day)) %&gt;% select(create_date, 5:8) %&gt;% semi_join(flights_samp, by = &quot;create_date&quot;) ## # A tibble: 46,640 x 5 ## create_date sched_dep_time dep_delay arr_time sched_arr_time ## &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013-01-15 500 -7 645 648 ## 2 2013-01-15 525 -7 825 820 ## 3 2013-01-15 530 3 839 831 ## 4 2013-01-15 540 -6 829 850 ## 5 2013-01-15 540 -5 1014 1017 ## 6 2013-01-15 600 -17 710 715 ## 7 2013-01-15 600 -11 637 709 ## 8 2013-01-15 600 -8 934 910 ## 9 2013-01-15 600 -8 658 658 ## 10 2013-01-15 600 -7 851 859 ## # ... with 46,630 more rows In some situations I find the nest, unnest method more elegant though the semi_join method seems to run goes faster on large dataframes There are also other more specialized functions in the tidyverse to help with various sampling strategies 25.2.5.1 Include cubic term Let’s look at this example if we had allowed year to be a 3rd order polynomial. We’re really stretching our degrees of freedom (in relation to our number of observations) in this case – these might be less likely to generalize to other data well. by_country3 %&gt;% semi_join(bad_fit, by = &quot;country&quot;) %&gt;% mutate( # create centered data data_cent = purrr::map(data, center_value), # create cubic (3rd order) data mod_cubic = purrr::map(data_cent, lm_quad_2, var = &quot;year_cent + I(year_cent^2) + I(year_cent^3)&quot;), # get predictions for 3rd order model data_cubic = purrr::map2(data_cent, mod_cubic, add_predictions)) %&gt;% unnest(data_cubic) %&gt;% ggplot(aes(x = year, group = country))+ geom_point(aes(y = lifeExp, colour = country))+ geom_line(aes(y = pred, colour = country))+ facet_wrap(~country)+ theme(axis.text.x = element_text(angle = 90, hjust = 1)) interpretibility of coefficients beyond quadratic term becomes less strait forward to explain Multiple graphs in chunk There are a variety of ways to have multiple graphs outputted and aligned side by side: build graphs separately and use gridExtra::grid.arrange() Ensure metrics have been gathered into a single column and then use facet_wrap()/facet_grid() (ggforce is a helpful extension package to ggplot2 that gives more functionality to these faceting functions) manipulate chunk options, e.g. figures below have the following options set in the R code chunk: out.width = \"33%\", fig.asp = 1, fig.width = 3, fig.show='hold',, fig.align='default' nz &lt;- filter(gapminder, country == &quot;New Zealand&quot;) nz %&gt;% ggplot(aes(year, lifeExp)) + geom_line() + ggtitle(&quot;Full data = &quot;) nz_mod &lt;- lm(lifeExp ~ year, data = nz) nz %&gt;% add_predictions(nz_mod) %&gt;% ggplot(aes(year, pred)) + geom_line() + ggtitle(&quot;Linear trend + &quot;) nz %&gt;% add_residuals(nz_mod) %&gt;% ggplot(aes(year, resid)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 3) + geom_line() + ggtitle(&quot;Remaining pattern&quot;) list(quantile()) examples Some of these examples may not represent best practices. prob_vals &lt;- c(0, .25, .5, .75, 1) iris %&gt;% group_by(Species) %&gt;% summarise(Petal.Length_q = list(quantile(Petal.Length))) %&gt;% mutate(probs = list(prob_vals)) %&gt;% unnest() ## # A tibble: 15 x 3 ## Species Petal.Length_q probs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1 0 ## 2 setosa 1.4 0.25 ## 3 setosa 1.5 0.5 ## 4 setosa 1.58 0.75 ## 5 setosa 1.9 1 ## 6 versicolor 3 0 ## 7 versicolor 4 0.25 ## 8 versicolor 4.35 0.5 ## 9 versicolor 4.6 0.75 ## 10 versicolor 5.1 1 ## 11 virginica 4.5 0 ## 12 virginica 5.1 0.25 ## 13 virginica 5.55 0.5 ## 14 virginica 5.88 0.75 ## 15 virginica 6.9 1 Example for using quantile across range of columns Also notice dynamic method for extracting names iris %&gt;% group_by(Species) %&gt;% summarise_all(funs(list(quantile(., probs = prob_vals)))) %&gt;% mutate(probs = map(Petal.Length, names)) %&gt;% unnest() ## # A tibble: 15 x 6 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width probs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 setosa 4.3 2.3 1 0.1 0% ## 2 setosa 4.8 3.2 1.4 0.2 25% ## 3 setosa 5 3.4 1.5 0.2 50% ## 4 setosa 5.2 3.68 1.58 0.3 75% ## 5 setosa 5.8 4.4 1.9 0.6 100% ## 6 versicolor 4.9 2 3 1 0% ## 7 versicolor 5.6 2.52 4 1.2 25% ## 8 versicolor 5.9 2.8 4.35 1.3 50% ## 9 versicolor 6.3 3 4.6 1.5 75% ## 10 versicolor 7 3.4 5.1 1.8 100% ## 11 virginica 4.9 2.2 4.5 1.4 0% ## 12 virginica 6.22 2.8 5.1 1.8 25% ## 13 virginica 6.5 3 5.55 2 50% ## 14 virginica 6.9 3.18 5.88 2.3 75% ## 15 virginica 7.9 3.8 6.9 2.5 100% Extracting names Maybe not best practice: quantile(1:100) %&gt;% as.data.frame() %&gt;% rownames_to_column() ## rowname . ## 1 0% 1.00 ## 2 25% 25.75 ## 3 50% 50.50 ## 4 75% 75.25 ## 5 100% 100.00 Better would be to use enframe() here: quantile(1:100) %&gt;% tibble::enframe() ## # A tibble: 5 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 0% 1 ## 2 25% 25.8 ## 3 50% 50.5 ## 4 75% 75.2 ## 5 100% 100 invoke_map example (book) I liked Hadley’s example with invoke_map and wanted to save it: sim &lt;- tribble( ~f, ~params, &quot;runif&quot;, list(min = -1, max = -1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sims = invoke_map(f, params, n = 10)) ## # A tibble: 3 x 3 ## f params sims ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 runif &lt;list [2]&gt; &lt;dbl [10]&gt; ## 2 rnorm &lt;list [1]&gt; &lt;dbl [10]&gt; ## 3 rpois &lt;list [1]&gt; &lt;int [10]&gt; named list example (book) I liked Hadley’s example where you have a list of named vectors that you need to iterate over both the values as well as the names and the use of enframe to facilitate this. Below is the copied example and notes: x &lt;- list( a = 1:5, b = 3:4, c = 5:6 ) df &lt;- enframe(x) df ## # A tibble: 3 x 2 ## name value ## &lt;chr&gt; &lt;list&gt; ## 1 a &lt;int [5]&gt; ## 2 b &lt;int [2]&gt; ## 3 c &lt;int [2]&gt; The advantage of this structure is that it generalises in a straightforward way - names are useful if you have character vector of metadata, but don’t help if you have other types of data, or multiple vectors. Now if you want to iterate over names and values in parallel, you can use map2(): df %&gt;% mutate( smry = map2_chr(name, value, ~ stringr::str_c(.x, &quot;: &quot;, .y[1])) ) ## # A tibble: 3 x 3 ## name value smry ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; ## 1 a &lt;int [5]&gt; a: 1 ## 2 b &lt;int [2]&gt; b: 3 ## 3 c &lt;int [2]&gt; c: 5 Make sure the following packages are installed: Note that if using .drop = FALSE in the latter case that you are creating replicated rows for list-col values↩ "],
["27-r-markdown.html", "Ch. 27: R Markdown 27.2 R Markdown basics 27.3: Text formatting with Markdown 27.4: Code chunks", " Ch. 27: R Markdown Functions and notes: shortcut for inserting code chunk is cmd/ctrl+alt+i shortcut for running entire code chunks: cmd/ctrl+shift+enter chunk options chunk name is first part after type of code in chunk, e.g. code chunk by name: \"```{r by-name}\" eval = FALSE show example output code, but don’t evaluate include = FALSE evaluate code but don’t show code or output echo = FALSE is for when you just want the output but not the code itself message = FALSE or warning = False prevents messages or warnings appearing in the finished line error = TRUE causes code to render even if there is an error results = 'hide' hides printied output and fig.show = 'hide' hides plots allows you to hide particular bits of output cache = TRUE save output of chunk to separate folder (speeds-up rendering) dependson = \"chunk_name\" update chunk if dependency changes cache.extra if output from function changes, will re-render – useful for if you only want to update if for example a file changes, e.g. rawdata &lt;- readr::read_csv(&quot;a_very_large_file.csv&quot;) good idea to name code chunks after main object created knitr::clean_cache clear out your caches knitr::opts_chunk use to change knitting options e.g. # when writing books and tutorials knitr::opts_chunk$set( comment = &quot;#&gt;&quot;, collapse = TRUE ) # hiding code for report knitr::opts_chunk$set( echo = FALSE ) # may also set `message = FALSE` and `warning = FALSE` rmarkdown::render programmatically knit documents e.g. rmarkdown::render(\"27-r-markdown.Rmd\", output_format = \"all\") to render all formats in YAML header knitr::kable to make dataframe more visible for printing when knitting also see xtable, stargazer, pander, tables, and ascii packages format helpful when inserting numbers into texts, e.g. comma &lt;- function(x) format(x, digits = 2, big.mark = &quot;,&quot;) comma(3452345) ## [1] &quot;3,452,345&quot; comma(.12358124331) ## [1] &quot;0.12&quot; Use params: in YAML header to add in specific values or create parameterized reports, e.g. params: start: !r lubridate::ymd(&quot;2015-01-01&quot;) snapshot: !r lubridate::ymd_hms(&quot;2015-01-01 12:30:00&quot;) Full chunk options here: https://yihui.name/knitr/options/ 27.2 R Markdown basics 27.2.1 Create a new notebook using File &gt; New File &gt; R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output. Done seperately. Create a new R Markdown document with File &gt; New File &gt; R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update. Done seperately. Compare and contrast the R notebook and R markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other? Both by default have code chunks display ‘in-line’ while working, though with RMD can force to not output in-line. When rendering, default of notebooks will be to render whichever chunks have been rendered during interactive session, whereas RMD document needs directions from code chunk options I generally prefer .Rmd files to notebooks.46 Create one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.) Done seperately. HTML does not have page numbers. Plots or other outputs with interactive components will often only be viewable from html (e.g. flexdashboard, plotly, …). Some input options will work across all formats, e.g. toc: true, however other options like code folding may be specific to a format, e.g. code folding will only work with html. 27.3: Text formatting with Markdown Print file from Hadley’s github page with commmon formatting: cat(readr::read_file(&quot;https://raw.githubusercontent.com/hadley/r4ds/master/rmarkdown/markdown.Rmd&quot;)) Other notes The following will actually run in the console when knitted (and not in the knitted document): summary(mpg) 27.3.1 Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. this is a weak example (see __ for better examples): Using the R Markdown quick reference, figure out how to: Add a footnote. Here is a foonote reference47 and another48 and a 3rd49 and an in-line one50 Add a horizontal rule. A linked phrase. pagebreaks above and below (AKA horizontal rules) Add a block quote. There is no spoon. -The Matrix Copy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown in to a local R markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features. It’s interesting that the count of number of diamonds spikes at whole numbers… 27.4: Code chunks 27.4.7 Add a section that explores how diamond sizes vary by cut, colour, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option. put this into a code chunk: knitr::opts_chunk$set(echo = FALSE) Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes. diamonds %&gt;% filter(min_rank(-carat) &lt;= 20) %&gt;% select(starts_with(&quot;c&quot;)) %&gt;% arrange(desc(carat)) %&gt;% knitr::kable(caption = &quot;The four C&#39;s of the 20 biggest diamonds&quot;) Table 1: The four C’s of the 20 biggest diamonds carat cut color clarity 5.01 Fair J I1 4.50 Fair J I1 4.13 Fair H I1 4.01 Premium I I1 4.01 Premium J I1 4.00 Very Good I I1 3.67 Premium I I1 3.65 Fair H I1 3.51 Premium J VS2 3.50 Ideal H I1 3.40 Fair D I1 3.24 Premium H I1 3.22 Ideal I I1 3.11 Fair J I1 3.05 Premium E I1 3.04 Very Good I SI2 3.04 Premium I SI2 3.02 Fair I I1 3.01 Premium I I1 3.01 Premium F I1 3.01 Fair H I1 3.01 Premium G SI2 3.01 Ideal J SI2 3.01 Ideal J I1 3.01 Premium I SI2 3.01 Fair I SI2 3.01 Fair I SI2 3.01 Good I SI2 3.01 Good I SI2 3.01 Good H SI2 3.01 Premium J SI2 3.01 Premium J SI2 Modify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats. diamonds %&gt;% summarise(`proportion big` = (sum(carat &gt; 2.5) / n()) %&gt;% comma()) %&gt;% knitr::kable() proportion big 0.0023 Set up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching. lubridate::now() ## [1] &quot;2019-06-05 19:31:49 EDT&quot; lubridate::now() ## [1] &quot;2019-06-05 19:31:49 EDT&quot; lubridate::now() ## [1] &quot;2019-06-05 19:31:50 EDT&quot; lubridate::now() ## [1] &quot;2019-06-05 19:31:50 EDT&quot; Make sure the following packages are installed: I’ve found some of my company’s security software sometimes acts-up when working interactively if I have my chunk output in-line (just slows down). Hence, I ‘uncheck’ Show output inline for all Rmarkdown documents from Tools–&gt;Global Options –&gt;Appearance.↩ Here is the foonote.↩ here’s one with multiple blocks. boo ya this is an awesome foonote. don’t you believe it!↩ and the third↩ Superb fourth footnote.↩ "],
["28-graphics-for-communication.html", "Ch. 28: Graphics for communication 28.2: Label 28.3: Annotations 28.4: Scales Appendix", " Ch. 28: Graphics for communication Functions and notes: labs() to add labels common args: title, subtitle, caption, x, y, colour, … for mathematical equations use quote and see ?plotmath e.g. within labs() could do y = quote(alpha + beta + frac(delta, theta)) geom_text() similar to geom_point() but with argument label that adds text where the point would be use nudge_x and nudge_y to move position around use vjust (‘top’, ‘center’, or ‘bottom’) and hjust (‘left’, ‘center’, or ‘right’) to control alignment of text can use +Inf and -Inf to put text in exact corners use stringr::str_wrap() to automatically add line breaks geom_label() is like geom_text() but draws a box around the data that makes easier to see (can adjust alpha and fill of background box) ggrepel::geom_label_repel() is like geom_label() but prevents overlap of labels geom_hline() and geom_vline for reference lines (often use size = 2 and colour = white) geom_rect() to draw rectangle around points (controlled by xmin, xmax, ymin, ymax) geom_segment() to draw attention to a point with an arrow, (common args: arrow, x, y, xend, yend) annotate can add in labels by hand (not from values of dataframe) scale_x_continuous(), scale_y_continuous(), scale_colour_discrete(), … scale_{aes}_{scale type}() breaks and labels are key args (can set labels = NULL to remove values) scale_colour_brewer(palette = \"Set1\")for color blind people scale_colour_manual() for definining colours with specific values, e.g. scale_colour_manual(values = c(Republican = \"red\", Democratic = \"blue\")) for continuous scales try scale_colour_gradient(), scale_fill_gradient(), scale_colour_gradient2() (two colour gradient, e.g. + / - values), viridis::scale_colour_viridis() date scales are a little different, e.g. scale_x_date() takes args date_labels (e.g. date_labels = \"'%y\") and date_breaks (e.g. date_breaks = \"2 days\") scale_x_log10(), scale_y_log10… to substitute values with a particular transformation theme() customize any non-data components of plots e.g. remove legend with theme(legend.position = \"none\") (could also have inputted “left”, “top”, “bottom”, or “right”) guides() to control display of individual legends – use in conjunction with guide_legend() or guide_colourbar() coord_cartesian() to zoom using xlim and ylim args can customize your themes, e.g. theme_bw(), theme_classic()…, see ggthemes for a bunch of others ggsave() defaults to save most recent plot key options: fig.width, fig.height, fig.asp, out.width, out.height (see chapter for details) other options: fig.align, fig.cap, dev (e.g. dev = \"png\") 28.2: Label 28.2.1 Create one plot on the fuel economy data with customised title, subtitle, caption, x, y, and colour labels. mpg %&gt;% ggplot(aes(x = hwy, displ))+ geom_count(aes(colour = class))+ labs(title = &quot;Larger displacement has lower gas mileage efficiency&quot;, subtitle = &quot;SUV and pickup classes` tend to be highest on disp&quot;, caption = &quot;Data is for cars made in either 1999 or 2008&quot;, colour = &quot;Car class&quot;) The geom_smooth() is somewhat misleading because the hwy for large engines is skewed upwards due to the inclusion of lightweight sports cars with big engines. Use your modelling tools to fit and display a better model. mpg %&gt;% ggplot(aes(x = hwy, displ))+ geom_count(aes(colour = class))+ labs(title = &quot;Larger displacement has lower gas mileage efficiency&quot;, subtitle = &quot;SUV and pickup classes` tend to be highest on disp&quot;, caption = &quot;Data is for cars made in either 1999 or 2008&quot;, colour = &quot;Car class&quot;)+ geom_smooth() You could take into account the class of the car mpg %&gt;% ggplot(aes(x = hwy, displ, colour = class))+ geom_count()+ labs(title = &quot;Larger displacement has lower gas mileage efficiency&quot;, subtitle = &quot;SUV and pickup classes` tend to be highest on disp&quot;, caption = &quot;Data is for cars made in either 1999 or 2008&quot;, colour = &quot;Car class&quot;)+ geom_smooth()+ facet_wrap(~class) Take an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand. Done seperately. 28.3: Annotations 28.3.1 Use geom_text() with infinite positions to place text at the four corners of the plot. data_label &lt;- tibble(x = c(Inf, -Inf), hjust = c(&quot;right&quot;, &quot;left&quot;), y = c(Inf, -Inf), vjust = c(&quot;top&quot;, &quot;bottom&quot;)) %&gt;% expand(nesting(x, hjust), nesting(y, vjust)) %&gt;% mutate(label = glue::glue(&quot;hjust: {hjust}; vjust: {vjust}&quot;)) mpg %&gt;% ggplot(aes(x = hwy, displ))+ geom_count(aes(colour = class))+ labs(title = &quot;Larger displacement has lower gas mileage efficiency&quot;, subtitle = &quot;SUV and pickup classes` tend to be highest on disp&quot;, caption = &quot;Data is for cars made in either 1999 or 2008&quot;, colour = &quot;Car class&quot;)+ geom_text(aes(x = x, y = y, label = label, hjust = hjust, vjust = vjust), data = data_label) Read the documentation for annotate(). How can you use it to add a text label to a plot without having to create a tibble? function adds geoms, but not mapped from variables of a dataframe, so can pass in small items or single labels mpg %&gt;% ggplot(aes(x = hwy, displ))+ geom_count(aes(colour = class))+ labs(title = &quot;Larger displacement has lower gas mileage efficiency&quot;, subtitle = &quot;SUV and pickup classes` tend to be highest on disp&quot;, caption = &quot;Data is for cars made in either 1999 or 2008&quot;, colour = &quot;Car class&quot;)+ annotate(&quot;text&quot;, x = Inf, y = Inf, label = paste0(&quot;Mean highway mpg: &quot;, round(mean(mpg$hwy))), vjust = &quot;top&quot;, hjust = &quot;right&quot;) How do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: think about the underlying data.) data_label_single &lt;- tibble(x = Inf, y = Inf, label = paste0(&quot;Mean highway mpg: &quot;, round(mean(mpg$hwy)))) data_label &lt;- mpg %&gt;% group_by(class) %&gt;% summarise(hwy = round(mean(hwy))) %&gt;% mutate(label = paste0(&quot;hwy mpg for &quot;, class, &quot;: &quot;, hwy)) %&gt;% mutate(x = Inf, y = Inf) mpg %&gt;% ggplot(aes(x = hwy, displ))+ geom_count(aes(colour = class))+ labs(title = &quot;Larger displacement has lower gas mileage efficiency&quot;, subtitle = &quot;SUV and pickup classes` tend to be highest on disp&quot;, caption = &quot;Data is for cars made in either 1999 or 2008&quot;, colour = &quot;Car class&quot;)+ facet_wrap(~class)+ geom_smooth()+ geom_text(aes(x = x, y = y, label = label), data = data_label, vjust = &quot;top&quot;, hjust = &quot;right&quot;) What arguments to geom_label() control the appearance of the background box? fill argument controls background color alpha controls it’s relative brighness best_in_class &lt;- mpg %&gt;% group_by(class) %&gt;% filter(row_number(desc(hwy)) == 1) ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = class)) + geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.1, fill = &quot;green&quot;) What are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options. b &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point() df &lt;- data.frame(x1 = 2.62, x2 = 3.57, y1 = 21.0, y2 = 15.0) b + geom_curve( aes(x = x1, y = y1, xend = x2, yend = y2), data = df, arrow = arrow(length = unit(0.03, &quot;npc&quot;)) ) angle (in degrees), length (use unit() function to specify with number and type, e.g. “inches”), ends (“last”, “first”, or “both” – specifying which end), type (“open” or “closed”) See 28.3.1.5 for more notes on line options (not specific to arrow()) 28.4: Scales 28.4.4 Why doesn’t the following code override the default scale? df &lt;- tibble(x = rnorm(100), y = rnorm(100)) ggplot(df, aes(x, y)) + geom_hex() + scale_colour_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() geom_hex uses fill, not colour df &lt;- tibble(x = rnorm(100), y = rnorm(100)) ggplot(df, aes(x, y)) + geom_hex() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() What is the first argument to every scale? How does it compare to labs()? name, i.e. what the title will be for that axis/legend/… labs first argument is ... so requires you to name the input Change the display of the presidential terms by: Combining the two variants shown above. Improving the display of the y axis. Labelling each term with the name of the president. Adding informative plot labels. Placing breaks every 4 years (this is trickier than it seems!). presidential %&gt;% mutate(id = 33L + row_number()) %&gt;% ggplot(aes(start, id, colour = party)) + geom_point() + geom_segment(aes(xend = end, yend = id)) + geom_text(aes(label = name), vjust = &quot;bottom&quot;, nudge_y = 0.2)+ scale_colour_manual(values = c(Republican = &quot;red&quot;, Democratic = &quot;blue&quot;))+ scale_x_date(&quot;Year in 20th and 21st century&quot;, date_breaks = &quot;4 years&quot;, date_labels = &quot;&#39;%y&quot;)+ # scale_x_date(NULL, breaks = presidential$start, date_labels = &quot;&#39;%y&quot;)+ scale_y_continuous(breaks = c(36, 39, 42), labels = c(&quot;36th&quot;, &quot;39th&quot;, &quot;42nd&quot;))+ labs(y = &quot;President number&quot;, x = &quot;Year&quot;) Use override.aes to make the legend on the following plot easier to see. diamonds %&gt;% ggplot(aes(carat, price)) + geom_point(aes(colour = cut), alpha = 1/20)+ guides(colour = guide_legend(override.aes = list(alpha = 1))) Appendix 28.3.1.5 Not arrow() function specifically, but other line end options ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = class)) + geom_segment(aes(xend = displ +5, yend = hwy + 5), data = best_in_class, lineend = &quot;round&quot;) b &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point() df &lt;- data.frame(x1 = 2.62, x2 = 3.57, y1 = 21.0, y2 = 15.0) b + geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2, colour = &quot;curve&quot;), data = df) + geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2, colour = &quot;segment&quot;), data = df) b + geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2), data = df, curvature = -0.2) b + geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2), data = df, curvature = 1) "]
]
